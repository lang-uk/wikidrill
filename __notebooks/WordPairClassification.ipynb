{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Word Pair Classification using TextPairClassifier in Flair\n",
        "\n",
        "Let's see what's possible here! 🚀\n",
        "\n",
        "__Goal__: solve word pair classification task using a model that is originally build to classify sentence pairs.\n",
        "\n",
        "Main thing to consider: TextPairClassifier has an option to embedd separately (which goes in the direction of siamese networks). if embedd_separately is set to False (default configuration), this model creates a single sentence out of two by adding a [SEP] between them.\n",
        "\n",
        "__Dataset__: Dummy dataset to see if the model works. I will take an existing sentence-pair dataset and keep only first words.\n",
        "\n",
        "__Steps__:\n",
        "- We will try to solve word pair classification task using TransformerDocumentEmbeddings + TextPairClassifier (similar to sentence-pair classification task)\n",
        "- We will try to use FlairWordEmbeddings and/or WordEmbeddings\n",
        "- TODO: We will look if TransformerWordEmbeddings can be fine-tuned together with TextPairClassifier model.\n",
        "\n",
        "\n",
        "__Notes__: \n",
        "- ❌ I just ran into the same issue (using the latest flair version) as described [in the last message here](https://github.com/flairNLP/flair/issues/2536). That's why I will use flair==0.10. I will create a PR to fix it soonish.\n",
        "- My dataset is just a dummy dataset. We are classifying two random words as entailment and not_entailment. This means that results don't make sense. This is just to test if the model runs and share some ideas for you work 😊\n",
        "\n",
        "\n",
        "Let's try 👉"
      ],
      "metadata": {
        "id": "CXxjzluJyPM3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LtYFhE9tVYP"
      },
      "outputs": [],
      "source": [
        "!pip install flair==0.10 &> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import flair\n",
        "\n",
        "print(flair.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MX9Kqxivtd4m",
        "outputId": "4dedf32e-d2f1-426b-a551-bad4e5de5f57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1. Create a dataset of single word pairs\n",
        "\n",
        "I will take an existing sentence-pair dataset and leave only first words in each sentence. The goal is to test if TextPairClassifier can be used with WordEmbeddings, FlairEmbeddings, or TransformerWordEmbeddings!\n",
        "\n",
        "If you are using your own custom dataset, you will need to load it as described in [this issue](https://github.com/flairNLP/flair/issues/2536)."
      ],
      "metadata": {
        "id": "aw0qZnhiuZ9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flair.datasets import GLUE_RTE\n",
        "from flair.embeddings import TransformerDocumentEmbeddings\n",
        "from flair.models import TextPairClassifier\n",
        "from flair.trainers import ModelTrainer\n",
        "\n",
        "# Step 1: You can look at an existing text pair corpus (e.g. GLUE_RTE, GLUE_MRPC)\n",
        "corpus = GLUE_RTE()\n",
        "\n",
        "# language inference: predict if sentence A entails or contradicts sentence B\n",
        "print(corpus)\n",
        "print(corpus.train[0])\n",
        "\n",
        "label_type = 'entailment'\n",
        "\n",
        "label_dictionary = corpus.make_label_dictionary(label_type=label_type)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taySdR6euZnH",
        "outputId": "20a33029-5454-498a-a02d-233f4786b1f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-21 16:59:15,395 Reading data from /root/.flair/datasets/glue/RTE\n",
            "2022-11-21 16:59:15,397 Train: /root/.flair/datasets/glue/RTE/train.tsv\n",
            "2022-11-21 16:59:15,400 Dev: /root/.flair/datasets/glue/RTE/dev.tsv\n",
            "2022-11-21 16:59:15,402 Test: None\n",
            "Corpus: 2241 train + 277 dev + 249 test sentences\n",
            "DataPair:\n",
            " − First Sentence: \"No Weapons of Mass Destruction Found in Iraq Yet .\"   [− Tokens: 10]\n",
            " − Second Sentence: \"Weapons of Mass Destruction Found in Iraq .\"   [− Tokens: 8]\n",
            " − Labels: [not_entailment (1.0)]\n",
            "2022-11-21 16:59:20,828 Computing label dictionary. Progress:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2241/2241 [00:00<00:00, 53986.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-21 16:59:20,877 Corpus contains the labels: entailment (#2241)\n",
            "2022-11-21 16:59:20,881 Created (for label 'entailment') Dictionary with 3 tags: <unk>, not_entailment, entailment\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's inspect our original dataset"
      ],
      "metadata": {
        "id": "tJX6_suw0e-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print a few data pairs from our original train set\n",
        "print('A random instance from training set:')\n",
        "print(corpus.train[2])\n",
        "\n",
        "# print a few data pairs from our original dev set\n",
        "print('\\nA random instance from validation set:')\n",
        "print(corpus.dev[5])\n",
        "\n",
        "# print a few data pairs from our original test set\n",
        "print('\\nA random instance from testing set:')\n",
        "print(corpus.test[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFhM5mv-wbUP",
        "outputId": "5e05770a-003e-47d1-e66f-d60ccf0933c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A random instance from training set:\n",
            "DataPair:\n",
            " − First Sentence: \"Judie Vivian , chief executive at ProMedica , a medical service company that helps sustain the 2-year-old Vietnam Heart Institute in Ho Chi Minh City ( formerly Saigon ) , said that so far about 1,500 children have received treatment .\"   [− Tokens: 41]\n",
            " − Second Sentence: \"The previous name of Ho Chi Minh City was Saigon .\"   [− Tokens: 11]\n",
            " − Labels: [entailment (1.0)]\n",
            "\n",
            "A random instance from validation set:\n",
            "DataPair:\n",
            " − First Sentence: \"In 1979 , the leaders signed the Egypt-Israel peace treaty on the White House lawn . Both President Begin and Sadat received the Nobel Peace Prize for their work . The two nations have enjoyed peaceful relations to this day .\"   [− Tokens: 41]\n",
            " − Second Sentence: \"The Israel-Egypt Peace Agreement was signed in 1979 .\"   [− Tokens: 9]\n",
            " − Labels: [entailment (1.0)]\n",
            "\n",
            "A random instance from testing set:\n",
            "DataPair:\n",
            " − First Sentence: \"Herceptin was already approved to treat the sickest breast cancer patients , and the company said , Monday , it will discuss with federal regulators the possibility of prescribing the drug for more breast cancer patients .\"   [− Tokens: 37]\n",
            " − Second Sentence: \"Herceptin can be used to treat breast cancer .\"   [− Tokens: 9]\n",
            " − Labels: [entailment (1.0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will only keep first words of each sentence"
      ],
      "metadata": {
        "id": "VhyXEz5y0nr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flair.data import Sentence\n",
        "\n",
        "for data_split in [corpus.train, corpus.dev, corpus.test]:\n",
        "  for sentence in data_split:\n",
        "    sentence.first = Sentence(sentence.first[0].text)\n",
        "    sentence.second = Sentence(sentence.second[0].text)"
      ],
      "metadata": {
        "id": "IBhjEOoIvFYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print a few data pairs from our new train set\n",
        "print('A random instance from training set:')\n",
        "print(corpus.train[2])\n",
        "\n",
        "# print a few data pairs from our new dev set\n",
        "print('\\nA random instance from validation set:')\n",
        "print(corpus.dev[5])\n",
        "\n",
        "# print a few data pairs from our new test set\n",
        "print('\\nA random instance from testing set:')\n",
        "print(corpus.test[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfBdlZykw9VH",
        "outputId": "cb1168fe-a981-41e5-9771-aa7fada3ac0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A random instance from training set:\n",
            "DataPair:\n",
            " − First Sentence: \"Judie\"   [− Tokens: 1]\n",
            " − Second Sentence: \"The\"   [− Tokens: 1]\n",
            " − Labels: [entailment (1.0)]\n",
            "\n",
            "A random instance from validation set:\n",
            "DataPair:\n",
            " − First Sentence: \"In\"   [− Tokens: 1]\n",
            " − Second Sentence: \"The\"   [− Tokens: 1]\n",
            " − Labels: [entailment (1.0)]\n",
            "\n",
            "A random instance from testing set:\n",
            "DataPair:\n",
            " − First Sentence: \"Herceptin\"   [− Tokens: 1]\n",
            " − Second Sentence: \"Herceptin\"   [− Tokens: 1]\n",
            " − Labels: [entailment (1.0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2. Two ways to use TransformerDocumentEmbeddings + TextPairClassifier.\n",
        "\n",
        "Keep in mind that TransformerDocumentEmbeddings are using CLS token representation instead of actual word (or sub-word) representation. This might not be ideal for your case where we have single words (but it can work anyway given that we fine-tune the model).\n",
        "\n"
      ],
      "metadata": {
        "id": "BKRW2584GU-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flair.embeddings import WordEmbeddings\n",
        "from flair.embeddings import TransformerDocumentEmbeddings\n",
        "from flair.models import TextPairClassifier\n",
        "from flair.trainers import ModelTrainer\n",
        "\n",
        "# Step 2: Pick English GloVe embeddings\n",
        "embeddings = TransformerDocumentEmbeddings('prajjwal1/bert-tiny', fine_tune=True)\n",
        "\n",
        "# Step 3: Use text pair classification model\n",
        "classifier = TextPairClassifier(document_embeddings=embeddings,\n",
        "                                label_type=label_type,\n",
        "                                label_dictionary=label_dictionary,\n",
        "                                embed_separately=False,\n",
        "                                )\n",
        "\n",
        "# Step 4: Initialize trainer and train the model\n",
        "trainer = ModelTrainer(classifier, corpus)\n",
        "\n",
        "# if you are using transformer embeddings, you can simply call trainer.fine_tune()\n",
        "trainer.fine_tune(base_path='resources/word-pair-test-flair',\n",
        "                  use_final_model_for_eval=False,\n",
        "                  learning_rate=2e-5,\n",
        "                  mini_batch_size=16,\n",
        "                  max_epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mMFVgOk8oTI",
        "outputId": "efda137e-b234-4a30-972e-baebaa7314ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-21 17:00:20,580 No model_max_length in Tokenizer's config.json - setting it to 512. Specify desired model_max_length by passing it as attribute to embedding instance.\n",
            "2022-11-21 17:00:21,388 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:00:21,391 Model: \"TextPairClassifier(\n",
            "  (loss_function): CrossEntropyLoss()\n",
            "  (document_embeddings): TransformerDocumentEmbeddings(\n",
            "    (model): BertModel(\n",
            "      (embeddings): BertEmbeddings(\n",
            "        (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 128)\n",
            "        (token_type_embeddings): Embedding(2, 128)\n",
            "        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (encoder): BertEncoder(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (key): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (value): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
            "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (key): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (value): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
            "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): BertPooler(\n",
            "        (dense): Linear(in_features=128, out_features=128, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): Linear(in_features=128, out_features=3, bias=True)\n",
            "  (weights): None\n",
            "  (weight_tensor) None\n",
            ")\"\n",
            "2022-11-21 17:00:21,394 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:00:21,396 Corpus: \"Corpus: 2241 train + 277 dev + 249 test sentences\"\n",
            "2022-11-21 17:00:21,398 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:00:21,399 Parameters:\n",
            "2022-11-21 17:00:21,403  - learning_rate: \"2e-05\"\n",
            "2022-11-21 17:00:21,404  - mini_batch_size: \"16\"\n",
            "2022-11-21 17:00:21,406  - patience: \"3\"\n",
            "2022-11-21 17:00:21,407  - anneal_factor: \"0.5\"\n",
            "2022-11-21 17:00:21,409  - max_epochs: \"10\"\n",
            "2022-11-21 17:00:21,410  - shuffle: \"True\"\n",
            "2022-11-21 17:00:21,414  - train_with_dev: \"False\"\n",
            "2022-11-21 17:00:21,415  - batch_growth_annealing: \"False\"\n",
            "2022-11-21 17:00:21,416 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:00:21,419 Model training base path: \"resources/word-pair-test-flair\"\n",
            "2022-11-21 17:00:21,422 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:00:21,424 Device: cuda:0\n",
            "2022-11-21 17:00:21,425 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:00:21,426 Embeddings storage mode: none\n",
            "2022-11-21 17:00:21,434 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/flair/trainers/trainer.py:65: UserWarning: There should be no best model saved at epoch 1 except there is a model from previous trainings in your training folder. All previous best models will be deleted.\n",
            "  \"There should be no best model saved at epoch 1 except there is a model from previous trainings\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-21 17:00:21,689 epoch 1 - iter 14/141 - loss 0.14913489 - samples/sec: 887.96 - lr: 0.000002\n",
            "2022-11-21 17:00:21,931 epoch 1 - iter 28/141 - loss 0.14022871 - samples/sec: 937.75 - lr: 0.000004\n",
            "2022-11-21 17:00:22,175 epoch 1 - iter 42/141 - loss 0.13314404 - samples/sec: 929.21 - lr: 0.000006\n",
            "2022-11-21 17:00:22,420 epoch 1 - iter 56/141 - loss 0.12583450 - samples/sec: 928.16 - lr: 0.000008\n",
            "2022-11-21 17:00:22,666 epoch 1 - iter 70/141 - loss 0.11711008 - samples/sec: 918.57 - lr: 0.000010\n",
            "2022-11-21 17:00:22,905 epoch 1 - iter 84/141 - loss 0.10797442 - samples/sec: 951.39 - lr: 0.000012\n",
            "2022-11-21 17:00:23,148 epoch 1 - iter 98/141 - loss 0.10057374 - samples/sec: 931.72 - lr: 0.000014\n",
            "2022-11-21 17:00:23,394 epoch 1 - iter 112/141 - loss 0.09489216 - samples/sec: 922.68 - lr: 0.000016\n",
            "2022-11-21 17:00:23,637 epoch 1 - iter 126/141 - loss 0.09015299 - samples/sec: 935.01 - lr: 0.000018\n",
            "2022-11-21 17:00:23,874 epoch 1 - iter 140/141 - loss 0.08607270 - samples/sec: 954.65 - lr: 0.000020\n",
            "2022-11-21 17:00:23,889 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:00:23,891 EPOCH 1 done: loss 0.0862 - lr 0.0000200\n",
            "2022-11-21 17:00:24,051 DEV : loss 0.049223851412534714 - f1-score (micro avg)  0.4982\n",
            "2022-11-21 17:00:24,055 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:00:24,057 saving best model\n",
            "2022-11-21 17:00:24,095 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:00:24,349 epoch 2 - iter 14/141 - loss 0.05242965 - samples/sec: 895.01 - lr: 0.000020\n",
            "2022-11-21 17:00:24,589 epoch 2 - iter 28/141 - loss 0.05158105 - samples/sec: 946.62 - lr: 0.000020\n",
            "2022-11-21 17:00:24,834 epoch 2 - iter 42/141 - loss 0.05126672 - samples/sec: 925.03 - lr: 0.000019\n",
            "2022-11-21 17:00:25,073 epoch 2 - iter 56/141 - loss 0.05044133 - samples/sec: 949.71 - lr: 0.000019\n",
            "2022-11-21 17:00:25,326 epoch 2 - iter 70/141 - loss 0.05000975 - samples/sec: 894.77 - lr: 0.000019\n",
            "2022-11-21 17:00:25,572 epoch 2 - iter 84/141 - loss 0.04981243 - samples/sec: 924.80 - lr: 0.000019\n",
            "2022-11-21 17:00:25,814 epoch 2 - iter 98/141 - loss 0.04971010 - samples/sec: 933.25 - lr: 0.000018\n",
            "2022-11-21 17:00:26,067 epoch 2 - iter 112/141 - loss 0.04945082 - samples/sec: 897.20 - lr: 0.000018\n",
            "2022-11-21 17:00:26,329 epoch 2 - iter 126/141 - loss 0.04928793 - samples/sec: 866.08 - lr: 0.000018\n",
            "2022-11-21 17:00:26,582 epoch 2 - iter 140/141 - loss 0.04938373 - samples/sec: 897.58 - lr: 0.000018\n",
            "2022-11-21 17:00:26,597 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:00:26,598 EPOCH 2 done: loss 0.0496 - lr 0.0000178\n",
            "2022-11-21 17:00:26,759 DEV : loss 0.05089972913265228 - f1-score (micro avg)  0.4693\n",
            "2022-11-21 17:00:26,763 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:00:26,765 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:00:27,008 epoch 3 - iter 14/141 - loss 0.04707785 - samples/sec: 938.72 - lr: 0.000018\n",
            "2022-11-21 17:00:27,246 epoch 3 - iter 28/141 - loss 0.04661080 - samples/sec: 952.49 - lr: 0.000017\n",
            "2022-11-21 17:00:27,489 epoch 3 - iter 42/141 - loss 0.04610927 - samples/sec: 933.59 - lr: 0.000017\n",
            "2022-11-21 17:00:27,728 epoch 3 - iter 56/141 - loss 0.04610581 - samples/sec: 951.12 - lr: 0.000017\n",
            "2022-11-21 17:00:27,966 epoch 3 - iter 70/141 - loss 0.04628146 - samples/sec: 951.47 - lr: 0.000017\n",
            "2022-11-21 17:00:28,210 epoch 3 - iter 84/141 - loss 0.04614117 - samples/sec: 928.43 - lr: 0.000016\n",
            "2022-11-21 17:00:28,453 epoch 3 - iter 98/141 - loss 0.04598454 - samples/sec: 931.31 - lr: 0.000016\n",
            "2022-11-21 17:00:28,698 epoch 3 - iter 112/141 - loss 0.04603999 - samples/sec: 927.11 - lr: 0.000016\n",
            "2022-11-21 17:00:28,944 epoch 3 - iter 126/141 - loss 0.04607162 - samples/sec: 924.25 - lr: 0.000016\n",
            "2022-11-21 17:00:29,183 epoch 3 - iter 140/141 - loss 0.04601327 - samples/sec: 952.58 - lr: 0.000016\n",
            "2022-11-21 17:00:29,196 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:00:29,197 EPOCH 3 done: loss 0.0466 - lr 0.0000156\n",
            "2022-11-21 17:00:29,360 DEV : loss 0.04650456830859184 - f1-score (micro avg)  0.4693\n",
            "2022-11-21 17:00:29,362 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:00:29,365 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:00:29,609 epoch 4 - iter 14/141 - loss 0.04406916 - samples/sec: 933.41 - lr: 0.000015\n",
            "2022-11-21 17:00:29,858 epoch 4 - iter 28/141 - loss 0.04416798 - samples/sec: 909.94 - lr: 0.000015\n",
            "2022-11-21 17:00:30,108 epoch 4 - iter 42/141 - loss 0.04480383 - samples/sec: 908.67 - lr: 0.000015\n",
            "2022-11-21 17:00:30,361 epoch 4 - iter 56/141 - loss 0.04504664 - samples/sec: 896.11 - lr: 0.000015\n",
            "2022-11-21 17:00:30,606 epoch 4 - iter 70/141 - loss 0.04459200 - samples/sec: 928.60 - lr: 0.000014\n",
            "2022-11-21 17:00:30,853 epoch 4 - iter 84/141 - loss 0.04423627 - samples/sec: 916.20 - lr: 0.000014\n",
            "2022-11-21 17:00:31,102 epoch 4 - iter 98/141 - loss 0.04407377 - samples/sec: 914.76 - lr: 0.000014\n",
            "2022-11-21 17:00:31,351 epoch 4 - iter 112/141 - loss 0.04457424 - samples/sec: 910.73 - lr: 0.000014\n",
            "2022-11-21 17:00:31,598 epoch 4 - iter 126/141 - loss 0.04450678 - samples/sec: 914.75 - lr: 0.000014\n",
            "2022-11-21 17:00:31,843 epoch 4 - iter 140/141 - loss 0.04435952 - samples/sec: 927.97 - lr: 0.000013\n",
            "2022-11-21 17:00:31,857 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:00:31,859 EPOCH 4 done: loss 0.0446 - lr 0.0000133\n",
            "2022-11-21 17:00:32,018 DEV : loss 0.04834210127592087 - f1-score (micro avg)  0.4838\n",
            "2022-11-21 17:00:32,021 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:00:32,023 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:00:32,273 epoch 5 - iter 14/141 - loss 0.04355562 - samples/sec: 910.61 - lr: 0.000013\n",
            "2022-11-21 17:00:32,519 epoch 5 - iter 28/141 - loss 0.04607700 - samples/sec: 922.89 - lr: 0.000013\n",
            "2022-11-21 17:00:32,806 epoch 5 - iter 42/141 - loss 0.04478444 - samples/sec: 790.40 - lr: 0.000013\n",
            "2022-11-21 17:00:33,054 epoch 5 - iter 56/141 - loss 0.04464392 - samples/sec: 915.46 - lr: 0.000012\n",
            "2022-11-21 17:00:33,329 epoch 5 - iter 70/141 - loss 0.04395803 - samples/sec: 824.07 - lr: 0.000012\n",
            "2022-11-21 17:00:33,607 epoch 5 - iter 84/141 - loss 0.04424994 - samples/sec: 840.93 - lr: 0.000012\n",
            "2022-11-21 17:00:33,850 epoch 5 - iter 98/141 - loss 0.04417267 - samples/sec: 933.26 - lr: 0.000012\n",
            "2022-11-21 17:00:34,133 epoch 5 - iter 112/141 - loss 0.04419717 - samples/sec: 798.98 - lr: 0.000012\n",
            "2022-11-21 17:00:34,385 epoch 5 - iter 126/141 - loss 0.04414456 - samples/sec: 901.18 - lr: 0.000011\n",
            "2022-11-21 17:00:34,633 epoch 5 - iter 140/141 - loss 0.04411481 - samples/sec: 915.34 - lr: 0.000011\n",
            "2022-11-21 17:00:34,648 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:00:34,650 EPOCH 5 done: loss 0.0443 - lr 0.0000111\n",
            "2022-11-21 17:00:34,809 DEV : loss 0.046132560819387436 - f1-score (micro avg)  0.4693\n",
            "2022-11-21 17:00:34,812 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:00:34,815 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:00:35,062 epoch 6 - iter 14/141 - loss 0.04372350 - samples/sec: 919.03 - lr: 0.000011\n",
            "2022-11-21 17:00:35,305 epoch 6 - iter 28/141 - loss 0.04465225 - samples/sec: 934.16 - lr: 0.000011\n",
            "2022-11-21 17:00:35,546 epoch 6 - iter 42/141 - loss 0.04514808 - samples/sec: 938.64 - lr: 0.000010\n",
            "2022-11-21 17:00:35,791 epoch 6 - iter 56/141 - loss 0.04445987 - samples/sec: 928.02 - lr: 0.000010\n",
            "2022-11-21 17:00:36,039 epoch 6 - iter 70/141 - loss 0.04478298 - samples/sec: 910.82 - lr: 0.000010\n",
            "2022-11-21 17:00:36,290 epoch 6 - iter 84/141 - loss 0.04477016 - samples/sec: 904.13 - lr: 0.000010\n",
            "2022-11-21 17:00:36,546 epoch 6 - iter 98/141 - loss 0.04443845 - samples/sec: 886.98 - lr: 0.000010\n",
            "2022-11-21 17:00:36,796 epoch 6 - iter 112/141 - loss 0.04427623 - samples/sec: 904.84 - lr: 0.000009\n",
            "2022-11-21 17:00:37,034 epoch 6 - iter 126/141 - loss 0.04432702 - samples/sec: 956.26 - lr: 0.000009\n",
            "2022-11-21 17:00:37,281 epoch 6 - iter 140/141 - loss 0.04422061 - samples/sec: 918.71 - lr: 0.000009\n",
            "2022-11-21 17:00:37,295 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:00:37,297 EPOCH 6 done: loss 0.0445 - lr 0.0000089\n",
            "2022-11-21 17:00:37,459 DEV : loss 0.04598337784409523 - f1-score (micro avg)  0.4838\n",
            "2022-11-21 17:00:37,463 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:00:37,464 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:00:37,711 epoch 7 - iter 14/141 - loss 0.04380375 - samples/sec: 923.96 - lr: 0.000009\n",
            "2022-11-21 17:00:37,954 epoch 7 - iter 28/141 - loss 0.04409979 - samples/sec: 931.95 - lr: 0.000008\n",
            "2022-11-21 17:00:38,201 epoch 7 - iter 42/141 - loss 0.04406617 - samples/sec: 916.87 - lr: 0.000008\n",
            "2022-11-21 17:00:38,441 epoch 7 - iter 56/141 - loss 0.04401518 - samples/sec: 945.16 - lr: 0.000008\n",
            "2022-11-21 17:00:38,680 epoch 7 - iter 70/141 - loss 0.04384925 - samples/sec: 948.33 - lr: 0.000008\n",
            "2022-11-21 17:00:38,922 epoch 7 - iter 84/141 - loss 0.04386558 - samples/sec: 939.57 - lr: 0.000008\n",
            "2022-11-21 17:00:39,162 epoch 7 - iter 98/141 - loss 0.04394196 - samples/sec: 945.94 - lr: 0.000007\n",
            "2022-11-21 17:00:39,404 epoch 7 - iter 112/141 - loss 0.04364328 - samples/sec: 938.08 - lr: 0.000007\n",
            "2022-11-21 17:00:39,645 epoch 7 - iter 126/141 - loss 0.04388812 - samples/sec: 940.70 - lr: 0.000007\n",
            "2022-11-21 17:00:39,883 epoch 7 - iter 140/141 - loss 0.04369678 - samples/sec: 950.72 - lr: 0.000007\n",
            "2022-11-21 17:00:39,897 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:00:39,900 EPOCH 7 done: loss 0.0439 - lr 0.0000067\n",
            "2022-11-21 17:00:40,054 DEV : loss 0.045754559338092804 - f1-score (micro avg)  0.5054\n",
            "2022-11-21 17:00:40,056 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:00:40,059 saving best model\n",
            "2022-11-21 17:00:40,107 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:00:40,360 epoch 8 - iter 14/141 - loss 0.04178978 - samples/sec: 898.85 - lr: 0.000006\n",
            "2022-11-21 17:00:40,604 epoch 8 - iter 28/141 - loss 0.04160327 - samples/sec: 929.33 - lr: 0.000006\n",
            "2022-11-21 17:00:40,844 epoch 8 - iter 42/141 - loss 0.04192450 - samples/sec: 944.01 - lr: 0.000006\n",
            "2022-11-21 17:00:41,084 epoch 8 - iter 56/141 - loss 0.04241315 - samples/sec: 947.41 - lr: 0.000006\n",
            "2022-11-21 17:00:41,331 epoch 8 - iter 70/141 - loss 0.04261983 - samples/sec: 917.60 - lr: 0.000006\n",
            "2022-11-21 17:00:41,579 epoch 8 - iter 84/141 - loss 0.04292852 - samples/sec: 912.97 - lr: 0.000005\n",
            "2022-11-21 17:00:41,827 epoch 8 - iter 98/141 - loss 0.04317129 - samples/sec: 914.10 - lr: 0.000005\n",
            "2022-11-21 17:00:42,071 epoch 8 - iter 112/141 - loss 0.04337500 - samples/sec: 929.94 - lr: 0.000005\n",
            "2022-11-21 17:00:42,318 epoch 8 - iter 126/141 - loss 0.04349195 - samples/sec: 918.15 - lr: 0.000005\n",
            "2022-11-21 17:00:42,568 epoch 8 - iter 140/141 - loss 0.04349750 - samples/sec: 905.93 - lr: 0.000004\n",
            "2022-11-21 17:00:42,582 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:00:42,583 EPOCH 8 done: loss 0.0439 - lr 0.0000044\n",
            "2022-11-21 17:00:42,742 DEV : loss 0.04600101336836815 - f1-score (micro avg)  0.4693\n",
            "2022-11-21 17:00:42,745 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:00:42,747 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:00:43,001 epoch 9 - iter 14/141 - loss 0.04317630 - samples/sec: 895.70 - lr: 0.000004\n",
            "2022-11-21 17:00:43,250 epoch 9 - iter 28/141 - loss 0.04346949 - samples/sec: 909.75 - lr: 0.000004\n",
            "2022-11-21 17:00:43,508 epoch 9 - iter 42/141 - loss 0.04362668 - samples/sec: 880.12 - lr: 0.000004\n",
            "2022-11-21 17:00:43,758 epoch 9 - iter 56/141 - loss 0.04356071 - samples/sec: 906.18 - lr: 0.000004\n",
            "2022-11-21 17:00:44,004 epoch 9 - iter 70/141 - loss 0.04345762 - samples/sec: 923.75 - lr: 0.000003\n",
            "2022-11-21 17:00:44,261 epoch 9 - iter 84/141 - loss 0.04343377 - samples/sec: 880.76 - lr: 0.000003\n",
            "2022-11-21 17:00:44,526 epoch 9 - iter 98/141 - loss 0.04403778 - samples/sec: 856.66 - lr: 0.000003\n",
            "2022-11-21 17:00:44,962 epoch 9 - iter 112/141 - loss 0.04405599 - samples/sec: 518.30 - lr: 0.000003\n",
            "2022-11-21 17:00:45,374 epoch 9 - iter 126/141 - loss 0.04387407 - samples/sec: 548.59 - lr: 0.000002\n",
            "2022-11-21 17:00:45,738 epoch 9 - iter 140/141 - loss 0.04385015 - samples/sec: 623.00 - lr: 0.000002\n",
            "2022-11-21 17:00:45,759 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:00:45,762 EPOCH 9 done: loss 0.0439 - lr 0.0000022\n",
            "2022-11-21 17:00:46,001 DEV : loss 0.046217210590839386 - f1-score (micro avg)  0.4693\n",
            "2022-11-21 17:00:46,004 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:00:46,008 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:00:46,392 epoch 10 - iter 14/141 - loss 0.04353819 - samples/sec: 594.20 - lr: 0.000002\n",
            "2022-11-21 17:00:46,746 epoch 10 - iter 28/141 - loss 0.04332705 - samples/sec: 638.38 - lr: 0.000002\n",
            "2022-11-21 17:00:46,996 epoch 10 - iter 42/141 - loss 0.04292666 - samples/sec: 909.55 - lr: 0.000002\n",
            "2022-11-21 17:00:47,288 epoch 10 - iter 56/141 - loss 0.04291130 - samples/sec: 775.06 - lr: 0.000001\n",
            "2022-11-21 17:00:47,559 epoch 10 - iter 70/141 - loss 0.04337715 - samples/sec: 838.16 - lr: 0.000001\n",
            "2022-11-21 17:00:47,845 epoch 10 - iter 84/141 - loss 0.04307783 - samples/sec: 792.64 - lr: 0.000001\n",
            "2022-11-21 17:00:48,141 epoch 10 - iter 98/141 - loss 0.04281458 - samples/sec: 766.48 - lr: 0.000001\n",
            "2022-11-21 17:00:48,408 epoch 10 - iter 112/141 - loss 0.04295377 - samples/sec: 848.88 - lr: 0.000000\n",
            "2022-11-21 17:00:48,720 epoch 10 - iter 126/141 - loss 0.04302367 - samples/sec: 723.69 - lr: 0.000000\n",
            "2022-11-21 17:00:49,148 epoch 10 - iter 140/141 - loss 0.04309939 - samples/sec: 530.10 - lr: 0.000000\n",
            "2022-11-21 17:00:49,169 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:00:49,170 EPOCH 10 done: loss 0.0435 - lr 0.0000000\n",
            "2022-11-21 17:00:49,356 DEV : loss 0.04614556208252907 - f1-score (micro avg)  0.4801\n",
            "2022-11-21 17:00:49,359 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:00:49,419 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:00:49,423 loading file resources/word-pair-test-flair/best-model.pt\n",
            "2022-11-21 17:00:51,788 No model_max_length in Tokenizer's config.json - setting it to 512. Specify desired model_max_length by passing it as attribute to embedding instance.\n",
            "2022-11-21 17:00:51,999 0.4779\t0.4779\t0.4779\t0.4779\n",
            "2022-11-21 17:00:52,000 \n",
            "Results:\n",
            "- F-score (micro) 0.4779\n",
            "- F-score (macro) 0.4317\n",
            "- Accuracy 0.4779\n",
            "\n",
            "By class:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "    entailment     0.5053    0.7197    0.5938       132\n",
            "not_entailment     0.3934    0.2051    0.2697       117\n",
            "\n",
            "     micro avg     0.4779    0.4779    0.4779       249\n",
            "     macro avg     0.4494    0.4624    0.4317       249\n",
            "  weighted avg     0.4528    0.4779    0.4415       249\n",
            "   samples avg     0.4779    0.4779    0.4779       249\n",
            "\n",
            "2022-11-21 17:00:52,004 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_score': 0.4779116465863454,\n",
              " 'dev_score_history': [0.4981949458483754,\n",
              "  0.4693140794223827,\n",
              "  0.4693140794223827,\n",
              "  0.48375451263537905,\n",
              "  0.4693140794223827,\n",
              "  0.48375451263537905,\n",
              "  0.5054151624548736,\n",
              "  0.4693140794223827,\n",
              "  0.4693140794223827,\n",
              "  0.48014440433212996],\n",
              " 'train_loss_history': [0.08617967565322443,\n",
              "  0.049560129150435735,\n",
              "  0.04661472768775058,\n",
              "  0.04455986306476891,\n",
              "  0.04429628055364838,\n",
              "  0.04454586302154248,\n",
              "  0.04387105435518642,\n",
              "  0.04389802295679708,\n",
              "  0.0439148366065602,\n",
              "  0.04345289485787558],\n",
              " 'dev_loss_history': [tensor(0.0492, device='cuda:0'),\n",
              "  tensor(0.0509, device='cuda:0'),\n",
              "  tensor(0.0465, device='cuda:0'),\n",
              "  tensor(0.0483, device='cuda:0'),\n",
              "  tensor(0.0461, device='cuda:0'),\n",
              "  tensor(0.0460, device='cuda:0'),\n",
              "  tensor(0.0458, device='cuda:0'),\n",
              "  tensor(0.0460, device='cuda:0'),\n",
              "  tensor(0.0462, device='cuda:0'),\n",
              "  tensor(0.0461, device='cuda:0')]}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## trying to embed words separately instead of together.\n",
        "\n",
        "- embed_separately=True means: 'fine' and 'great' will be embedded separately (i.e. passed through the embedding model separately).\n",
        "- embed_separately=False means: 'fine' and 'great' will create a new sentence 'fine [SEP] great' and this will be embedded (i.e. a single sentence will be passed through the embedding model)."
      ],
      "metadata": {
        "id": "IFBU6pMTHb7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flair.embeddings import WordEmbeddings\n",
        "from flair.embeddings import TransformerDocumentEmbeddings\n",
        "from flair.models import TextPairClassifier\n",
        "from flair.trainers import ModelTrainer\n",
        "\n",
        "# Step 2: Pick English GloVe embeddings\n",
        "embeddings = TransformerDocumentEmbeddings('prajjwal1/bert-tiny')\n",
        "\n",
        "# Step 3: Use text pair classification model\n",
        "classifier = TextPairClassifier(document_embeddings=embeddings,\n",
        "                                label_type=label_type,\n",
        "                                label_dictionary=label_dictionary,\n",
        "                                embed_separately=True, # embedd sentences separately and concatenate them later\n",
        "                                )\n",
        "\n",
        "# Step 4: Initialize trainer and train the model\n",
        "trainer = ModelTrainer(classifier, corpus)\n",
        "\n",
        "# if you are using transformer embeddings, you can simply call trainer.fine_tune()\n",
        "trainer.fine_tune(base_path='resources/word-pair-test-flair',\n",
        "                  use_final_model_for_eval=False,\n",
        "                  learning_rate=2e-5,\n",
        "                  mini_batch_size=16,\n",
        "                  max_epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yi-XmkFZG4kx",
        "outputId": "ce2b5842-128a-4ae2-b940-6f3be167977d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-21 17:02:40,723 No model_max_length in Tokenizer's config.json - setting it to 512. Specify desired model_max_length by passing it as attribute to embedding instance.\n",
            "2022-11-21 17:02:41,537 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:02:41,539 Model: \"TextPairClassifier(\n",
            "  (loss_function): CrossEntropyLoss()\n",
            "  (document_embeddings): TransformerDocumentEmbeddings(\n",
            "    (model): BertModel(\n",
            "      (embeddings): BertEmbeddings(\n",
            "        (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 128)\n",
            "        (token_type_embeddings): Embedding(2, 128)\n",
            "        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (encoder): BertEncoder(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (key): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (value): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
            "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (key): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (value): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
            "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): BertPooler(\n",
            "        (dense): Linear(in_features=128, out_features=128, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): Linear(in_features=256, out_features=3, bias=True)\n",
            "  (weights): None\n",
            "  (weight_tensor) None\n",
            ")\"\n",
            "2022-11-21 17:02:41,541 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:02:41,544 Corpus: \"Corpus: 2241 train + 277 dev + 249 test sentences\"\n",
            "2022-11-21 17:02:41,545 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:02:41,548 Parameters:\n",
            "2022-11-21 17:02:41,550  - learning_rate: \"2e-05\"\n",
            "2022-11-21 17:02:41,552  - mini_batch_size: \"16\"\n",
            "2022-11-21 17:02:41,553  - patience: \"3\"\n",
            "2022-11-21 17:02:41,555  - anneal_factor: \"0.5\"\n",
            "2022-11-21 17:02:41,558  - max_epochs: \"10\"\n",
            "2022-11-21 17:02:41,559  - shuffle: \"True\"\n",
            "2022-11-21 17:02:41,561  - train_with_dev: \"False\"\n",
            "2022-11-21 17:02:41,564  - batch_growth_annealing: \"False\"\n",
            "2022-11-21 17:02:41,566 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:02:41,567 Model training base path: \"resources/word-pair-test-flair\"\n",
            "2022-11-21 17:02:41,570 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:02:41,572 Device: cuda:0\n",
            "2022-11-21 17:02:41,574 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:02:41,576 Embeddings storage mode: none\n",
            "2022-11-21 17:02:41,582 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/flair/trainers/trainer.py:65: UserWarning: There should be no best model saved at epoch 1 except there is a model from previous trainings in your training folder. All previous best models will be deleted.\n",
            "  \"There should be no best model saved at epoch 1 except there is a model from previous trainings\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-21 17:02:41,960 epoch 1 - iter 14/141 - loss 0.09959839 - samples/sec: 598.72 - lr: 0.000002\n",
            "2022-11-21 17:02:42,329 epoch 1 - iter 28/141 - loss 0.09516682 - samples/sec: 611.23 - lr: 0.000004\n",
            "2022-11-21 17:02:42,707 epoch 1 - iter 42/141 - loss 0.09305085 - samples/sec: 597.19 - lr: 0.000006\n",
            "2022-11-21 17:02:43,080 epoch 1 - iter 56/141 - loss 0.09119593 - samples/sec: 606.35 - lr: 0.000008\n",
            "2022-11-21 17:02:43,452 epoch 1 - iter 70/141 - loss 0.08587865 - samples/sec: 605.75 - lr: 0.000010\n",
            "2022-11-21 17:02:43,833 epoch 1 - iter 84/141 - loss 0.08145745 - samples/sec: 593.42 - lr: 0.000012\n",
            "2022-11-21 17:02:44,202 epoch 1 - iter 98/141 - loss 0.07789742 - samples/sec: 612.77 - lr: 0.000014\n",
            "2022-11-21 17:02:44,574 epoch 1 - iter 112/141 - loss 0.07593339 - samples/sec: 606.97 - lr: 0.000016\n",
            "2022-11-21 17:02:44,941 epoch 1 - iter 126/141 - loss 0.07390003 - samples/sec: 616.09 - lr: 0.000018\n",
            "2022-11-21 17:02:45,312 epoch 1 - iter 140/141 - loss 0.07169289 - samples/sec: 607.13 - lr: 0.000020\n",
            "2022-11-21 17:02:45,331 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:02:45,333 EPOCH 1 done: loss 0.0721 - lr 0.0000200\n",
            "2022-11-21 17:02:45,587 DEV : loss 0.051248494535684586 - f1-score (micro avg)  0.4729\n",
            "2022-11-21 17:02:45,590 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:02:45,593 saving best model\n",
            "2022-11-21 17:02:45,631 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:02:45,999 epoch 2 - iter 14/141 - loss 0.05494771 - samples/sec: 615.74 - lr: 0.000020\n",
            "2022-11-21 17:02:46,373 epoch 2 - iter 28/141 - loss 0.05614646 - samples/sec: 603.51 - lr: 0.000020\n",
            "2022-11-21 17:02:46,749 epoch 2 - iter 42/141 - loss 0.05449554 - samples/sec: 602.59 - lr: 0.000019\n",
            "2022-11-21 17:02:47,113 epoch 2 - iter 56/141 - loss 0.05444005 - samples/sec: 620.51 - lr: 0.000019\n",
            "2022-11-21 17:02:47,484 epoch 2 - iter 70/141 - loss 0.05375195 - samples/sec: 607.53 - lr: 0.000019\n",
            "2022-11-21 17:02:47,854 epoch 2 - iter 84/141 - loss 0.05376506 - samples/sec: 610.70 - lr: 0.000019\n",
            "2022-11-21 17:02:48,228 epoch 2 - iter 98/141 - loss 0.05331403 - samples/sec: 603.01 - lr: 0.000018\n",
            "2022-11-21 17:02:48,596 epoch 2 - iter 112/141 - loss 0.05291150 - samples/sec: 614.30 - lr: 0.000018\n",
            "2022-11-21 17:02:48,966 epoch 2 - iter 126/141 - loss 0.05277099 - samples/sec: 610.24 - lr: 0.000018\n",
            "2022-11-21 17:02:49,334 epoch 2 - iter 140/141 - loss 0.05265108 - samples/sec: 613.26 - lr: 0.000018\n",
            "2022-11-21 17:02:49,353 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:02:49,355 EPOCH 2 done: loss 0.0533 - lr 0.0000178\n",
            "2022-11-21 17:02:49,621 DEV : loss 0.04906433820724487 - f1-score (micro avg)  0.4657\n",
            "2022-11-21 17:02:49,625 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:02:49,627 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:02:50,000 epoch 3 - iter 14/141 - loss 0.04781942 - samples/sec: 606.74 - lr: 0.000018\n",
            "2022-11-21 17:02:50,377 epoch 3 - iter 28/141 - loss 0.04924952 - samples/sec: 599.17 - lr: 0.000017\n",
            "2022-11-21 17:02:50,747 epoch 3 - iter 42/141 - loss 0.04745179 - samples/sec: 611.51 - lr: 0.000017\n",
            "2022-11-21 17:02:51,118 epoch 3 - iter 56/141 - loss 0.04825053 - samples/sec: 608.72 - lr: 0.000017\n",
            "2022-11-21 17:02:51,490 epoch 3 - iter 70/141 - loss 0.04840634 - samples/sec: 606.85 - lr: 0.000017\n",
            "2022-11-21 17:02:51,862 epoch 3 - iter 84/141 - loss 0.04833026 - samples/sec: 608.48 - lr: 0.000016\n",
            "2022-11-21 17:02:52,235 epoch 3 - iter 98/141 - loss 0.04761250 - samples/sec: 604.29 - lr: 0.000016\n",
            "2022-11-21 17:02:52,608 epoch 3 - iter 112/141 - loss 0.04749406 - samples/sec: 605.99 - lr: 0.000016\n",
            "2022-11-21 17:02:52,983 epoch 3 - iter 126/141 - loss 0.04712896 - samples/sec: 601.80 - lr: 0.000016\n",
            "2022-11-21 17:02:53,357 epoch 3 - iter 140/141 - loss 0.04725577 - samples/sec: 603.75 - lr: 0.000016\n",
            "2022-11-21 17:02:53,394 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:02:53,398 EPOCH 3 done: loss 0.0474 - lr 0.0000156\n",
            "2022-11-21 17:02:53,714 DEV : loss 0.04683668538928032 - f1-score (micro avg)  0.4801\n",
            "2022-11-21 17:02:53,719 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:02:53,720 saving best model\n",
            "2022-11-21 17:02:53,772 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:02:54,190 epoch 4 - iter 14/141 - loss 0.04562462 - samples/sec: 539.97 - lr: 0.000015\n",
            "2022-11-21 17:02:54,613 epoch 4 - iter 28/141 - loss 0.04635127 - samples/sec: 534.98 - lr: 0.000015\n",
            "2022-11-21 17:02:55,003 epoch 4 - iter 42/141 - loss 0.04641957 - samples/sec: 579.48 - lr: 0.000015\n",
            "2022-11-21 17:02:55,423 epoch 4 - iter 56/141 - loss 0.04755603 - samples/sec: 538.04 - lr: 0.000015\n",
            "2022-11-21 17:02:55,819 epoch 4 - iter 70/141 - loss 0.04781047 - samples/sec: 569.35 - lr: 0.000014\n",
            "2022-11-21 17:02:56,245 epoch 4 - iter 84/141 - loss 0.04746264 - samples/sec: 531.52 - lr: 0.000014\n",
            "2022-11-21 17:02:56,700 epoch 4 - iter 98/141 - loss 0.04713127 - samples/sec: 496.02 - lr: 0.000014\n",
            "2022-11-21 17:02:57,216 epoch 4 - iter 112/141 - loss 0.04673765 - samples/sec: 440.90 - lr: 0.000014\n",
            "2022-11-21 17:02:57,640 epoch 4 - iter 126/141 - loss 0.04698950 - samples/sec: 532.34 - lr: 0.000014\n",
            "2022-11-21 17:02:58,060 epoch 4 - iter 140/141 - loss 0.04679503 - samples/sec: 538.93 - lr: 0.000013\n",
            "2022-11-21 17:02:58,082 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:02:58,084 EPOCH 4 done: loss 0.0475 - lr 0.0000133\n",
            "2022-11-21 17:02:58,361 DEV : loss 0.04634522274136543 - f1-score (micro avg)  0.4982\n",
            "2022-11-21 17:02:58,363 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:02:58,367 saving best model\n",
            "2022-11-21 17:02:58,416 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:02:58,827 epoch 5 - iter 14/141 - loss 0.04736581 - samples/sec: 550.84 - lr: 0.000013\n",
            "2022-11-21 17:02:59,217 epoch 5 - iter 28/141 - loss 0.04683811 - samples/sec: 578.32 - lr: 0.000013\n",
            "2022-11-21 17:02:59,638 epoch 5 - iter 42/141 - loss 0.04544280 - samples/sec: 537.73 - lr: 0.000013\n",
            "2022-11-21 17:03:00,171 epoch 5 - iter 56/141 - loss 0.04600533 - samples/sec: 423.20 - lr: 0.000012\n",
            "2022-11-21 17:03:00,593 epoch 5 - iter 70/141 - loss 0.04594578 - samples/sec: 545.60 - lr: 0.000012\n",
            "2022-11-21 17:03:01,033 epoch 5 - iter 84/141 - loss 0.04646750 - samples/sec: 514.20 - lr: 0.000012\n",
            "2022-11-21 17:03:01,455 epoch 5 - iter 98/141 - loss 0.04637830 - samples/sec: 536.39 - lr: 0.000012\n",
            "2022-11-21 17:03:01,906 epoch 5 - iter 112/141 - loss 0.04649513 - samples/sec: 500.05 - lr: 0.000012\n",
            "2022-11-21 17:03:02,328 epoch 5 - iter 126/141 - loss 0.04637279 - samples/sec: 537.17 - lr: 0.000011\n",
            "2022-11-21 17:03:02,708 epoch 5 - iter 140/141 - loss 0.04636462 - samples/sec: 594.72 - lr: 0.000011\n",
            "2022-11-21 17:03:02,729 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:03:02,732 EPOCH 5 done: loss 0.0469 - lr 0.0000111\n",
            "2022-11-21 17:03:03,000 DEV : loss 0.04609719291329384 - f1-score (micro avg)  0.5199\n",
            "2022-11-21 17:03:03,003 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:03:03,007 saving best model\n",
            "2022-11-21 17:03:03,068 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:03:03,528 epoch 6 - iter 14/141 - loss 0.04404098 - samples/sec: 490.88 - lr: 0.000011\n",
            "2022-11-21 17:03:03,926 epoch 6 - iter 28/141 - loss 0.04478460 - samples/sec: 568.44 - lr: 0.000011\n",
            "2022-11-21 17:03:04,321 epoch 6 - iter 42/141 - loss 0.04483174 - samples/sec: 571.87 - lr: 0.000010\n",
            "2022-11-21 17:03:04,749 epoch 6 - iter 56/141 - loss 0.04542293 - samples/sec: 529.14 - lr: 0.000010\n",
            "2022-11-21 17:03:05,156 epoch 6 - iter 70/141 - loss 0.04514279 - samples/sec: 555.25 - lr: 0.000010\n",
            "2022-11-21 17:03:05,579 epoch 6 - iter 84/141 - loss 0.04532088 - samples/sec: 534.27 - lr: 0.000010\n",
            "2022-11-21 17:03:06,028 epoch 6 - iter 98/141 - loss 0.04523911 - samples/sec: 503.08 - lr: 0.000010\n",
            "2022-11-21 17:03:06,518 epoch 6 - iter 112/141 - loss 0.04540151 - samples/sec: 460.61 - lr: 0.000009\n",
            "2022-11-21 17:03:06,902 epoch 6 - iter 126/141 - loss 0.04545482 - samples/sec: 588.93 - lr: 0.000009\n",
            "2022-11-21 17:03:07,323 epoch 6 - iter 140/141 - loss 0.04558561 - samples/sec: 536.64 - lr: 0.000009\n",
            "2022-11-21 17:03:07,342 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:03:07,345 EPOCH 6 done: loss 0.0457 - lr 0.0000089\n",
            "2022-11-21 17:03:07,629 DEV : loss 0.04638714715838432 - f1-score (micro avg)  0.509\n",
            "2022-11-21 17:03:07,633 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:03:07,636 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:03:08,028 epoch 7 - iter 14/141 - loss 0.04763890 - samples/sec: 578.63 - lr: 0.000009\n",
            "2022-11-21 17:03:08,424 epoch 7 - iter 28/141 - loss 0.04619213 - samples/sec: 572.33 - lr: 0.000008\n",
            "2022-11-21 17:03:08,862 epoch 7 - iter 42/141 - loss 0.04573396 - samples/sec: 515.53 - lr: 0.000008\n",
            "2022-11-21 17:03:09,310 epoch 7 - iter 56/141 - loss 0.04538817 - samples/sec: 503.26 - lr: 0.000008\n",
            "2022-11-21 17:03:09,719 epoch 7 - iter 70/141 - loss 0.04479290 - samples/sec: 551.37 - lr: 0.000008\n",
            "2022-11-21 17:03:10,126 epoch 7 - iter 84/141 - loss 0.04516387 - samples/sec: 555.09 - lr: 0.000008\n",
            "2022-11-21 17:03:10,590 epoch 7 - iter 98/141 - loss 0.04523391 - samples/sec: 486.78 - lr: 0.000007\n",
            "2022-11-21 17:03:11,085 epoch 7 - iter 112/141 - loss 0.04553992 - samples/sec: 462.07 - lr: 0.000007\n",
            "2022-11-21 17:03:11,507 epoch 7 - iter 126/141 - loss 0.04549178 - samples/sec: 536.28 - lr: 0.000007\n",
            "2022-11-21 17:03:11,906 epoch 7 - iter 140/141 - loss 0.04521370 - samples/sec: 566.60 - lr: 0.000007\n",
            "2022-11-21 17:03:11,926 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:03:11,929 EPOCH 7 done: loss 0.0457 - lr 0.0000067\n",
            "2022-11-21 17:03:12,193 DEV : loss 0.04592077061533928 - f1-score (micro avg)  0.5307\n",
            "2022-11-21 17:03:12,195 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:03:12,200 saving best model\n",
            "2022-11-21 17:03:12,242 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:03:12,676 epoch 8 - iter 14/141 - loss 0.04375301 - samples/sec: 520.60 - lr: 0.000006\n",
            "2022-11-21 17:03:13,102 epoch 8 - iter 28/141 - loss 0.04392774 - samples/sec: 530.48 - lr: 0.000006\n",
            "2022-11-21 17:03:13,555 epoch 8 - iter 42/141 - loss 0.04472893 - samples/sec: 498.75 - lr: 0.000006\n",
            "2022-11-21 17:03:14,012 epoch 8 - iter 56/141 - loss 0.04435790 - samples/sec: 493.46 - lr: 0.000006\n",
            "2022-11-21 17:03:14,421 epoch 8 - iter 70/141 - loss 0.04429766 - samples/sec: 552.64 - lr: 0.000006\n",
            "2022-11-21 17:03:14,834 epoch 8 - iter 84/141 - loss 0.04401849 - samples/sec: 547.11 - lr: 0.000005\n",
            "2022-11-21 17:03:15,261 epoch 8 - iter 98/141 - loss 0.04416136 - samples/sec: 528.76 - lr: 0.000005\n",
            "2022-11-21 17:03:15,704 epoch 8 - iter 112/141 - loss 0.04415113 - samples/sec: 510.10 - lr: 0.000005\n",
            "2022-11-21 17:03:16,143 epoch 8 - iter 126/141 - loss 0.04445755 - samples/sec: 514.34 - lr: 0.000005\n",
            "2022-11-21 17:03:16,586 epoch 8 - iter 140/141 - loss 0.04455314 - samples/sec: 510.01 - lr: 0.000004\n",
            "2022-11-21 17:03:16,612 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:03:16,614 EPOCH 8 done: loss 0.0447 - lr 0.0000044\n",
            "2022-11-21 17:03:16,871 DEV : loss 0.046003442257642746 - f1-score (micro avg)  0.5199\n",
            "2022-11-21 17:03:16,876 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:03:16,878 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:03:17,299 epoch 9 - iter 14/141 - loss 0.04325991 - samples/sec: 538.29 - lr: 0.000004\n",
            "2022-11-21 17:03:17,691 epoch 9 - iter 28/141 - loss 0.04402263 - samples/sec: 576.55 - lr: 0.000004\n",
            "2022-11-21 17:03:18,087 epoch 9 - iter 42/141 - loss 0.04428604 - samples/sec: 570.56 - lr: 0.000004\n",
            "2022-11-21 17:03:18,469 epoch 9 - iter 56/141 - loss 0.04397467 - samples/sec: 593.36 - lr: 0.000004\n",
            "2022-11-21 17:03:18,895 epoch 9 - iter 70/141 - loss 0.04385776 - samples/sec: 530.05 - lr: 0.000003\n",
            "2022-11-21 17:03:19,426 epoch 9 - iter 84/141 - loss 0.04403731 - samples/sec: 425.45 - lr: 0.000003\n",
            "2022-11-21 17:03:19,869 epoch 9 - iter 98/141 - loss 0.04417664 - samples/sec: 510.69 - lr: 0.000003\n",
            "2022-11-21 17:03:20,297 epoch 9 - iter 112/141 - loss 0.04434179 - samples/sec: 527.92 - lr: 0.000003\n",
            "2022-11-21 17:03:20,749 epoch 9 - iter 126/141 - loss 0.04440995 - samples/sec: 500.46 - lr: 0.000002\n",
            "2022-11-21 17:03:21,176 epoch 9 - iter 140/141 - loss 0.04447755 - samples/sec: 529.10 - lr: 0.000002\n",
            "2022-11-21 17:03:21,204 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:03:21,210 EPOCH 9 done: loss 0.0450 - lr 0.0000022\n",
            "2022-11-21 17:03:21,471 DEV : loss 0.04603560268878937 - f1-score (micro avg)  0.5126\n",
            "2022-11-21 17:03:21,473 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:03:21,477 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:03:21,921 epoch 10 - iter 14/141 - loss 0.04542375 - samples/sec: 508.94 - lr: 0.000002\n",
            "2022-11-21 17:03:22,342 epoch 10 - iter 28/141 - loss 0.04499946 - samples/sec: 535.87 - lr: 0.000002\n",
            "2022-11-21 17:03:22,853 epoch 10 - iter 42/141 - loss 0.04477397 - samples/sec: 442.29 - lr: 0.000002\n",
            "2022-11-21 17:03:23,279 epoch 10 - iter 56/141 - loss 0.04491395 - samples/sec: 529.72 - lr: 0.000001\n",
            "2022-11-21 17:03:23,679 epoch 10 - iter 70/141 - loss 0.04476494 - samples/sec: 566.04 - lr: 0.000001\n",
            "2022-11-21 17:03:24,059 epoch 10 - iter 84/141 - loss 0.04472150 - samples/sec: 593.19 - lr: 0.000001\n",
            "2022-11-21 17:03:24,440 epoch 10 - iter 98/141 - loss 0.04477510 - samples/sec: 592.85 - lr: 0.000001\n",
            "2022-11-21 17:03:24,845 epoch 10 - iter 112/141 - loss 0.04465906 - samples/sec: 557.70 - lr: 0.000000\n",
            "2022-11-21 17:03:25,211 epoch 10 - iter 126/141 - loss 0.04486931 - samples/sec: 617.58 - lr: 0.000000\n",
            "2022-11-21 17:03:25,591 epoch 10 - iter 140/141 - loss 0.04480185 - samples/sec: 594.74 - lr: 0.000000\n",
            "2022-11-21 17:03:25,611 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:03:25,612 EPOCH 10 done: loss 0.0451 - lr 0.0000000\n",
            "2022-11-21 17:03:25,876 DEV : loss 0.04600881412625313 - f1-score (micro avg)  0.5199\n",
            "2022-11-21 17:03:25,879 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:03:25,929 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:03:25,931 loading file resources/word-pair-test-flair/best-model.pt\n",
            "2022-11-21 17:03:28,306 No model_max_length in Tokenizer's config.json - setting it to 512. Specify desired model_max_length by passing it as attribute to embedding instance.\n",
            "2022-11-21 17:03:28,604 0.5502\t0.5502\t0.5502\t0.5502\n",
            "2022-11-21 17:03:28,606 \n",
            "Results:\n",
            "- F-score (micro) 0.5502\n",
            "- F-score (macro) 0.5194\n",
            "- Accuracy 0.5502\n",
            "\n",
            "By class:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "    entailment     0.5556    0.7576    0.6410       132\n",
            "not_entailment     0.5362    0.3162    0.3978       117\n",
            "\n",
            "     micro avg     0.5502    0.5502    0.5502       249\n",
            "     macro avg     0.5459    0.5369    0.5194       249\n",
            "  weighted avg     0.5465    0.5502    0.5268       249\n",
            "   samples avg     0.5502    0.5502    0.5502       249\n",
            "\n",
            "2022-11-21 17:03:28,609 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_score': 0.5502008032128514,\n",
              " 'dev_score_history': [0.4729241877256318,\n",
              "  0.46570397111913364,\n",
              "  0.48014440433212996,\n",
              "  0.4981949458483754,\n",
              "  0.51985559566787,\n",
              "  0.5090252707581228,\n",
              "  0.5306859205776173,\n",
              "  0.51985559566787,\n",
              "  0.5126353790613718,\n",
              "  0.51985559566787],\n",
              " 'train_loss_history': [0.07207265992294407,\n",
              "  0.053268743925784005,\n",
              "  0.047408804078019125,\n",
              "  0.04746661044986372,\n",
              "  0.04686431778908202,\n",
              "  0.045738561764618796,\n",
              "  0.04571370057685202,\n",
              "  0.04469791082652857,\n",
              "  0.0450230745639401,\n",
              "  0.045077076505093315],\n",
              " 'dev_loss_history': [tensor(0.0512, device='cuda:0'),\n",
              "  tensor(0.0491, device='cuda:0'),\n",
              "  tensor(0.0468, device='cuda:0'),\n",
              "  tensor(0.0463, device='cuda:0'),\n",
              "  tensor(0.0461, device='cuda:0'),\n",
              "  tensor(0.0464, device='cuda:0'),\n",
              "  tensor(0.0459, device='cuda:0'),\n",
              "  tensor(0.0460, device='cuda:0'),\n",
              "  tensor(0.0460, device='cuda:0'),\n",
              "  tensor(0.0460, device='cuda:0')]}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Train a model using WordEmbeddings"
      ],
      "metadata": {
        "id": "nFQQ2Q3p0035"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Issue: DocumentEmbeddings (or a sentence representation) is a single vector. Whereas WordEmbeddings is a list of word vectors. \n",
        "\n",
        "TextClassifier accepts document embeddings. Since we are using a word per sentence, we can use DocumentPoolEmbeddings (i.e. this averages a single word vector which changes only the shape of a tensor)."
      ],
      "metadata": {
        "id": "wb31JUG02TAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flair.embeddings import WordEmbeddings\n",
        "from flair.embeddings import DocumentPoolEmbeddings\n",
        "from flair.models import TextPairClassifier\n",
        "from flair.trainers import ModelTrainer\n",
        "\n",
        "# Step 2: Pick English GloVe embeddings\n",
        "embeddings = DocumentPoolEmbeddings([WordEmbeddings('en')])\n",
        "\n",
        "# Step 3: Use text pair classification model\n",
        "classifier = TextPairClassifier(document_embeddings=embeddings,\n",
        "                                label_type=label_type,\n",
        "                                label_dictionary=label_dictionary,\n",
        "                                embed_separately=True,\n",
        "                                )\n",
        "\n",
        "# Step 4: Initialize trainer and train the model\n",
        "trainer = ModelTrainer(classifier, corpus)\n",
        "\n",
        "# if you are using transformer embeddings, you can simply call trainer.fine_tune()\n",
        "trainer.train(base_path='resources/word-pair-test-flair',\n",
        "              use_final_model_for_eval=False,\n",
        "              learning_rate=0.1,\n",
        "              max_epochs=100,\n",
        "              )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMpFcWoluXhQ",
        "outputId": "7ebc7edd-a00f-4568-ff28-7e31ff274aa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-21 17:04:05,331 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:05,333 Model: \"TextPairClassifier(\n",
            "  (loss_function): CrossEntropyLoss()\n",
            "  (document_embeddings): DocumentPoolEmbeddings(\n",
            "    fine_tune_mode=none, pooling=mean\n",
            "    (embeddings): StackedEmbeddings(\n",
            "      (list_embedding_0): WordEmbeddings(\n",
            "        'en'\n",
            "        (embedding): Embedding(1000001, 300)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): Linear(in_features=600, out_features=3, bias=True)\n",
            "  (weights): None\n",
            "  (weight_tensor) None\n",
            ")\"\n",
            "2022-11-21 17:04:05,336 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:05,337 Corpus: \"Corpus: 2241 train + 277 dev + 249 test sentences\"\n",
            "2022-11-21 17:04:05,339 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:05,340 Parameters:\n",
            "2022-11-21 17:04:05,341  - learning_rate: \"0.1\"\n",
            "2022-11-21 17:04:05,343  - mini_batch_size: \"32\"\n",
            "2022-11-21 17:04:05,344  - patience: \"3\"\n",
            "2022-11-21 17:04:05,346  - anneal_factor: \"0.5\"\n",
            "2022-11-21 17:04:05,347  - max_epochs: \"100\"\n",
            "2022-11-21 17:04:05,349  - shuffle: \"True\"\n",
            "2022-11-21 17:04:05,350  - train_with_dev: \"False\"\n",
            "2022-11-21 17:04:05,352  - batch_growth_annealing: \"False\"\n",
            "2022-11-21 17:04:05,353 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:05,355 Model training base path: \"resources/word-pair-test-flair\"\n",
            "2022-11-21 17:04:05,364 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:05,366 Device: cuda:0\n",
            "2022-11-21 17:04:05,368 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:05,369 Embeddings storage mode: cpu\n",
            "2022-11-21 17:04:05,375 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:05,457 epoch 1 - iter 7/71 - loss 0.02962308 - samples/sec: 2822.11 - lr: 0.100000\n",
            "2022-11-21 17:04:05,528 epoch 1 - iter 14/71 - loss 0.02788655 - samples/sec: 3255.51 - lr: 0.100000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/flair/trainers/trainer.py:65: UserWarning: There should be no best model saved at epoch 1 except there is a model from previous trainings in your training folder. All previous best models will be deleted.\n",
            "  \"There should be no best model saved at epoch 1 except there is a model from previous trainings\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-21 17:04:05,605 epoch 1 - iter 21/71 - loss 0.02686282 - samples/sec: 3015.11 - lr: 0.100000\n",
            "2022-11-21 17:04:05,680 epoch 1 - iter 28/71 - loss 0.02606895 - samples/sec: 3096.57 - lr: 0.100000\n",
            "2022-11-21 17:04:05,750 epoch 1 - iter 35/71 - loss 0.02548538 - samples/sec: 3285.58 - lr: 0.100000\n",
            "2022-11-21 17:04:05,823 epoch 1 - iter 42/71 - loss 0.02511740 - samples/sec: 3201.32 - lr: 0.100000\n",
            "2022-11-21 17:04:05,894 epoch 1 - iter 49/71 - loss 0.02478288 - samples/sec: 3272.58 - lr: 0.100000\n",
            "2022-11-21 17:04:05,966 epoch 1 - iter 56/71 - loss 0.02450089 - samples/sec: 3232.60 - lr: 0.100000\n",
            "2022-11-21 17:04:06,041 epoch 1 - iter 63/71 - loss 0.02430677 - samples/sec: 3069.20 - lr: 0.100000\n",
            "2022-11-21 17:04:06,116 epoch 1 - iter 70/71 - loss 0.02415550 - samples/sec: 3113.38 - lr: 0.100000\n",
            "2022-11-21 17:04:06,120 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:06,122 EPOCH 1 done: loss 0.0246 - lr 0.1000000\n",
            "2022-11-21 17:04:06,235 DEV : loss 0.023937370628118515 - f1-score (micro avg)  0.4657\n",
            "2022-11-21 17:04:06,240 BAD EPOCHS (no improvement): 0\n",
            "2022-11-21 17:04:06,243 saving best model\n",
            "2022-11-21 17:04:11,662 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:11,728 epoch 2 - iter 7/71 - loss 0.02271705 - samples/sec: 3598.44 - lr: 0.100000\n",
            "2022-11-21 17:04:11,791 epoch 2 - iter 14/71 - loss 0.02240077 - samples/sec: 3739.24 - lr: 0.100000\n",
            "2022-11-21 17:04:11,852 epoch 2 - iter 21/71 - loss 0.02225509 - samples/sec: 3840.39 - lr: 0.100000\n",
            "2022-11-21 17:04:11,913 epoch 2 - iter 28/71 - loss 0.02222248 - samples/sec: 3818.15 - lr: 0.100000\n",
            "2022-11-21 17:04:11,974 epoch 2 - iter 35/71 - loss 0.02218859 - samples/sec: 3817.34 - lr: 0.100000\n",
            "2022-11-21 17:04:12,040 epoch 2 - iter 42/71 - loss 0.02219492 - samples/sec: 3684.97 - lr: 0.100000\n",
            "2022-11-21 17:04:12,114 epoch 2 - iter 49/71 - loss 0.02220125 - samples/sec: 3322.09 - lr: 0.100000\n",
            "2022-11-21 17:04:12,176 epoch 2 - iter 56/71 - loss 0.02215081 - samples/sec: 3784.39 - lr: 0.100000\n",
            "2022-11-21 17:04:12,238 epoch 2 - iter 63/71 - loss 0.02214892 - samples/sec: 3762.94 - lr: 0.100000\n",
            "2022-11-21 17:04:12,302 epoch 2 - iter 70/71 - loss 0.02209073 - samples/sec: 3720.39 - lr: 0.100000\n",
            "2022-11-21 17:04:12,306 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:12,307 EPOCH 2 done: loss 0.0224 - lr 0.1000000\n",
            "2022-11-21 17:04:12,415 DEV : loss 0.022815879434347153 - f1-score (micro avg)  0.556\n",
            "2022-11-21 17:04:12,421 BAD EPOCHS (no improvement): 0\n",
            "2022-11-21 17:04:12,422 saving best model\n",
            "2022-11-21 17:04:18,345 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:18,411 epoch 3 - iter 7/71 - loss 0.02162096 - samples/sec: 3610.44 - lr: 0.100000\n",
            "2022-11-21 17:04:18,475 epoch 3 - iter 14/71 - loss 0.02167654 - samples/sec: 3652.78 - lr: 0.100000\n",
            "2022-11-21 17:04:18,538 epoch 3 - iter 21/71 - loss 0.02167675 - samples/sec: 3666.95 - lr: 0.100000\n",
            "2022-11-21 17:04:18,601 epoch 3 - iter 28/71 - loss 0.02160383 - samples/sec: 3776.94 - lr: 0.100000\n",
            "2022-11-21 17:04:18,666 epoch 3 - iter 35/71 - loss 0.02158795 - samples/sec: 3556.42 - lr: 0.100000\n",
            "2022-11-21 17:04:18,730 epoch 3 - iter 42/71 - loss 0.02164483 - samples/sec: 3599.63 - lr: 0.100000\n",
            "2022-11-21 17:04:18,793 epoch 3 - iter 49/71 - loss 0.02167264 - samples/sec: 3682.08 - lr: 0.100000\n",
            "2022-11-21 17:04:18,855 epoch 3 - iter 56/71 - loss 0.02162030 - samples/sec: 3815.11 - lr: 0.100000\n",
            "2022-11-21 17:04:18,918 epoch 3 - iter 63/71 - loss 0.02165854 - samples/sec: 3652.25 - lr: 0.100000\n",
            "2022-11-21 17:04:18,980 epoch 3 - iter 70/71 - loss 0.02163060 - samples/sec: 3759.66 - lr: 0.100000\n",
            "2022-11-21 17:04:18,984 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:18,987 EPOCH 3 done: loss 0.0220 - lr 0.1000000\n",
            "2022-11-21 17:04:19,092 DEV : loss 0.0227860938757658 - f1-score (micro avg)  0.5415\n",
            "2022-11-21 17:04:19,096 BAD EPOCHS (no improvement): 1\n",
            "2022-11-21 17:04:19,099 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:19,166 epoch 4 - iter 7/71 - loss 0.02119915 - samples/sec: 3524.41 - lr: 0.100000\n",
            "2022-11-21 17:04:19,233 epoch 4 - iter 14/71 - loss 0.02124075 - samples/sec: 3632.37 - lr: 0.100000\n",
            "2022-11-21 17:04:19,299 epoch 4 - iter 21/71 - loss 0.02144030 - samples/sec: 3515.71 - lr: 0.100000\n",
            "2022-11-21 17:04:19,357 epoch 4 - iter 28/71 - loss 0.02139510 - samples/sec: 3979.65 - lr: 0.100000\n",
            "2022-11-21 17:04:19,416 epoch 4 - iter 35/71 - loss 0.02137165 - samples/sec: 3992.96 - lr: 0.100000\n",
            "2022-11-21 17:04:19,479 epoch 4 - iter 42/71 - loss 0.02141042 - samples/sec: 3672.60 - lr: 0.100000\n",
            "2022-11-21 17:04:19,544 epoch 4 - iter 49/71 - loss 0.02140689 - samples/sec: 3616.03 - lr: 0.100000\n",
            "2022-11-21 17:04:19,606 epoch 4 - iter 56/71 - loss 0.02139852 - samples/sec: 3797.24 - lr: 0.100000\n",
            "2022-11-21 17:04:19,669 epoch 4 - iter 63/71 - loss 0.02138239 - samples/sec: 3734.67 - lr: 0.100000\n",
            "2022-11-21 17:04:19,730 epoch 4 - iter 70/71 - loss 0.02134802 - samples/sec: 3776.97 - lr: 0.100000\n",
            "2022-11-21 17:04:19,734 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:19,736 EPOCH 4 done: loss 0.0217 - lr 0.1000000\n",
            "2022-11-21 17:04:19,837 DEV : loss 0.022817252203822136 - f1-score (micro avg)  0.5848\n",
            "2022-11-21 17:04:19,842 BAD EPOCHS (no improvement): 0\n",
            "2022-11-21 17:04:19,844 saving best model\n",
            "2022-11-21 17:04:25,662 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:25,746 epoch 5 - iter 7/71 - loss 0.02120489 - samples/sec: 2810.69 - lr: 0.100000\n",
            "2022-11-21 17:04:25,808 epoch 5 - iter 14/71 - loss 0.02151606 - samples/sec: 3815.17 - lr: 0.100000\n",
            "2022-11-21 17:04:25,869 epoch 5 - iter 21/71 - loss 0.02119603 - samples/sec: 3813.13 - lr: 0.100000\n",
            "2022-11-21 17:04:25,930 epoch 5 - iter 28/71 - loss 0.02117885 - samples/sec: 3789.75 - lr: 0.100000\n",
            "2022-11-21 17:04:25,992 epoch 5 - iter 35/71 - loss 0.02106960 - samples/sec: 3799.34 - lr: 0.100000\n",
            "2022-11-21 17:04:26,054 epoch 5 - iter 42/71 - loss 0.02114515 - samples/sec: 3783.51 - lr: 0.100000\n",
            "2022-11-21 17:04:26,116 epoch 5 - iter 49/71 - loss 0.02114910 - samples/sec: 3753.88 - lr: 0.100000\n",
            "2022-11-21 17:04:26,183 epoch 5 - iter 56/71 - loss 0.02115119 - samples/sec: 3491.50 - lr: 0.100000\n",
            "2022-11-21 17:04:26,245 epoch 5 - iter 63/71 - loss 0.02110303 - samples/sec: 3750.40 - lr: 0.100000\n",
            "2022-11-21 17:04:26,310 epoch 5 - iter 70/71 - loss 0.02110425 - samples/sec: 3585.87 - lr: 0.100000\n",
            "2022-11-21 17:04:26,315 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:26,316 EPOCH 5 done: loss 0.0215 - lr 0.1000000\n",
            "2022-11-21 17:04:26,420 DEV : loss 0.022666580975055695 - f1-score (micro avg)  0.574\n",
            "2022-11-21 17:04:26,423 BAD EPOCHS (no improvement): 1\n",
            "2022-11-21 17:04:26,427 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:26,489 epoch 6 - iter 7/71 - loss 0.02083518 - samples/sec: 3775.92 - lr: 0.100000\n",
            "2022-11-21 17:04:26,554 epoch 6 - iter 14/71 - loss 0.02098374 - samples/sec: 3606.67 - lr: 0.100000\n",
            "2022-11-21 17:04:26,620 epoch 6 - iter 21/71 - loss 0.02075555 - samples/sec: 3523.25 - lr: 0.100000\n",
            "2022-11-21 17:04:26,683 epoch 6 - iter 28/71 - loss 0.02083036 - samples/sec: 3674.19 - lr: 0.100000\n",
            "2022-11-21 17:04:26,747 epoch 6 - iter 35/71 - loss 0.02068004 - samples/sec: 3600.56 - lr: 0.100000\n",
            "2022-11-21 17:04:26,816 epoch 6 - iter 42/71 - loss 0.02074173 - samples/sec: 3378.60 - lr: 0.100000\n",
            "2022-11-21 17:04:26,877 epoch 6 - iter 49/71 - loss 0.02086100 - samples/sec: 3857.78 - lr: 0.100000\n",
            "2022-11-21 17:04:26,937 epoch 6 - iter 56/71 - loss 0.02091868 - samples/sec: 3842.38 - lr: 0.100000\n",
            "2022-11-21 17:04:26,997 epoch 6 - iter 63/71 - loss 0.02089488 - samples/sec: 3934.78 - lr: 0.100000\n",
            "2022-11-21 17:04:27,060 epoch 6 - iter 70/71 - loss 0.02090043 - samples/sec: 3669.83 - lr: 0.100000\n",
            "2022-11-21 17:04:27,064 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:27,065 EPOCH 6 done: loss 0.0213 - lr 0.1000000\n",
            "2022-11-21 17:04:27,165 DEV : loss 0.02337842620909214 - f1-score (micro avg)  0.5415\n",
            "2022-11-21 17:04:27,170 BAD EPOCHS (no improvement): 2\n",
            "2022-11-21 17:04:27,173 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:27,232 epoch 7 - iter 7/71 - loss 0.02122957 - samples/sec: 3976.97 - lr: 0.100000\n",
            "2022-11-21 17:04:27,292 epoch 7 - iter 14/71 - loss 0.02092147 - samples/sec: 3840.15 - lr: 0.100000\n",
            "2022-11-21 17:04:27,353 epoch 7 - iter 21/71 - loss 0.02100772 - samples/sec: 3838.91 - lr: 0.100000\n",
            "2022-11-21 17:04:27,412 epoch 7 - iter 28/71 - loss 0.02100261 - samples/sec: 3918.13 - lr: 0.100000\n",
            "2022-11-21 17:04:27,475 epoch 7 - iter 35/71 - loss 0.02096734 - samples/sec: 3712.26 - lr: 0.100000\n",
            "2022-11-21 17:04:27,537 epoch 7 - iter 42/71 - loss 0.02083094 - samples/sec: 3786.04 - lr: 0.100000\n",
            "2022-11-21 17:04:27,600 epoch 7 - iter 49/71 - loss 0.02081167 - samples/sec: 3705.14 - lr: 0.100000\n",
            "2022-11-21 17:04:27,660 epoch 7 - iter 56/71 - loss 0.02080067 - samples/sec: 3856.15 - lr: 0.100000\n",
            "2022-11-21 17:04:27,722 epoch 7 - iter 63/71 - loss 0.02079569 - samples/sec: 3804.13 - lr: 0.100000\n",
            "2022-11-21 17:04:27,782 epoch 7 - iter 70/71 - loss 0.02081358 - samples/sec: 3891.71 - lr: 0.100000\n",
            "2022-11-21 17:04:27,786 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:27,787 EPOCH 7 done: loss 0.0211 - lr 0.1000000\n",
            "2022-11-21 17:04:27,893 DEV : loss 0.02296355925500393 - f1-score (micro avg)  0.4838\n",
            "2022-11-21 17:04:27,898 BAD EPOCHS (no improvement): 3\n",
            "2022-11-21 17:04:27,900 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:27,961 epoch 8 - iter 7/71 - loss 0.02036750 - samples/sec: 3856.58 - lr: 0.100000\n",
            "2022-11-21 17:04:28,023 epoch 8 - iter 14/71 - loss 0.02044102 - samples/sec: 3771.28 - lr: 0.100000\n",
            "2022-11-21 17:04:28,084 epoch 8 - iter 21/71 - loss 0.02052785 - samples/sec: 3847.92 - lr: 0.100000\n",
            "2022-11-21 17:04:28,144 epoch 8 - iter 28/71 - loss 0.02044922 - samples/sec: 3839.09 - lr: 0.100000\n",
            "2022-11-21 17:04:28,205 epoch 8 - iter 35/71 - loss 0.02044231 - samples/sec: 3851.98 - lr: 0.100000\n",
            "2022-11-21 17:04:28,270 epoch 8 - iter 42/71 - loss 0.02043623 - samples/sec: 3569.85 - lr: 0.100000\n",
            "2022-11-21 17:04:28,334 epoch 8 - iter 49/71 - loss 0.02056260 - samples/sec: 3619.14 - lr: 0.100000\n",
            "2022-11-21 17:04:28,394 epoch 8 - iter 56/71 - loss 0.02060017 - samples/sec: 3881.35 - lr: 0.100000\n",
            "2022-11-21 17:04:28,455 epoch 8 - iter 63/71 - loss 0.02061655 - samples/sec: 3846.44 - lr: 0.100000\n",
            "2022-11-21 17:04:28,517 epoch 8 - iter 70/71 - loss 0.02061740 - samples/sec: 3740.71 - lr: 0.100000\n",
            "2022-11-21 17:04:28,522 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:28,523 EPOCH 8 done: loss 0.0209 - lr 0.1000000\n",
            "2022-11-21 17:04:28,626 DEV : loss 0.023539429530501366 - f1-score (micro avg)  0.4838\n",
            "Epoch     8: reducing learning rate of group 0 to 5.0000e-02.\n",
            "2022-11-21 17:04:28,630 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:04:28,634 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:28,695 epoch 9 - iter 7/71 - loss 0.02061681 - samples/sec: 3869.65 - lr: 0.050000\n",
            "2022-11-21 17:04:28,754 epoch 9 - iter 14/71 - loss 0.02059278 - samples/sec: 3972.82 - lr: 0.050000\n",
            "2022-11-21 17:04:28,816 epoch 9 - iter 21/71 - loss 0.02061004 - samples/sec: 3713.36 - lr: 0.050000\n",
            "2022-11-21 17:04:28,876 epoch 9 - iter 28/71 - loss 0.02049609 - samples/sec: 3922.28 - lr: 0.050000\n",
            "2022-11-21 17:04:28,935 epoch 9 - iter 35/71 - loss 0.02044884 - samples/sec: 3975.68 - lr: 0.050000\n",
            "2022-11-21 17:04:28,994 epoch 9 - iter 42/71 - loss 0.02050779 - samples/sec: 3958.39 - lr: 0.050000\n",
            "2022-11-21 17:04:29,052 epoch 9 - iter 49/71 - loss 0.02051102 - samples/sec: 3959.56 - lr: 0.050000\n",
            "2022-11-21 17:04:29,113 epoch 9 - iter 56/71 - loss 0.02046445 - samples/sec: 3846.69 - lr: 0.050000\n",
            "2022-11-21 17:04:29,175 epoch 9 - iter 63/71 - loss 0.02051565 - samples/sec: 3745.11 - lr: 0.050000\n",
            "2022-11-21 17:04:29,234 epoch 9 - iter 70/71 - loss 0.02049573 - samples/sec: 3945.41 - lr: 0.050000\n",
            "2022-11-21 17:04:29,238 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:29,239 EPOCH 9 done: loss 0.0206 - lr 0.0500000\n",
            "2022-11-21 17:04:29,344 DEV : loss 0.022819334641098976 - f1-score (micro avg)  0.5199\n",
            "2022-11-21 17:04:29,350 BAD EPOCHS (no improvement): 1\n",
            "2022-11-21 17:04:29,352 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:29,418 epoch 10 - iter 7/71 - loss 0.02068570 - samples/sec: 3603.99 - lr: 0.050000\n",
            "2022-11-21 17:04:29,482 epoch 10 - iter 14/71 - loss 0.02065038 - samples/sec: 3658.35 - lr: 0.050000\n",
            "2022-11-21 17:04:29,550 epoch 10 - iter 21/71 - loss 0.02063839 - samples/sec: 3479.65 - lr: 0.050000\n",
            "2022-11-21 17:04:29,613 epoch 10 - iter 28/71 - loss 0.02042034 - samples/sec: 3743.38 - lr: 0.050000\n",
            "2022-11-21 17:04:29,675 epoch 10 - iter 35/71 - loss 0.02032741 - samples/sec: 3735.44 - lr: 0.050000\n",
            "2022-11-21 17:04:29,735 epoch 10 - iter 42/71 - loss 0.02028840 - samples/sec: 3890.13 - lr: 0.050000\n",
            "2022-11-21 17:04:29,799 epoch 10 - iter 49/71 - loss 0.02035064 - samples/sec: 3646.54 - lr: 0.050000\n",
            "2022-11-21 17:04:29,861 epoch 10 - iter 56/71 - loss 0.02035707 - samples/sec: 3796.55 - lr: 0.050000\n",
            "2022-11-21 17:04:29,922 epoch 10 - iter 63/71 - loss 0.02032230 - samples/sec: 3849.86 - lr: 0.050000\n",
            "2022-11-21 17:04:29,985 epoch 10 - iter 70/71 - loss 0.02037332 - samples/sec: 3693.91 - lr: 0.050000\n",
            "2022-11-21 17:04:29,989 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:29,991 EPOCH 10 done: loss 0.0208 - lr 0.0500000\n",
            "2022-11-21 17:04:30,093 DEV : loss 0.022970303893089294 - f1-score (micro avg)  0.5271\n",
            "2022-11-21 17:04:30,099 BAD EPOCHS (no improvement): 2\n",
            "2022-11-21 17:04:30,101 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:30,163 epoch 11 - iter 7/71 - loss 0.02009074 - samples/sec: 3794.09 - lr: 0.050000\n",
            "2022-11-21 17:04:30,225 epoch 11 - iter 14/71 - loss 0.02037268 - samples/sec: 3770.96 - lr: 0.050000\n",
            "2022-11-21 17:04:30,290 epoch 11 - iter 21/71 - loss 0.02032112 - samples/sec: 3640.92 - lr: 0.050000\n",
            "2022-11-21 17:04:30,354 epoch 11 - iter 28/71 - loss 0.02026283 - samples/sec: 3630.76 - lr: 0.050000\n",
            "2022-11-21 17:04:30,417 epoch 11 - iter 35/71 - loss 0.02027408 - samples/sec: 3666.35 - lr: 0.050000\n",
            "2022-11-21 17:04:30,477 epoch 11 - iter 42/71 - loss 0.02024612 - samples/sec: 3889.90 - lr: 0.050000\n",
            "2022-11-21 17:04:30,540 epoch 11 - iter 49/71 - loss 0.02019489 - samples/sec: 3704.31 - lr: 0.050000\n",
            "2022-11-21 17:04:30,602 epoch 11 - iter 56/71 - loss 0.02024529 - samples/sec: 3742.17 - lr: 0.050000\n",
            "2022-11-21 17:04:30,664 epoch 11 - iter 63/71 - loss 0.02029495 - samples/sec: 3718.12 - lr: 0.050000\n",
            "2022-11-21 17:04:30,726 epoch 11 - iter 70/71 - loss 0.02034062 - samples/sec: 3780.25 - lr: 0.050000\n",
            "2022-11-21 17:04:30,730 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:30,731 EPOCH 11 done: loss 0.0207 - lr 0.0500000\n",
            "2022-11-21 17:04:30,831 DEV : loss 0.02256959117949009 - f1-score (micro avg)  0.5343\n",
            "2022-11-21 17:04:30,836 BAD EPOCHS (no improvement): 3\n",
            "2022-11-21 17:04:30,839 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:30,905 epoch 12 - iter 7/71 - loss 0.02054738 - samples/sec: 3553.24 - lr: 0.050000\n",
            "2022-11-21 17:04:30,966 epoch 12 - iter 14/71 - loss 0.02061349 - samples/sec: 3838.25 - lr: 0.050000\n",
            "2022-11-21 17:04:31,026 epoch 12 - iter 21/71 - loss 0.02067539 - samples/sec: 3909.96 - lr: 0.050000\n",
            "2022-11-21 17:04:31,086 epoch 12 - iter 28/71 - loss 0.02057368 - samples/sec: 3833.96 - lr: 0.050000\n",
            "2022-11-21 17:04:31,148 epoch 12 - iter 35/71 - loss 0.02050874 - samples/sec: 3782.82 - lr: 0.050000\n",
            "2022-11-21 17:04:31,209 epoch 12 - iter 42/71 - loss 0.02049434 - samples/sec: 3880.55 - lr: 0.050000\n",
            "2022-11-21 17:04:31,272 epoch 12 - iter 49/71 - loss 0.02043280 - samples/sec: 3713.48 - lr: 0.050000\n",
            "2022-11-21 17:04:31,331 epoch 12 - iter 56/71 - loss 0.02031946 - samples/sec: 3919.37 - lr: 0.050000\n",
            "2022-11-21 17:04:31,393 epoch 12 - iter 63/71 - loss 0.02029702 - samples/sec: 3840.28 - lr: 0.050000\n",
            "2022-11-21 17:04:31,454 epoch 12 - iter 70/71 - loss 0.02023670 - samples/sec: 3819.50 - lr: 0.050000\n",
            "2022-11-21 17:04:31,458 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:31,459 EPOCH 12 done: loss 0.0206 - lr 0.0500000\n",
            "2022-11-21 17:04:31,562 DEV : loss 0.02257739193737507 - f1-score (micro avg)  0.5415\n",
            "Epoch    12: reducing learning rate of group 0 to 2.5000e-02.\n",
            "2022-11-21 17:04:31,566 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:04:31,570 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:31,633 epoch 13 - iter 7/71 - loss 0.02100024 - samples/sec: 3728.58 - lr: 0.025000\n",
            "2022-11-21 17:04:31,696 epoch 13 - iter 14/71 - loss 0.02051688 - samples/sec: 3716.28 - lr: 0.025000\n",
            "2022-11-21 17:04:31,758 epoch 13 - iter 21/71 - loss 0.02035215 - samples/sec: 3776.40 - lr: 0.025000\n",
            "2022-11-21 17:04:31,819 epoch 13 - iter 28/71 - loss 0.02014829 - samples/sec: 3842.90 - lr: 0.025000\n",
            "2022-11-21 17:04:31,879 epoch 13 - iter 35/71 - loss 0.02017805 - samples/sec: 3878.02 - lr: 0.025000\n",
            "2022-11-21 17:04:31,942 epoch 13 - iter 42/71 - loss 0.02026824 - samples/sec: 3629.99 - lr: 0.025000\n",
            "2022-11-21 17:04:32,003 epoch 13 - iter 49/71 - loss 0.02019680 - samples/sec: 3854.74 - lr: 0.025000\n",
            "2022-11-21 17:04:32,063 epoch 13 - iter 56/71 - loss 0.02022814 - samples/sec: 3860.81 - lr: 0.025000\n",
            "2022-11-21 17:04:32,125 epoch 13 - iter 63/71 - loss 0.02019581 - samples/sec: 3733.44 - lr: 0.025000\n",
            "2022-11-21 17:04:32,188 epoch 13 - iter 70/71 - loss 0.02017066 - samples/sec: 3700.97 - lr: 0.025000\n",
            "2022-11-21 17:04:32,192 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:32,194 EPOCH 13 done: loss 0.0205 - lr 0.0250000\n",
            "2022-11-21 17:04:32,294 DEV : loss 0.02272873744368553 - f1-score (micro avg)  0.5451\n",
            "2022-11-21 17:04:32,298 BAD EPOCHS (no improvement): 1\n",
            "2022-11-21 17:04:32,302 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:32,363 epoch 14 - iter 7/71 - loss 0.01979363 - samples/sec: 3836.12 - lr: 0.025000\n",
            "2022-11-21 17:04:32,424 epoch 14 - iter 14/71 - loss 0.01984318 - samples/sec: 3820.90 - lr: 0.025000\n",
            "2022-11-21 17:04:32,486 epoch 14 - iter 21/71 - loss 0.01991717 - samples/sec: 3800.14 - lr: 0.025000\n",
            "2022-11-21 17:04:32,550 epoch 14 - iter 28/71 - loss 0.01988130 - samples/sec: 3645.64 - lr: 0.025000\n",
            "2022-11-21 17:04:32,609 epoch 14 - iter 35/71 - loss 0.02007207 - samples/sec: 3864.83 - lr: 0.025000\n",
            "2022-11-21 17:04:32,672 epoch 14 - iter 42/71 - loss 0.02012068 - samples/sec: 3720.99 - lr: 0.025000\n",
            "2022-11-21 17:04:32,733 epoch 14 - iter 49/71 - loss 0.02005547 - samples/sec: 3810.54 - lr: 0.025000\n",
            "2022-11-21 17:04:32,792 epoch 14 - iter 56/71 - loss 0.02006629 - samples/sec: 3921.99 - lr: 0.025000\n",
            "2022-11-21 17:04:32,854 epoch 14 - iter 63/71 - loss 0.02008379 - samples/sec: 3789.60 - lr: 0.025000\n",
            "2022-11-21 17:04:32,915 epoch 14 - iter 70/71 - loss 0.02013799 - samples/sec: 3809.29 - lr: 0.025000\n",
            "2022-11-21 17:04:32,919 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:32,921 EPOCH 14 done: loss 0.0203 - lr 0.0250000\n",
            "2022-11-21 17:04:33,022 DEV : loss 0.022634431719779968 - f1-score (micro avg)  0.5235\n",
            "2022-11-21 17:04:33,027 BAD EPOCHS (no improvement): 2\n",
            "2022-11-21 17:04:33,029 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:33,093 epoch 15 - iter 7/71 - loss 0.01997357 - samples/sec: 3703.89 - lr: 0.025000\n",
            "2022-11-21 17:04:33,154 epoch 15 - iter 14/71 - loss 0.02006482 - samples/sec: 3825.61 - lr: 0.025000\n",
            "2022-11-21 17:04:33,220 epoch 15 - iter 21/71 - loss 0.02012663 - samples/sec: 3535.33 - lr: 0.025000\n",
            "2022-11-21 17:04:33,283 epoch 15 - iter 28/71 - loss 0.02011906 - samples/sec: 3760.89 - lr: 0.025000\n",
            "2022-11-21 17:04:33,344 epoch 15 - iter 35/71 - loss 0.02024241 - samples/sec: 3798.10 - lr: 0.025000\n",
            "2022-11-21 17:04:33,406 epoch 15 - iter 42/71 - loss 0.02010109 - samples/sec: 3797.25 - lr: 0.025000\n",
            "2022-11-21 17:04:33,468 epoch 15 - iter 49/71 - loss 0.02013322 - samples/sec: 3753.77 - lr: 0.025000\n",
            "2022-11-21 17:04:33,533 epoch 15 - iter 56/71 - loss 0.02019489 - samples/sec: 3573.38 - lr: 0.025000\n",
            "2022-11-21 17:04:33,596 epoch 15 - iter 63/71 - loss 0.02014010 - samples/sec: 3748.74 - lr: 0.025000\n",
            "2022-11-21 17:04:33,659 epoch 15 - iter 70/71 - loss 0.02009694 - samples/sec: 3712.24 - lr: 0.025000\n",
            "2022-11-21 17:04:33,662 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:33,663 EPOCH 15 done: loss 0.0205 - lr 0.0250000\n",
            "2022-11-21 17:04:33,769 DEV : loss 0.022735759615898132 - f1-score (micro avg)  0.5451\n",
            "2022-11-21 17:04:33,773 BAD EPOCHS (no improvement): 3\n",
            "2022-11-21 17:04:33,776 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:33,837 epoch 16 - iter 7/71 - loss 0.01997799 - samples/sec: 3856.53 - lr: 0.025000\n",
            "2022-11-21 17:04:33,897 epoch 16 - iter 14/71 - loss 0.02004098 - samples/sec: 3867.02 - lr: 0.025000\n",
            "2022-11-21 17:04:33,957 epoch 16 - iter 21/71 - loss 0.01990057 - samples/sec: 3888.52 - lr: 0.025000\n",
            "2022-11-21 17:04:34,016 epoch 16 - iter 28/71 - loss 0.01980944 - samples/sec: 3932.84 - lr: 0.025000\n",
            "2022-11-21 17:04:34,077 epoch 16 - iter 35/71 - loss 0.01984529 - samples/sec: 3815.95 - lr: 0.025000\n",
            "2022-11-21 17:04:34,136 epoch 16 - iter 42/71 - loss 0.01987509 - samples/sec: 3901.18 - lr: 0.025000\n",
            "2022-11-21 17:04:34,195 epoch 16 - iter 49/71 - loss 0.01999305 - samples/sec: 3914.00 - lr: 0.025000\n",
            "2022-11-21 17:04:34,258 epoch 16 - iter 56/71 - loss 0.02001119 - samples/sec: 3711.49 - lr: 0.025000\n",
            "2022-11-21 17:04:34,319 epoch 16 - iter 63/71 - loss 0.02005983 - samples/sec: 3831.76 - lr: 0.025000\n",
            "2022-11-21 17:04:34,379 epoch 16 - iter 70/71 - loss 0.02008196 - samples/sec: 3881.35 - lr: 0.025000\n",
            "2022-11-21 17:04:34,384 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:34,385 EPOCH 16 done: loss 0.0204 - lr 0.0250000\n",
            "2022-11-21 17:04:34,489 DEV : loss 0.022621940821409225 - f1-score (micro avg)  0.5235\n",
            "Epoch    16: reducing learning rate of group 0 to 1.2500e-02.\n",
            "2022-11-21 17:04:34,494 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:04:34,497 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:34,560 epoch 17 - iter 7/71 - loss 0.02076575 - samples/sec: 3807.75 - lr: 0.012500\n",
            "2022-11-21 17:04:34,621 epoch 17 - iter 14/71 - loss 0.02027625 - samples/sec: 3797.50 - lr: 0.012500\n",
            "2022-11-21 17:04:34,681 epoch 17 - iter 21/71 - loss 0.02018773 - samples/sec: 3901.97 - lr: 0.012500\n",
            "2022-11-21 17:04:34,746 epoch 17 - iter 28/71 - loss 0.02011862 - samples/sec: 3545.19 - lr: 0.012500\n",
            "2022-11-21 17:04:34,808 epoch 17 - iter 35/71 - loss 0.02010269 - samples/sec: 3687.49 - lr: 0.012500\n",
            "2022-11-21 17:04:34,871 epoch 17 - iter 42/71 - loss 0.02000051 - samples/sec: 3765.31 - lr: 0.012500\n",
            "2022-11-21 17:04:34,933 epoch 17 - iter 49/71 - loss 0.01997903 - samples/sec: 3748.74 - lr: 0.012500\n",
            "2022-11-21 17:04:34,995 epoch 17 - iter 56/71 - loss 0.01996877 - samples/sec: 3746.14 - lr: 0.012500\n",
            "2022-11-21 17:04:35,057 epoch 17 - iter 63/71 - loss 0.01997136 - samples/sec: 3772.95 - lr: 0.012500\n",
            "2022-11-21 17:04:35,119 epoch 17 - iter 70/71 - loss 0.02002645 - samples/sec: 3802.46 - lr: 0.012500\n",
            "2022-11-21 17:04:35,123 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:35,124 EPOCH 17 done: loss 0.0204 - lr 0.0125000\n",
            "2022-11-21 17:04:35,229 DEV : loss 0.0227312371134758 - f1-score (micro avg)  0.5415\n",
            "2022-11-21 17:04:35,235 BAD EPOCHS (no improvement): 1\n",
            "2022-11-21 17:04:35,236 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:35,299 epoch 18 - iter 7/71 - loss 0.02014716 - samples/sec: 3833.99 - lr: 0.012500\n",
            "2022-11-21 17:04:35,361 epoch 18 - iter 14/71 - loss 0.01988404 - samples/sec: 3783.16 - lr: 0.012500\n",
            "2022-11-21 17:04:35,422 epoch 18 - iter 21/71 - loss 0.01996770 - samples/sec: 3816.60 - lr: 0.012500\n",
            "2022-11-21 17:04:35,484 epoch 18 - iter 28/71 - loss 0.01988637 - samples/sec: 3804.80 - lr: 0.012500\n",
            "2022-11-21 17:04:35,548 epoch 18 - iter 35/71 - loss 0.01986214 - samples/sec: 3616.63 - lr: 0.012500\n",
            "2022-11-21 17:04:35,609 epoch 18 - iter 42/71 - loss 0.01988804 - samples/sec: 3871.42 - lr: 0.012500\n",
            "2022-11-21 17:04:35,671 epoch 18 - iter 49/71 - loss 0.01997205 - samples/sec: 3768.66 - lr: 0.012500\n",
            "2022-11-21 17:04:35,735 epoch 18 - iter 56/71 - loss 0.02007815 - samples/sec: 3606.45 - lr: 0.012500\n",
            "2022-11-21 17:04:35,800 epoch 18 - iter 63/71 - loss 0.02002386 - samples/sec: 3590.92 - lr: 0.012500\n",
            "2022-11-21 17:04:35,860 epoch 18 - iter 70/71 - loss 0.02001748 - samples/sec: 3898.38 - lr: 0.012500\n",
            "2022-11-21 17:04:35,864 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:35,866 EPOCH 18 done: loss 0.0204 - lr 0.0125000\n",
            "2022-11-21 17:04:35,966 DEV : loss 0.022715996950864792 - f1-score (micro avg)  0.5379\n",
            "2022-11-21 17:04:35,970 BAD EPOCHS (no improvement): 2\n",
            "2022-11-21 17:04:35,973 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:36,034 epoch 19 - iter 7/71 - loss 0.01924051 - samples/sec: 3804.48 - lr: 0.012500\n",
            "2022-11-21 17:04:36,099 epoch 19 - iter 14/71 - loss 0.01996607 - samples/sec: 3627.59 - lr: 0.012500\n",
            "2022-11-21 17:04:36,159 epoch 19 - iter 21/71 - loss 0.01985365 - samples/sec: 3858.10 - lr: 0.012500\n",
            "2022-11-21 17:04:36,222 epoch 19 - iter 28/71 - loss 0.02009826 - samples/sec: 3678.78 - lr: 0.012500\n",
            "2022-11-21 17:04:36,285 epoch 19 - iter 35/71 - loss 0.02011895 - samples/sec: 3722.14 - lr: 0.012500\n",
            "2022-11-21 17:04:36,357 epoch 19 - iter 42/71 - loss 0.02016013 - samples/sec: 3205.16 - lr: 0.012500\n",
            "2022-11-21 17:04:36,431 epoch 19 - iter 49/71 - loss 0.02011948 - samples/sec: 3205.39 - lr: 0.012500\n",
            "2022-11-21 17:04:36,505 epoch 19 - iter 56/71 - loss 0.02006150 - samples/sec: 3151.55 - lr: 0.012500\n",
            "2022-11-21 17:04:36,572 epoch 19 - iter 63/71 - loss 0.01996995 - samples/sec: 3521.64 - lr: 0.012500\n",
            "2022-11-21 17:04:36,637 epoch 19 - iter 70/71 - loss 0.02000520 - samples/sec: 3638.29 - lr: 0.012500\n",
            "2022-11-21 17:04:36,642 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:36,645 EPOCH 19 done: loss 0.0203 - lr 0.0125000\n",
            "2022-11-21 17:04:36,752 DEV : loss 0.022718310356140137 - f1-score (micro avg)  0.5379\n",
            "2022-11-21 17:04:36,756 BAD EPOCHS (no improvement): 3\n",
            "2022-11-21 17:04:36,759 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:36,823 epoch 20 - iter 7/71 - loss 0.02019298 - samples/sec: 3699.54 - lr: 0.012500\n",
            "2022-11-21 17:04:36,890 epoch 20 - iter 14/71 - loss 0.02005745 - samples/sec: 3548.44 - lr: 0.012500\n",
            "2022-11-21 17:04:36,963 epoch 20 - iter 21/71 - loss 0.02004706 - samples/sec: 3259.68 - lr: 0.012500\n",
            "2022-11-21 17:04:37,034 epoch 20 - iter 28/71 - loss 0.02003763 - samples/sec: 3306.84 - lr: 0.012500\n",
            "2022-11-21 17:04:37,098 epoch 20 - iter 35/71 - loss 0.01997360 - samples/sec: 3699.63 - lr: 0.012500\n",
            "2022-11-21 17:04:37,160 epoch 20 - iter 42/71 - loss 0.02004573 - samples/sec: 3764.81 - lr: 0.012500\n",
            "2022-11-21 17:04:37,228 epoch 20 - iter 49/71 - loss 0.01998453 - samples/sec: 3444.96 - lr: 0.012500\n",
            "2022-11-21 17:04:37,296 epoch 20 - iter 56/71 - loss 0.01997649 - samples/sec: 3410.50 - lr: 0.012500\n",
            "2022-11-21 17:04:37,360 epoch 20 - iter 63/71 - loss 0.02000313 - samples/sec: 3689.41 - lr: 0.012500\n",
            "2022-11-21 17:04:37,430 epoch 20 - iter 70/71 - loss 0.01999229 - samples/sec: 3354.98 - lr: 0.012500\n",
            "2022-11-21 17:04:37,434 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:37,435 EPOCH 20 done: loss 0.0203 - lr 0.0125000\n",
            "2022-11-21 17:04:37,554 DEV : loss 0.022671665996313095 - f1-score (micro avg)  0.5451\n",
            "Epoch    20: reducing learning rate of group 0 to 6.2500e-03.\n",
            "2022-11-21 17:04:37,561 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:04:37,564 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:37,636 epoch 21 - iter 7/71 - loss 0.01972063 - samples/sec: 3227.55 - lr: 0.006250\n",
            "2022-11-21 17:04:37,710 epoch 21 - iter 14/71 - loss 0.01981132 - samples/sec: 3170.84 - lr: 0.006250\n",
            "2022-11-21 17:04:37,784 epoch 21 - iter 21/71 - loss 0.01998385 - samples/sec: 3163.30 - lr: 0.006250\n",
            "2022-11-21 17:04:37,854 epoch 21 - iter 28/71 - loss 0.01998816 - samples/sec: 3375.77 - lr: 0.006250\n",
            "2022-11-21 17:04:37,918 epoch 21 - iter 35/71 - loss 0.01983448 - samples/sec: 3625.81 - lr: 0.006250\n",
            "2022-11-21 17:04:37,982 epoch 21 - iter 42/71 - loss 0.01983122 - samples/sec: 3667.38 - lr: 0.006250\n",
            "2022-11-21 17:04:38,047 epoch 21 - iter 49/71 - loss 0.01985486 - samples/sec: 3636.68 - lr: 0.006250\n",
            "2022-11-21 17:04:38,109 epoch 21 - iter 56/71 - loss 0.01988040 - samples/sec: 3755.35 - lr: 0.006250\n",
            "2022-11-21 17:04:38,180 epoch 21 - iter 63/71 - loss 0.01997213 - samples/sec: 3317.28 - lr: 0.006250\n",
            "2022-11-21 17:04:38,244 epoch 21 - iter 70/71 - loss 0.01997146 - samples/sec: 3715.94 - lr: 0.006250\n",
            "2022-11-21 17:04:38,249 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:38,251 EPOCH 21 done: loss 0.0202 - lr 0.0062500\n",
            "2022-11-21 17:04:38,360 DEV : loss 0.02267390862107277 - f1-score (micro avg)  0.5415\n",
            "2022-11-21 17:04:38,365 BAD EPOCHS (no improvement): 1\n",
            "2022-11-21 17:04:38,367 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:38,439 epoch 22 - iter 7/71 - loss 0.01975217 - samples/sec: 3313.83 - lr: 0.006250\n",
            "2022-11-21 17:04:38,510 epoch 22 - iter 14/71 - loss 0.01985381 - samples/sec: 3342.11 - lr: 0.006250\n",
            "2022-11-21 17:04:38,577 epoch 22 - iter 21/71 - loss 0.02015357 - samples/sec: 3501.02 - lr: 0.006250\n",
            "2022-11-21 17:04:38,643 epoch 22 - iter 28/71 - loss 0.02007328 - samples/sec: 3545.47 - lr: 0.006250\n",
            "2022-11-21 17:04:38,713 epoch 22 - iter 35/71 - loss 0.02002984 - samples/sec: 3347.37 - lr: 0.006250\n",
            "2022-11-21 17:04:38,783 epoch 22 - iter 42/71 - loss 0.02008329 - samples/sec: 3395.90 - lr: 0.006250\n",
            "2022-11-21 17:04:38,850 epoch 22 - iter 49/71 - loss 0.02004987 - samples/sec: 3549.75 - lr: 0.006250\n",
            "2022-11-21 17:04:38,914 epoch 22 - iter 56/71 - loss 0.02004553 - samples/sec: 3615.70 - lr: 0.006250\n",
            "2022-11-21 17:04:38,976 epoch 22 - iter 63/71 - loss 0.01999887 - samples/sec: 3851.99 - lr: 0.006250\n",
            "2022-11-21 17:04:39,040 epoch 22 - iter 70/71 - loss 0.01996346 - samples/sec: 3620.18 - lr: 0.006250\n",
            "2022-11-21 17:04:39,044 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:39,046 EPOCH 22 done: loss 0.0201 - lr 0.0062500\n",
            "2022-11-21 17:04:39,152 DEV : loss 0.022699369117617607 - f1-score (micro avg)  0.5379\n",
            "2022-11-21 17:04:39,157 BAD EPOCHS (no improvement): 2\n",
            "2022-11-21 17:04:39,159 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:39,222 epoch 23 - iter 7/71 - loss 0.01986248 - samples/sec: 3753.92 - lr: 0.006250\n",
            "2022-11-21 17:04:39,287 epoch 23 - iter 14/71 - loss 0.01949646 - samples/sec: 3598.99 - lr: 0.006250\n",
            "2022-11-21 17:04:39,347 epoch 23 - iter 21/71 - loss 0.01973106 - samples/sec: 3863.40 - lr: 0.006250\n",
            "2022-11-21 17:04:39,408 epoch 23 - iter 28/71 - loss 0.01973091 - samples/sec: 3822.00 - lr: 0.006250\n",
            "2022-11-21 17:04:39,470 epoch 23 - iter 35/71 - loss 0.01985907 - samples/sec: 3759.22 - lr: 0.006250\n",
            "2022-11-21 17:04:39,534 epoch 23 - iter 42/71 - loss 0.01994260 - samples/sec: 3639.22 - lr: 0.006250\n",
            "2022-11-21 17:04:39,594 epoch 23 - iter 49/71 - loss 0.01992935 - samples/sec: 3874.13 - lr: 0.006250\n",
            "2022-11-21 17:04:39,656 epoch 23 - iter 56/71 - loss 0.01996121 - samples/sec: 3775.30 - lr: 0.006250\n",
            "2022-11-21 17:04:39,717 epoch 23 - iter 63/71 - loss 0.01996760 - samples/sec: 3827.78 - lr: 0.006250\n",
            "2022-11-21 17:04:39,780 epoch 23 - iter 70/71 - loss 0.01995054 - samples/sec: 3711.73 - lr: 0.006250\n",
            "2022-11-21 17:04:39,784 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:39,785 EPOCH 23 done: loss 0.0203 - lr 0.0062500\n",
            "2022-11-21 17:04:39,889 DEV : loss 0.02267610654234886 - f1-score (micro avg)  0.5451\n",
            "2022-11-21 17:04:39,895 BAD EPOCHS (no improvement): 3\n",
            "2022-11-21 17:04:39,897 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:39,957 epoch 24 - iter 7/71 - loss 0.02018576 - samples/sec: 3920.89 - lr: 0.006250\n",
            "2022-11-21 17:04:40,019 epoch 24 - iter 14/71 - loss 0.01991005 - samples/sec: 3736.96 - lr: 0.006250\n",
            "2022-11-21 17:04:40,081 epoch 24 - iter 21/71 - loss 0.01995306 - samples/sec: 3779.76 - lr: 0.006250\n",
            "2022-11-21 17:04:40,144 epoch 24 - iter 28/71 - loss 0.01995610 - samples/sec: 3703.10 - lr: 0.006250\n",
            "2022-11-21 17:04:40,205 epoch 24 - iter 35/71 - loss 0.02006571 - samples/sec: 3831.79 - lr: 0.006250\n",
            "2022-11-21 17:04:40,267 epoch 24 - iter 42/71 - loss 0.02001945 - samples/sec: 3756.32 - lr: 0.006250\n",
            "2022-11-21 17:04:40,331 epoch 24 - iter 49/71 - loss 0.02000302 - samples/sec: 3644.01 - lr: 0.006250\n",
            "2022-11-21 17:04:40,390 epoch 24 - iter 56/71 - loss 0.01994289 - samples/sec: 3944.86 - lr: 0.006250\n",
            "2022-11-21 17:04:40,451 epoch 24 - iter 63/71 - loss 0.01995657 - samples/sec: 3823.09 - lr: 0.006250\n",
            "2022-11-21 17:04:40,514 epoch 24 - iter 70/71 - loss 0.01994691 - samples/sec: 3697.75 - lr: 0.006250\n",
            "2022-11-21 17:04:40,518 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:40,519 EPOCH 24 done: loss 0.0203 - lr 0.0062500\n",
            "2022-11-21 17:04:40,620 DEV : loss 0.022673549130558968 - f1-score (micro avg)  0.5415\n",
            "Epoch    24: reducing learning rate of group 0 to 3.1250e-03.\n",
            "2022-11-21 17:04:40,624 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:04:40,628 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:40,688 epoch 25 - iter 7/71 - loss 0.01981504 - samples/sec: 3892.98 - lr: 0.003125\n",
            "2022-11-21 17:04:40,748 epoch 25 - iter 14/71 - loss 0.01990545 - samples/sec: 3867.93 - lr: 0.003125\n",
            "2022-11-21 17:04:40,812 epoch 25 - iter 21/71 - loss 0.01990382 - samples/sec: 3669.29 - lr: 0.003125\n",
            "2022-11-21 17:04:40,875 epoch 25 - iter 28/71 - loss 0.01985675 - samples/sec: 3772.98 - lr: 0.003125\n",
            "2022-11-21 17:04:40,935 epoch 25 - iter 35/71 - loss 0.02001661 - samples/sec: 3861.40 - lr: 0.003125\n",
            "2022-11-21 17:04:40,996 epoch 25 - iter 42/71 - loss 0.02005238 - samples/sec: 3805.99 - lr: 0.003125\n",
            "2022-11-21 17:04:41,058 epoch 25 - iter 49/71 - loss 0.02000016 - samples/sec: 3719.56 - lr: 0.003125\n",
            "2022-11-21 17:04:41,121 epoch 25 - iter 56/71 - loss 0.01999149 - samples/sec: 3723.11 - lr: 0.003125\n",
            "2022-11-21 17:04:41,184 epoch 25 - iter 63/71 - loss 0.01996517 - samples/sec: 3666.84 - lr: 0.003125\n",
            "2022-11-21 17:04:41,246 epoch 25 - iter 70/71 - loss 0.01994129 - samples/sec: 3815.06 - lr: 0.003125\n",
            "2022-11-21 17:04:41,250 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:41,251 EPOCH 25 done: loss 0.0202 - lr 0.0031250\n",
            "2022-11-21 17:04:41,354 DEV : loss 0.022688118740916252 - f1-score (micro avg)  0.5415\n",
            "2022-11-21 17:04:41,360 BAD EPOCHS (no improvement): 1\n",
            "2022-11-21 17:04:41,362 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:41,424 epoch 26 - iter 7/71 - loss 0.02003560 - samples/sec: 3855.19 - lr: 0.003125\n",
            "2022-11-21 17:04:41,485 epoch 26 - iter 14/71 - loss 0.01986989 - samples/sec: 3857.64 - lr: 0.003125\n",
            "2022-11-21 17:04:41,549 epoch 26 - iter 21/71 - loss 0.02002952 - samples/sec: 3615.14 - lr: 0.003125\n",
            "2022-11-21 17:04:41,611 epoch 26 - iter 28/71 - loss 0.01988928 - samples/sec: 3781.34 - lr: 0.003125\n",
            "2022-11-21 17:04:41,671 epoch 26 - iter 35/71 - loss 0.01991021 - samples/sec: 3870.02 - lr: 0.003125\n",
            "2022-11-21 17:04:41,733 epoch 26 - iter 42/71 - loss 0.01991129 - samples/sec: 3724.74 - lr: 0.003125\n",
            "2022-11-21 17:04:41,793 epoch 26 - iter 49/71 - loss 0.01996534 - samples/sec: 3849.50 - lr: 0.003125\n",
            "2022-11-21 17:04:41,856 epoch 26 - iter 56/71 - loss 0.01995650 - samples/sec: 3703.96 - lr: 0.003125\n",
            "2022-11-21 17:04:41,918 epoch 26 - iter 63/71 - loss 0.01993542 - samples/sec: 3770.09 - lr: 0.003125\n",
            "2022-11-21 17:04:41,981 epoch 26 - iter 70/71 - loss 0.01993387 - samples/sec: 3702.32 - lr: 0.003125\n",
            "2022-11-21 17:04:41,985 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:41,986 EPOCH 26 done: loss 0.0202 - lr 0.0031250\n",
            "2022-11-21 17:04:42,092 DEV : loss 0.02268635854125023 - f1-score (micro avg)  0.5415\n",
            "2022-11-21 17:04:42,096 BAD EPOCHS (no improvement): 2\n",
            "2022-11-21 17:04:42,099 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:42,161 epoch 27 - iter 7/71 - loss 0.01964311 - samples/sec: 3832.85 - lr: 0.003125\n",
            "2022-11-21 17:04:42,222 epoch 27 - iter 14/71 - loss 0.01977806 - samples/sec: 3779.18 - lr: 0.003125\n",
            "2022-11-21 17:04:42,284 epoch 27 - iter 21/71 - loss 0.01990770 - samples/sec: 3789.08 - lr: 0.003125\n",
            "2022-11-21 17:04:42,346 epoch 27 - iter 28/71 - loss 0.01972063 - samples/sec: 3742.47 - lr: 0.003125\n",
            "2022-11-21 17:04:42,407 epoch 27 - iter 35/71 - loss 0.01989159 - samples/sec: 3773.87 - lr: 0.003125\n",
            "2022-11-21 17:04:42,484 epoch 27 - iter 42/71 - loss 0.01993861 - samples/sec: 3028.04 - lr: 0.003125\n",
            "2022-11-21 17:04:42,590 epoch 27 - iter 49/71 - loss 0.01990871 - samples/sec: 2170.93 - lr: 0.003125\n",
            "2022-11-21 17:04:42,661 epoch 27 - iter 56/71 - loss 0.01988316 - samples/sec: 3317.77 - lr: 0.003125\n",
            "2022-11-21 17:04:42,723 epoch 27 - iter 63/71 - loss 0.01985613 - samples/sec: 3805.44 - lr: 0.003125\n",
            "2022-11-21 17:04:42,784 epoch 27 - iter 70/71 - loss 0.01993277 - samples/sec: 3816.44 - lr: 0.003125\n",
            "2022-11-21 17:04:42,789 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:42,790 EPOCH 27 done: loss 0.0202 - lr 0.0031250\n",
            "2022-11-21 17:04:42,897 DEV : loss 0.022687764838337898 - f1-score (micro avg)  0.5451\n",
            "2022-11-21 17:04:42,903 BAD EPOCHS (no improvement): 3\n",
            "2022-11-21 17:04:42,905 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:42,969 epoch 28 - iter 7/71 - loss 0.02059230 - samples/sec: 3696.64 - lr: 0.003125\n",
            "2022-11-21 17:04:43,033 epoch 28 - iter 14/71 - loss 0.02030841 - samples/sec: 3677.89 - lr: 0.003125\n",
            "2022-11-21 17:04:43,094 epoch 28 - iter 21/71 - loss 0.02024924 - samples/sec: 3852.18 - lr: 0.003125\n",
            "2022-11-21 17:04:43,154 epoch 28 - iter 28/71 - loss 0.02021524 - samples/sec: 3860.79 - lr: 0.003125\n",
            "2022-11-21 17:04:43,217 epoch 28 - iter 35/71 - loss 0.02016002 - samples/sec: 3705.30 - lr: 0.003125\n",
            "2022-11-21 17:04:43,281 epoch 28 - iter 42/71 - loss 0.01995399 - samples/sec: 3616.64 - lr: 0.003125\n",
            "2022-11-21 17:04:43,344 epoch 28 - iter 49/71 - loss 0.01988757 - samples/sec: 3752.62 - lr: 0.003125\n",
            "2022-11-21 17:04:43,405 epoch 28 - iter 56/71 - loss 0.01990235 - samples/sec: 3788.17 - lr: 0.003125\n",
            "2022-11-21 17:04:43,465 epoch 28 - iter 63/71 - loss 0.01983177 - samples/sec: 3897.00 - lr: 0.003125\n",
            "2022-11-21 17:04:43,528 epoch 28 - iter 70/71 - loss 0.01992807 - samples/sec: 3693.40 - lr: 0.003125\n",
            "2022-11-21 17:04:43,532 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:43,533 EPOCH 28 done: loss 0.0202 - lr 0.0031250\n",
            "2022-11-21 17:04:43,634 DEV : loss 0.022685285657644272 - f1-score (micro avg)  0.5415\n",
            "Epoch    28: reducing learning rate of group 0 to 1.5625e-03.\n",
            "2022-11-21 17:04:43,639 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:04:43,642 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:43,703 epoch 29 - iter 7/71 - loss 0.01971688 - samples/sec: 3825.83 - lr: 0.001563\n",
            "2022-11-21 17:04:43,766 epoch 29 - iter 14/71 - loss 0.01984967 - samples/sec: 3685.16 - lr: 0.001563\n",
            "2022-11-21 17:04:43,825 epoch 29 - iter 21/71 - loss 0.01982417 - samples/sec: 3979.63 - lr: 0.001563\n",
            "2022-11-21 17:04:43,887 epoch 29 - iter 28/71 - loss 0.01988991 - samples/sec: 3780.61 - lr: 0.001563\n",
            "2022-11-21 17:04:43,946 epoch 29 - iter 35/71 - loss 0.01990989 - samples/sec: 3898.59 - lr: 0.001563\n",
            "2022-11-21 17:04:44,006 epoch 29 - iter 42/71 - loss 0.01996956 - samples/sec: 3913.15 - lr: 0.001563\n",
            "2022-11-21 17:04:44,065 epoch 29 - iter 49/71 - loss 0.01992045 - samples/sec: 3930.62 - lr: 0.001563\n",
            "2022-11-21 17:04:44,127 epoch 29 - iter 56/71 - loss 0.01990014 - samples/sec: 3776.12 - lr: 0.001563\n",
            "2022-11-21 17:04:44,190 epoch 29 - iter 63/71 - loss 0.01994006 - samples/sec: 3697.33 - lr: 0.001563\n",
            "2022-11-21 17:04:44,251 epoch 29 - iter 70/71 - loss 0.01992512 - samples/sec: 3843.77 - lr: 0.001563\n",
            "2022-11-21 17:04:44,255 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:44,257 EPOCH 29 done: loss 0.0202 - lr 0.0015625\n",
            "2022-11-21 17:04:44,359 DEV : loss 0.022692641243338585 - f1-score (micro avg)  0.5415\n",
            "2022-11-21 17:04:44,365 BAD EPOCHS (no improvement): 1\n",
            "2022-11-21 17:04:44,367 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:44,429 epoch 30 - iter 7/71 - loss 0.01956636 - samples/sec: 3849.10 - lr: 0.001563\n",
            "2022-11-21 17:04:44,488 epoch 30 - iter 14/71 - loss 0.01952941 - samples/sec: 3912.03 - lr: 0.001563\n",
            "2022-11-21 17:04:44,550 epoch 30 - iter 21/71 - loss 0.01962325 - samples/sec: 3749.46 - lr: 0.001563\n",
            "2022-11-21 17:04:44,612 epoch 30 - iter 28/71 - loss 0.01978146 - samples/sec: 3768.51 - lr: 0.001563\n",
            "2022-11-21 17:04:44,672 epoch 30 - iter 35/71 - loss 0.01976128 - samples/sec: 3843.18 - lr: 0.001563\n",
            "2022-11-21 17:04:44,735 epoch 30 - iter 42/71 - loss 0.01982685 - samples/sec: 3742.35 - lr: 0.001563\n",
            "2022-11-21 17:04:44,794 epoch 30 - iter 49/71 - loss 0.01989455 - samples/sec: 3877.49 - lr: 0.001563\n",
            "2022-11-21 17:04:44,859 epoch 30 - iter 56/71 - loss 0.01996083 - samples/sec: 3629.69 - lr: 0.001563\n",
            "2022-11-21 17:04:44,919 epoch 30 - iter 63/71 - loss 0.01995389 - samples/sec: 3848.69 - lr: 0.001563\n",
            "2022-11-21 17:04:44,980 epoch 30 - iter 70/71 - loss 0.01992357 - samples/sec: 3850.24 - lr: 0.001563\n",
            "2022-11-21 17:04:44,984 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:44,985 EPOCH 30 done: loss 0.0201 - lr 0.0015625\n",
            "2022-11-21 17:04:45,093 DEV : loss 0.022692710161209106 - f1-score (micro avg)  0.5415\n",
            "2022-11-21 17:04:45,098 BAD EPOCHS (no improvement): 2\n",
            "2022-11-21 17:04:45,099 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:45,161 epoch 31 - iter 7/71 - loss 0.01959952 - samples/sec: 3855.53 - lr: 0.001563\n",
            "2022-11-21 17:04:45,221 epoch 31 - iter 14/71 - loss 0.01979761 - samples/sec: 3907.52 - lr: 0.001563\n",
            "2022-11-21 17:04:45,281 epoch 31 - iter 21/71 - loss 0.01974487 - samples/sec: 3833.73 - lr: 0.001563\n",
            "2022-11-21 17:04:45,341 epoch 31 - iter 28/71 - loss 0.01974220 - samples/sec: 3854.70 - lr: 0.001563\n",
            "2022-11-21 17:04:45,402 epoch 31 - iter 35/71 - loss 0.01979544 - samples/sec: 3810.41 - lr: 0.001563\n",
            "2022-11-21 17:04:45,462 epoch 31 - iter 42/71 - loss 0.01992856 - samples/sec: 3852.88 - lr: 0.001563\n",
            "2022-11-21 17:04:45,526 epoch 31 - iter 49/71 - loss 0.01990104 - samples/sec: 3647.55 - lr: 0.001563\n",
            "2022-11-21 17:04:45,585 epoch 31 - iter 56/71 - loss 0.01988000 - samples/sec: 3939.06 - lr: 0.001563\n",
            "2022-11-21 17:04:45,646 epoch 31 - iter 63/71 - loss 0.01988235 - samples/sec: 3894.08 - lr: 0.001563\n",
            "2022-11-21 17:04:45,706 epoch 31 - iter 70/71 - loss 0.01992163 - samples/sec: 3863.33 - lr: 0.001563\n",
            "2022-11-21 17:04:45,710 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:45,711 EPOCH 31 done: loss 0.0201 - lr 0.0015625\n",
            "2022-11-21 17:04:45,814 DEV : loss 0.022693626582622528 - f1-score (micro avg)  0.5415\n",
            "2022-11-21 17:04:45,818 BAD EPOCHS (no improvement): 3\n",
            "2022-11-21 17:04:45,821 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:45,884 epoch 32 - iter 7/71 - loss 0.01938841 - samples/sec: 3770.49 - lr: 0.001563\n",
            "2022-11-21 17:04:45,946 epoch 32 - iter 14/71 - loss 0.01975723 - samples/sec: 3717.13 - lr: 0.001563\n",
            "2022-11-21 17:04:46,008 epoch 32 - iter 21/71 - loss 0.01982412 - samples/sec: 3772.43 - lr: 0.001563\n",
            "2022-11-21 17:04:46,071 epoch 32 - iter 28/71 - loss 0.01976384 - samples/sec: 3739.40 - lr: 0.001563\n",
            "2022-11-21 17:04:46,137 epoch 32 - iter 35/71 - loss 0.01980828 - samples/sec: 3456.46 - lr: 0.001563\n",
            "2022-11-21 17:04:46,202 epoch 32 - iter 42/71 - loss 0.01973163 - samples/sec: 3596.46 - lr: 0.001563\n",
            "2022-11-21 17:04:46,268 epoch 32 - iter 49/71 - loss 0.01984625 - samples/sec: 3510.25 - lr: 0.001563\n",
            "2022-11-21 17:04:46,337 epoch 32 - iter 56/71 - loss 0.01990134 - samples/sec: 3357.23 - lr: 0.001563\n",
            "2022-11-21 17:04:46,402 epoch 32 - iter 63/71 - loss 0.01993770 - samples/sec: 3610.80 - lr: 0.001563\n",
            "2022-11-21 17:04:46,464 epoch 32 - iter 70/71 - loss 0.01991948 - samples/sec: 3724.62 - lr: 0.001563\n",
            "2022-11-21 17:04:46,468 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:46,470 EPOCH 32 done: loss 0.0201 - lr 0.0015625\n",
            "2022-11-21 17:04:46,572 DEV : loss 0.02269507758319378 - f1-score (micro avg)  0.5415\n",
            "Epoch    32: reducing learning rate of group 0 to 7.8125e-04.\n",
            "2022-11-21 17:04:46,576 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:04:46,580 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:46,641 epoch 33 - iter 7/71 - loss 0.02012190 - samples/sec: 3818.12 - lr: 0.000781\n",
            "2022-11-21 17:04:46,702 epoch 33 - iter 14/71 - loss 0.02031617 - samples/sec: 3833.60 - lr: 0.000781\n",
            "2022-11-21 17:04:46,764 epoch 33 - iter 21/71 - loss 0.02024537 - samples/sec: 3788.83 - lr: 0.000781\n",
            "2022-11-21 17:04:46,824 epoch 33 - iter 28/71 - loss 0.02000330 - samples/sec: 3892.69 - lr: 0.000781\n",
            "2022-11-21 17:04:46,885 epoch 33 - iter 35/71 - loss 0.01994366 - samples/sec: 3807.32 - lr: 0.000781\n",
            "2022-11-21 17:04:46,949 epoch 33 - iter 42/71 - loss 0.01983866 - samples/sec: 3652.33 - lr: 0.000781\n",
            "2022-11-21 17:04:47,009 epoch 33 - iter 49/71 - loss 0.01991938 - samples/sec: 3880.44 - lr: 0.000781\n",
            "2022-11-21 17:04:47,069 epoch 33 - iter 56/71 - loss 0.01990910 - samples/sec: 3927.55 - lr: 0.000781\n",
            "2022-11-21 17:04:47,129 epoch 33 - iter 63/71 - loss 0.01992609 - samples/sec: 3871.34 - lr: 0.000781\n",
            "2022-11-21 17:04:47,194 epoch 33 - iter 70/71 - loss 0.01991410 - samples/sec: 3538.62 - lr: 0.000781\n",
            "2022-11-21 17:04:47,198 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:47,200 EPOCH 33 done: loss 0.0202 - lr 0.0007813\n",
            "2022-11-21 17:04:47,304 DEV : loss 0.0226945448666811 - f1-score (micro avg)  0.5415\n",
            "2022-11-21 17:04:47,308 BAD EPOCHS (no improvement): 1\n",
            "2022-11-21 17:04:47,311 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:47,372 epoch 34 - iter 7/71 - loss 0.01965082 - samples/sec: 3882.60 - lr: 0.000781\n",
            "2022-11-21 17:04:47,433 epoch 34 - iter 14/71 - loss 0.01969583 - samples/sec: 3800.06 - lr: 0.000781\n",
            "2022-11-21 17:04:47,492 epoch 34 - iter 21/71 - loss 0.01984718 - samples/sec: 3927.22 - lr: 0.000781\n",
            "2022-11-21 17:04:47,556 epoch 34 - iter 28/71 - loss 0.01990964 - samples/sec: 3599.47 - lr: 0.000781\n",
            "2022-11-21 17:04:47,617 epoch 34 - iter 35/71 - loss 0.02004259 - samples/sec: 3846.54 - lr: 0.000781\n",
            "2022-11-21 17:04:47,679 epoch 34 - iter 42/71 - loss 0.02007116 - samples/sec: 3755.66 - lr: 0.000781\n",
            "2022-11-21 17:04:47,740 epoch 34 - iter 49/71 - loss 0.02005338 - samples/sec: 3798.54 - lr: 0.000781\n",
            "2022-11-21 17:04:47,801 epoch 34 - iter 56/71 - loss 0.01999190 - samples/sec: 3808.29 - lr: 0.000781\n",
            "2022-11-21 17:04:47,862 epoch 34 - iter 63/71 - loss 0.01995443 - samples/sec: 3823.18 - lr: 0.000781\n",
            "2022-11-21 17:04:47,926 epoch 34 - iter 70/71 - loss 0.01991512 - samples/sec: 3714.49 - lr: 0.000781\n",
            "2022-11-21 17:04:47,930 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:47,931 EPOCH 34 done: loss 0.0202 - lr 0.0007813\n",
            "2022-11-21 17:04:48,034 DEV : loss 0.02269846573472023 - f1-score (micro avg)  0.5415\n",
            "2022-11-21 17:04:48,040 BAD EPOCHS (no improvement): 2\n",
            "2022-11-21 17:04:48,042 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:48,109 epoch 35 - iter 7/71 - loss 0.01925726 - samples/sec: 3647.39 - lr: 0.000781\n",
            "2022-11-21 17:04:48,171 epoch 35 - iter 14/71 - loss 0.01979261 - samples/sec: 3762.11 - lr: 0.000781\n",
            "2022-11-21 17:04:48,230 epoch 35 - iter 21/71 - loss 0.01992165 - samples/sec: 3912.68 - lr: 0.000781\n",
            "2022-11-21 17:04:48,297 epoch 35 - iter 28/71 - loss 0.01976285 - samples/sec: 3480.74 - lr: 0.000781\n",
            "2022-11-21 17:04:48,358 epoch 35 - iter 35/71 - loss 0.01982892 - samples/sec: 3843.62 - lr: 0.000781\n",
            "2022-11-21 17:04:48,417 epoch 35 - iter 42/71 - loss 0.01982689 - samples/sec: 3918.28 - lr: 0.000781\n",
            "2022-11-21 17:04:48,477 epoch 35 - iter 49/71 - loss 0.01996954 - samples/sec: 3932.79 - lr: 0.000781\n",
            "2022-11-21 17:04:48,539 epoch 35 - iter 56/71 - loss 0.01995293 - samples/sec: 3747.59 - lr: 0.000781\n",
            "2022-11-21 17:04:48,599 epoch 35 - iter 63/71 - loss 0.01992154 - samples/sec: 3939.98 - lr: 0.000781\n",
            "2022-11-21 17:04:48,659 epoch 35 - iter 70/71 - loss 0.01990769 - samples/sec: 3870.56 - lr: 0.000781\n",
            "2022-11-21 17:04:48,663 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:48,664 EPOCH 35 done: loss 0.0204 - lr 0.0007813\n",
            "2022-11-21 17:04:48,764 DEV : loss 0.02269737794995308 - f1-score (micro avg)  0.5415\n",
            "2022-11-21 17:04:48,768 BAD EPOCHS (no improvement): 3\n",
            "2022-11-21 17:04:48,771 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:48,831 epoch 36 - iter 7/71 - loss 0.02010791 - samples/sec: 3933.71 - lr: 0.000781\n",
            "2022-11-21 17:04:48,892 epoch 36 - iter 14/71 - loss 0.02007807 - samples/sec: 3819.93 - lr: 0.000781\n",
            "2022-11-21 17:04:48,953 epoch 36 - iter 21/71 - loss 0.02002793 - samples/sec: 3724.38 - lr: 0.000781\n",
            "2022-11-21 17:04:49,015 epoch 36 - iter 28/71 - loss 0.02007846 - samples/sec: 3817.09 - lr: 0.000781\n",
            "2022-11-21 17:04:49,077 epoch 36 - iter 35/71 - loss 0.02000546 - samples/sec: 3767.91 - lr: 0.000781\n",
            "2022-11-21 17:04:49,138 epoch 36 - iter 42/71 - loss 0.01990367 - samples/sec: 3788.25 - lr: 0.000781\n",
            "2022-11-21 17:04:49,199 epoch 36 - iter 49/71 - loss 0.01987677 - samples/sec: 3814.01 - lr: 0.000781\n",
            "2022-11-21 17:04:49,262 epoch 36 - iter 56/71 - loss 0.01987720 - samples/sec: 3715.50 - lr: 0.000781\n",
            "2022-11-21 17:04:49,325 epoch 36 - iter 63/71 - loss 0.01990374 - samples/sec: 3652.79 - lr: 0.000781\n",
            "2022-11-21 17:04:49,386 epoch 36 - iter 70/71 - loss 0.01991759 - samples/sec: 3815.09 - lr: 0.000781\n",
            "2022-11-21 17:04:49,391 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:49,392 EPOCH 36 done: loss 0.0200 - lr 0.0007813\n",
            "2022-11-21 17:04:49,500 DEV : loss 0.022699955850839615 - f1-score (micro avg)  0.5415\n",
            "Epoch    36: reducing learning rate of group 0 to 3.9063e-04.\n",
            "2022-11-21 17:04:49,505 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:04:49,508 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:49,570 epoch 37 - iter 7/71 - loss 0.01900634 - samples/sec: 3787.29 - lr: 0.000391\n",
            "2022-11-21 17:04:49,631 epoch 37 - iter 14/71 - loss 0.01927430 - samples/sec: 3826.48 - lr: 0.000391\n",
            "2022-11-21 17:04:49,693 epoch 37 - iter 21/71 - loss 0.01949584 - samples/sec: 3779.03 - lr: 0.000391\n",
            "2022-11-21 17:04:49,755 epoch 37 - iter 28/71 - loss 0.01975303 - samples/sec: 3788.85 - lr: 0.000391\n",
            "2022-11-21 17:04:49,816 epoch 37 - iter 35/71 - loss 0.01987676 - samples/sec: 3820.51 - lr: 0.000391\n",
            "2022-11-21 17:04:49,877 epoch 37 - iter 42/71 - loss 0.01988387 - samples/sec: 3822.58 - lr: 0.000391\n",
            "2022-11-21 17:04:49,938 epoch 37 - iter 49/71 - loss 0.01988192 - samples/sec: 3825.35 - lr: 0.000391\n",
            "2022-11-21 17:04:50,000 epoch 37 - iter 56/71 - loss 0.01997423 - samples/sec: 3739.09 - lr: 0.000391\n",
            "2022-11-21 17:04:50,063 epoch 37 - iter 63/71 - loss 0.02000102 - samples/sec: 3699.66 - lr: 0.000391\n",
            "2022-11-21 17:04:50,124 epoch 37 - iter 70/71 - loss 0.01990719 - samples/sec: 3827.79 - lr: 0.000391\n",
            "2022-11-21 17:04:50,128 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:50,130 EPOCH 37 done: loss 0.0203 - lr 0.0003906\n",
            "2022-11-21 17:04:50,236 DEV : loss 0.022701947018504143 - f1-score (micro avg)  0.5379\n",
            "2022-11-21 17:04:50,242 BAD EPOCHS (no improvement): 1\n",
            "2022-11-21 17:04:50,244 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:50,307 epoch 38 - iter 7/71 - loss 0.02026302 - samples/sec: 3711.79 - lr: 0.000391\n",
            "2022-11-21 17:04:50,372 epoch 38 - iter 14/71 - loss 0.01966119 - samples/sec: 3628.54 - lr: 0.000391\n",
            "2022-11-21 17:04:50,433 epoch 38 - iter 21/71 - loss 0.01983960 - samples/sec: 3881.53 - lr: 0.000391\n",
            "2022-11-21 17:04:50,494 epoch 38 - iter 28/71 - loss 0.01996523 - samples/sec: 3805.64 - lr: 0.000391\n",
            "2022-11-21 17:04:50,559 epoch 38 - iter 35/71 - loss 0.01993056 - samples/sec: 3642.79 - lr: 0.000391\n",
            "2022-11-21 17:04:50,620 epoch 38 - iter 42/71 - loss 0.01986884 - samples/sec: 3821.15 - lr: 0.000391\n",
            "2022-11-21 17:04:50,682 epoch 38 - iter 49/71 - loss 0.01987292 - samples/sec: 3758.68 - lr: 0.000391\n",
            "2022-11-21 17:04:50,744 epoch 38 - iter 56/71 - loss 0.01991448 - samples/sec: 3772.75 - lr: 0.000391\n",
            "2022-11-21 17:04:50,805 epoch 38 - iter 63/71 - loss 0.01993911 - samples/sec: 3858.97 - lr: 0.000391\n",
            "2022-11-21 17:04:50,868 epoch 38 - iter 70/71 - loss 0.01990451 - samples/sec: 3705.13 - lr: 0.000391\n",
            "2022-11-21 17:04:50,873 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:50,874 EPOCH 38 done: loss 0.0204 - lr 0.0003906\n",
            "2022-11-21 17:04:50,978 DEV : loss 0.02270401082932949 - f1-score (micro avg)  0.5379\n",
            "2022-11-21 17:04:50,983 BAD EPOCHS (no improvement): 2\n",
            "2022-11-21 17:04:50,986 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:51,051 epoch 39 - iter 7/71 - loss 0.01944818 - samples/sec: 3683.82 - lr: 0.000391\n",
            "2022-11-21 17:04:51,116 epoch 39 - iter 14/71 - loss 0.01964335 - samples/sec: 3600.03 - lr: 0.000391\n",
            "2022-11-21 17:04:51,178 epoch 39 - iter 21/71 - loss 0.01992448 - samples/sec: 3817.11 - lr: 0.000391\n",
            "2022-11-21 17:04:51,242 epoch 39 - iter 28/71 - loss 0.01991240 - samples/sec: 3598.79 - lr: 0.000391\n",
            "2022-11-21 17:04:51,303 epoch 39 - iter 35/71 - loss 0.01994765 - samples/sec: 3881.77 - lr: 0.000391\n",
            "2022-11-21 17:04:51,364 epoch 39 - iter 42/71 - loss 0.01994608 - samples/sec: 3859.91 - lr: 0.000391\n",
            "2022-11-21 17:04:51,426 epoch 39 - iter 49/71 - loss 0.01993324 - samples/sec: 3759.36 - lr: 0.000391\n",
            "2022-11-21 17:04:51,489 epoch 39 - iter 56/71 - loss 0.01991408 - samples/sec: 3704.28 - lr: 0.000391\n",
            "2022-11-21 17:04:51,572 epoch 39 - iter 63/71 - loss 0.01994505 - samples/sec: 2838.05 - lr: 0.000391\n",
            "2022-11-21 17:04:51,653 epoch 39 - iter 70/71 - loss 0.01991338 - samples/sec: 2884.97 - lr: 0.000391\n",
            "2022-11-21 17:04:51,659 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:51,662 EPOCH 39 done: loss 0.0201 - lr 0.0003906\n",
            "2022-11-21 17:04:51,814 DEV : loss 0.022704225033521652 - f1-score (micro avg)  0.5379\n",
            "2022-11-21 17:04:51,823 BAD EPOCHS (no improvement): 3\n",
            "2022-11-21 17:04:51,825 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:51,910 epoch 40 - iter 7/71 - loss 0.01965071 - samples/sec: 2809.38 - lr: 0.000391\n",
            "2022-11-21 17:04:51,982 epoch 40 - iter 14/71 - loss 0.01972537 - samples/sec: 3268.38 - lr: 0.000391\n",
            "2022-11-21 17:04:52,056 epoch 40 - iter 21/71 - loss 0.01996691 - samples/sec: 3164.80 - lr: 0.000391\n",
            "2022-11-21 17:04:52,121 epoch 40 - iter 28/71 - loss 0.01987912 - samples/sec: 3573.35 - lr: 0.000391\n",
            "2022-11-21 17:04:52,190 epoch 40 - iter 35/71 - loss 0.01990376 - samples/sec: 3424.43 - lr: 0.000391\n",
            "2022-11-21 17:04:52,255 epoch 40 - iter 42/71 - loss 0.01988384 - samples/sec: 3663.22 - lr: 0.000391\n",
            "2022-11-21 17:04:52,321 epoch 40 - iter 49/71 - loss 0.01986189 - samples/sec: 3598.03 - lr: 0.000391\n",
            "2022-11-21 17:04:52,383 epoch 40 - iter 56/71 - loss 0.01989468 - samples/sec: 3743.00 - lr: 0.000391\n",
            "2022-11-21 17:04:52,455 epoch 40 - iter 63/71 - loss 0.01991055 - samples/sec: 3333.64 - lr: 0.000391\n",
            "2022-11-21 17:04:52,522 epoch 40 - iter 70/71 - loss 0.01990750 - samples/sec: 3474.19 - lr: 0.000391\n",
            "2022-11-21 17:04:52,527 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:52,530 EPOCH 40 done: loss 0.0202 - lr 0.0003906\n",
            "2022-11-21 17:04:52,637 DEV : loss 0.02270599827170372 - f1-score (micro avg)  0.5379\n",
            "Epoch    40: reducing learning rate of group 0 to 1.9531e-04.\n",
            "2022-11-21 17:04:52,645 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:04:52,648 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:52,715 epoch 41 - iter 7/71 - loss 0.01957528 - samples/sec: 3488.62 - lr: 0.000195\n",
            "2022-11-21 17:04:52,807 epoch 41 - iter 14/71 - loss 0.01955306 - samples/sec: 2537.66 - lr: 0.000195\n",
            "2022-11-21 17:04:52,874 epoch 41 - iter 21/71 - loss 0.01980146 - samples/sec: 3524.36 - lr: 0.000195\n",
            "2022-11-21 17:04:52,958 epoch 41 - iter 28/71 - loss 0.01979912 - samples/sec: 2765.04 - lr: 0.000195\n",
            "2022-11-21 17:04:53,030 epoch 41 - iter 35/71 - loss 0.01976846 - samples/sec: 3278.54 - lr: 0.000195\n",
            "2022-11-21 17:04:53,111 epoch 41 - iter 42/71 - loss 0.01972936 - samples/sec: 2865.79 - lr: 0.000195\n",
            "2022-11-21 17:04:53,185 epoch 41 - iter 49/71 - loss 0.01986954 - samples/sec: 3156.23 - lr: 0.000195\n",
            "2022-11-21 17:04:53,255 epoch 41 - iter 56/71 - loss 0.01985947 - samples/sec: 3390.09 - lr: 0.000195\n",
            "2022-11-21 17:04:53,317 epoch 41 - iter 63/71 - loss 0.01989809 - samples/sec: 3764.53 - lr: 0.000195\n",
            "2022-11-21 17:04:53,383 epoch 41 - iter 70/71 - loss 0.01991335 - samples/sec: 3600.21 - lr: 0.000195\n",
            "2022-11-21 17:04:53,388 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:53,390 EPOCH 41 done: loss 0.0200 - lr 0.0001953\n",
            "2022-11-21 17:04:53,497 DEV : loss 0.022706056013703346 - f1-score (micro avg)  0.5379\n",
            "2022-11-21 17:04:53,506 BAD EPOCHS (no improvement): 1\n",
            "2022-11-21 17:04:53,509 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:53,577 epoch 42 - iter 7/71 - loss 0.02018685 - samples/sec: 3631.95 - lr: 0.000195\n",
            "2022-11-21 17:04:53,673 epoch 42 - iter 14/71 - loss 0.01989934 - samples/sec: 2413.24 - lr: 0.000195\n",
            "2022-11-21 17:04:53,738 epoch 42 - iter 21/71 - loss 0.01991924 - samples/sec: 3609.31 - lr: 0.000195\n",
            "2022-11-21 17:04:53,807 epoch 42 - iter 28/71 - loss 0.01986297 - samples/sec: 3366.88 - lr: 0.000195\n",
            "2022-11-21 17:04:53,873 epoch 42 - iter 35/71 - loss 0.01997985 - samples/sec: 3642.05 - lr: 0.000195\n",
            "2022-11-21 17:04:53,941 epoch 42 - iter 42/71 - loss 0.02010566 - samples/sec: 3421.33 - lr: 0.000195\n",
            "2022-11-21 17:04:54,021 epoch 42 - iter 49/71 - loss 0.02003884 - samples/sec: 2993.73 - lr: 0.000195\n",
            "2022-11-21 17:04:54,089 epoch 42 - iter 56/71 - loss 0.02002870 - samples/sec: 3430.38 - lr: 0.000195\n",
            "2022-11-21 17:04:54,172 epoch 42 - iter 63/71 - loss 0.01995427 - samples/sec: 2850.26 - lr: 0.000195\n",
            "2022-11-21 17:04:54,238 epoch 42 - iter 70/71 - loss 0.01990971 - samples/sec: 3600.15 - lr: 0.000195\n",
            "2022-11-21 17:04:54,243 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:54,245 EPOCH 42 done: loss 0.0201 - lr 0.0001953\n",
            "2022-11-21 17:04:54,374 DEV : loss 0.02270601876080036 - f1-score (micro avg)  0.5379\n",
            "2022-11-21 17:04:54,383 BAD EPOCHS (no improvement): 2\n",
            "2022-11-21 17:04:54,386 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:54,466 epoch 43 - iter 7/71 - loss 0.01935392 - samples/sec: 3108.24 - lr: 0.000195\n",
            "2022-11-21 17:04:54,542 epoch 43 - iter 14/71 - loss 0.01990557 - samples/sec: 3045.77 - lr: 0.000195\n",
            "2022-11-21 17:04:54,611 epoch 43 - iter 21/71 - loss 0.02004373 - samples/sec: 3438.85 - lr: 0.000195\n",
            "2022-11-21 17:04:54,685 epoch 43 - iter 28/71 - loss 0.02000302 - samples/sec: 3151.39 - lr: 0.000195\n",
            "2022-11-21 17:04:54,756 epoch 43 - iter 35/71 - loss 0.01992489 - samples/sec: 3308.28 - lr: 0.000195\n",
            "2022-11-21 17:04:54,825 epoch 43 - iter 42/71 - loss 0.01988778 - samples/sec: 3399.56 - lr: 0.000195\n",
            "2022-11-21 17:04:54,894 epoch 43 - iter 49/71 - loss 0.01984401 - samples/sec: 3399.88 - lr: 0.000195\n",
            "2022-11-21 17:04:54,968 epoch 43 - iter 56/71 - loss 0.01986178 - samples/sec: 3150.84 - lr: 0.000195\n",
            "2022-11-21 17:04:55,049 epoch 43 - iter 63/71 - loss 0.01985301 - samples/sec: 2871.49 - lr: 0.000195\n",
            "2022-11-21 17:04:55,113 epoch 43 - iter 70/71 - loss 0.01991100 - samples/sec: 3740.28 - lr: 0.000195\n",
            "2022-11-21 17:04:55,119 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:55,121 EPOCH 43 done: loss 0.0201 - lr 0.0001953\n",
            "2022-11-21 17:04:55,229 DEV : loss 0.02270672284066677 - f1-score (micro avg)  0.5379\n",
            "2022-11-21 17:04:55,236 BAD EPOCHS (no improvement): 3\n",
            "2022-11-21 17:04:55,240 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:55,306 epoch 44 - iter 7/71 - loss 0.01969825 - samples/sec: 3628.92 - lr: 0.000195\n",
            "2022-11-21 17:04:55,371 epoch 44 - iter 14/71 - loss 0.01987393 - samples/sec: 3619.51 - lr: 0.000195\n",
            "2022-11-21 17:04:55,437 epoch 44 - iter 21/71 - loss 0.01993455 - samples/sec: 3493.50 - lr: 0.000195\n",
            "2022-11-21 17:04:55,510 epoch 44 - iter 28/71 - loss 0.01986736 - samples/sec: 3209.28 - lr: 0.000195\n",
            "2022-11-21 17:04:55,576 epoch 44 - iter 35/71 - loss 0.01987845 - samples/sec: 3686.43 - lr: 0.000195\n",
            "2022-11-21 17:04:55,641 epoch 44 - iter 42/71 - loss 0.01992990 - samples/sec: 3766.05 - lr: 0.000195\n",
            "2022-11-21 17:04:55,713 epoch 44 - iter 49/71 - loss 0.01993657 - samples/sec: 3363.09 - lr: 0.000195\n",
            "2022-11-21 17:04:55,779 epoch 44 - iter 56/71 - loss 0.01993241 - samples/sec: 3567.21 - lr: 0.000195\n",
            "2022-11-21 17:04:55,844 epoch 44 - iter 63/71 - loss 0.01985539 - samples/sec: 3631.15 - lr: 0.000195\n",
            "2022-11-21 17:04:55,909 epoch 44 - iter 70/71 - loss 0.01990842 - samples/sec: 3704.41 - lr: 0.000195\n",
            "2022-11-21 17:04:55,914 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:55,917 EPOCH 44 done: loss 0.0202 - lr 0.0001953\n",
            "2022-11-21 17:04:56,026 DEV : loss 0.022706689313054085 - f1-score (micro avg)  0.5379\n",
            "Epoch    44: reducing learning rate of group 0 to 9.7656e-05.\n",
            "2022-11-21 17:04:56,032 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:04:56,037 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:56,039 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:04:56,042 learning rate too small - quitting training!\n",
            "2022-11-21 17:04:56,043 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:05:02,005 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:05:02,009 loading file resources/word-pair-test-flair/best-model.pt\n",
            "2022-11-21 17:05:04,057 0.5221\t0.5221\t0.5221\t0.5221\n",
            "2022-11-21 17:05:04,059 \n",
            "Results:\n",
            "- F-score (micro) 0.5221\n",
            "- F-score (macro) 0.4427\n",
            "- Accuracy 0.5221\n",
            "\n",
            "By class:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "    entailment     0.5308    0.8485    0.6531       132\n",
            "not_entailment     0.4737    0.1538    0.2323       117\n",
            "\n",
            "     micro avg     0.5221    0.5221    0.5221       249\n",
            "     macro avg     0.5022    0.5012    0.4427       249\n",
            "  weighted avg     0.5040    0.5221    0.4553       249\n",
            "   samples avg     0.5221    0.5221    0.5221       249\n",
            "\n",
            "2022-11-21 17:05:04,062 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_score': 0.5220883534136547,\n",
              " 'dev_score_history': [0.46570397111913364,\n",
              "  0.555956678700361,\n",
              "  0.5415162454873647,\n",
              "  0.5848375451263538,\n",
              "  0.5740072202166066,\n",
              "  0.5415162454873647,\n",
              "  0.48375451263537905,\n",
              "  0.48375451263537905,\n",
              "  0.51985559566787,\n",
              "  0.5270758122743683,\n",
              "  0.5342960288808665,\n",
              "  0.5415162454873647,\n",
              "  0.5451263537906137,\n",
              "  0.5234657039711191,\n",
              "  0.5451263537906137,\n",
              "  0.5234657039711191,\n",
              "  0.5415162454873647,\n",
              "  0.5379061371841155,\n",
              "  0.5379061371841155,\n",
              "  0.5451263537906137,\n",
              "  0.5415162454873647,\n",
              "  0.5379061371841155,\n",
              "  0.5451263537906137,\n",
              "  0.5415162454873647,\n",
              "  0.5415162454873647,\n",
              "  0.5415162454873647,\n",
              "  0.5451263537906137,\n",
              "  0.5415162454873647,\n",
              "  0.5415162454873647,\n",
              "  0.5415162454873647,\n",
              "  0.5415162454873647,\n",
              "  0.5415162454873647,\n",
              "  0.5415162454873647,\n",
              "  0.5415162454873647,\n",
              "  0.5415162454873647,\n",
              "  0.5415162454873647,\n",
              "  0.5379061371841155,\n",
              "  0.5379061371841155,\n",
              "  0.5379061371841155,\n",
              "  0.5379061371841155,\n",
              "  0.5379061371841155,\n",
              "  0.5379061371841155,\n",
              "  0.5379061371841155,\n",
              "  0.5379061371841155],\n",
              " 'train_loss_history': [0.024592990776302025,\n",
              "  0.02240757506535235,\n",
              "  0.022006311197463872,\n",
              "  0.02167756929295449,\n",
              "  0.02145949084542788,\n",
              "  0.021289010097915603,\n",
              "  0.021089139845370183,\n",
              "  0.020904189425353117,\n",
              "  0.02064541135181119,\n",
              "  0.020766201773587744,\n",
              "  0.020691257058270858,\n",
              "  0.02055869614210899,\n",
              "  0.020483714084123087,\n",
              "  0.020324127195882565,\n",
              "  0.02049254227408018,\n",
              "  0.02035629251927789,\n",
              "  0.020382076435352958,\n",
              "  0.02040766440880506,\n",
              "  0.02032403135235849,\n",
              "  0.02025612777814648,\n",
              "  0.020165908794113698,\n",
              "  0.020088027439602568,\n",
              "  0.02026190876375613,\n",
              "  0.020270299613608787,\n",
              "  0.02016265381132583,\n",
              "  0.020197565669409136,\n",
              "  0.020179424366744594,\n",
              "  0.02022976784342264,\n",
              "  0.020161416024195727,\n",
              "  0.02012160586651177,\n",
              "  0.02012051589411319,\n",
              "  0.020128909470193464,\n",
              "  0.020220006778271057,\n",
              "  0.020168921581499813,\n",
              "  0.02036569428092824,\n",
              "  0.020044491087310497,\n",
              "  0.020304325913170097,\n",
              "  0.020366916109652775,\n",
              "  0.0200753715063833,\n",
              "  0.02024415014365697,\n",
              "  0.020036636253703336,\n",
              "  0.020143079390647103,\n",
              "  0.020100121171492544,\n",
              "  0.020169888315111437],\n",
              " 'dev_loss_history': [tensor(0.0239, device='cuda:0'),\n",
              "  tensor(0.0228, device='cuda:0'),\n",
              "  tensor(0.0228, device='cuda:0'),\n",
              "  tensor(0.0228, device='cuda:0'),\n",
              "  tensor(0.0227, device='cuda:0'),\n",
              "  tensor(0.0234, device='cuda:0'),\n",
              "  tensor(0.0230, device='cuda:0'),\n",
              "  tensor(0.0235, device='cuda:0'),\n",
              "  tensor(0.0228, device='cuda:0'),\n",
              "  tensor(0.0230, device='cuda:0'),\n",
              "  tensor(0.0226, device='cuda:0'),\n",
              "  tensor(0.0226, device='cuda:0'),\n",
              "  tensor(0.0227, device='cuda:0'),\n",
              "  tensor(0.0226, device='cuda:0'),\n",
              "  tensor(0.0227, device='cuda:0'),\n",
              "  tensor(0.0226, device='cuda:0'),\n",
              "  tensor(0.0227, device='cuda:0'),\n",
              "  tensor(0.0227, device='cuda:0'),\n",
              "  tensor(0.0227, device='cuda:0'),\n",
              "  tensor(0.0227, device='cuda:0'),\n",
              "  tensor(0.0227, device='cuda:0'),\n",
              "  tensor(0.0227, device='cuda:0'),\n",
              "  tensor(0.0227, device='cuda:0'),\n",
              "  tensor(0.0227, device='cuda:0'),\n",
              "  tensor(0.0227, device='cuda:0'),\n",
              "  tensor(0.0227, device='cuda:0'),\n",
              "  tensor(0.0227, device='cuda:0'),\n",
              "  tensor(0.0227, device='cuda:0'),\n",
              "  tensor(0.0227, device='cuda:0'),\n",
              "  tensor(0.0227, device='cuda:0'),\n",
              "  tensor(0.0227, device='cuda:0'),\n",
              "  tensor(0.0227, device='cuda:0'),\n",
              "  tensor(0.0227, device='cuda:0'),\n",
              "  tensor(0.0227, device='cuda:0'),\n",
              "  tensor(0.0227, device='cuda:0'),\n",
              "  tensor(0.0227, device='cuda:0'),\n",
              "  tensor(0.0227, device='cuda:0'),\n",
              "  tensor(0.0227, device='cuda:0'),\n",
              "  tensor(0.0227, device='cuda:0'),\n",
              "  tensor(0.0227, device='cuda:0'),\n",
              "  tensor(0.0227, device='cuda:0'),\n",
              "  tensor(0.0227, device='cuda:0'),\n",
              "  tensor(0.0227, device='cuda:0'),\n",
              "  tensor(0.0227, device='cuda:0')]}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Stack Flair and Word embeddings"
      ],
      "metadata": {
        "id": "43Z2p3n61rYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flair.embeddings import WordEmbeddings, FlairEmbeddings, StackedEmbeddings\n",
        "from flair.embeddings import DocumentPoolEmbeddings\n",
        "from flair.models import TextPairClassifier\n",
        "from flair.trainers import ModelTrainer\n",
        "\n",
        "embedding_stack = StackedEmbeddings([\n",
        "    WordEmbeddings('en'),\n",
        "    FlairEmbeddings('en-forward'),\n",
        "])\n",
        "\n",
        "embedding_stack = DocumentPoolEmbeddings([embedding_stack])\n",
        "\n",
        "# Step 3: Use text pair classification model\n",
        "classifier = TextPairClassifier(document_embeddings=embedding_stack,\n",
        "                                label_type=label_type,\n",
        "                                label_dictionary=label_dictionary,\n",
        "                                embed_separately=True,\n",
        "                                )\n",
        "\n",
        "# Step 4: Initialize trainer and train the model\n",
        "trainer = ModelTrainer(classifier, corpus)\n",
        "\n",
        "# if you are using transformer embeddings, you can simply call trainer.fine_tune()\n",
        "trainer.train(base_path='resources/word-pair-test-flair',\n",
        "              use_final_model_for_eval=False,\n",
        "              learning_rate=0.1,\n",
        "              max_epochs=100,\n",
        "              )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wd56EhT_1rJZ",
        "outputId": "3786e318-c694-4216-bc26-a6aa9655ee72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-21 17:05:53,029 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:05:53,032 Model: \"TextPairClassifier(\n",
            "  (loss_function): CrossEntropyLoss()\n",
            "  (document_embeddings): DocumentPoolEmbeddings(\n",
            "    fine_tune_mode=none, pooling=mean\n",
            "    (embeddings): StackedEmbeddings(\n",
            "      (list_embedding_0): StackedEmbeddings(\n",
            "        (list_embedding_0): WordEmbeddings(\n",
            "          'en'\n",
            "          (embedding): Embedding(1000001, 300)\n",
            "        )\n",
            "        (list_embedding_1): FlairEmbeddings(\n",
            "          (lm): LanguageModel(\n",
            "            (drop): Dropout(p=0.05, inplace=False)\n",
            "            (encoder): Embedding(300, 100)\n",
            "            (rnn): LSTM(100, 2048)\n",
            "            (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): Linear(in_features=4696, out_features=3, bias=True)\n",
            "  (weights): None\n",
            "  (weight_tensor) None\n",
            ")\"\n",
            "2022-11-21 17:05:53,033 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:05:53,037 Corpus: \"Corpus: 2241 train + 277 dev + 249 test sentences\"\n",
            "2022-11-21 17:05:53,038 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:05:53,039 Parameters:\n",
            "2022-11-21 17:05:53,043  - learning_rate: \"0.1\"\n",
            "2022-11-21 17:05:53,045  - mini_batch_size: \"32\"\n",
            "2022-11-21 17:05:53,046  - patience: \"3\"\n",
            "2022-11-21 17:05:53,048  - anneal_factor: \"0.5\"\n",
            "2022-11-21 17:05:53,050  - max_epochs: \"100\"\n",
            "2022-11-21 17:05:53,052  - shuffle: \"True\"\n",
            "2022-11-21 17:05:53,054  - train_with_dev: \"False\"\n",
            "2022-11-21 17:05:53,056  - batch_growth_annealing: \"False\"\n",
            "2022-11-21 17:05:53,058 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:05:53,059 Model training base path: \"resources/word-pair-test-flair\"\n",
            "2022-11-21 17:05:53,060 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:05:53,064 Device: cuda:0\n",
            "2022-11-21 17:05:53,066 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:05:53,068 Embeddings storage mode: cpu\n",
            "2022-11-21 17:05:53,089 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/flair/trainers/trainer.py:65: UserWarning: There should be no best model saved at epoch 1 except there is a model from previous trainings in your training folder. All previous best models will be deleted.\n",
            "  \"There should be no best model saved at epoch 1 except there is a model from previous trainings\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-21 17:05:53,334 epoch 1 - iter 7/71 - loss 0.02813241 - samples/sec: 925.79 - lr: 0.100000\n",
            "2022-11-21 17:05:53,545 epoch 1 - iter 14/71 - loss 0.02616650 - samples/sec: 1073.75 - lr: 0.100000\n",
            "2022-11-21 17:05:53,741 epoch 1 - iter 21/71 - loss 0.02519040 - samples/sec: 1156.79 - lr: 0.100000\n",
            "2022-11-21 17:05:53,943 epoch 1 - iter 28/71 - loss 0.02460725 - samples/sec: 1123.95 - lr: 0.100000\n",
            "2022-11-21 17:05:54,144 epoch 1 - iter 35/71 - loss 0.02417782 - samples/sec: 1130.63 - lr: 0.100000\n",
            "2022-11-21 17:05:54,341 epoch 1 - iter 42/71 - loss 0.02391600 - samples/sec: 1151.71 - lr: 0.100000\n",
            "2022-11-21 17:05:54,534 epoch 1 - iter 49/71 - loss 0.02373013 - samples/sec: 1175.77 - lr: 0.100000\n",
            "2022-11-21 17:05:54,726 epoch 1 - iter 56/71 - loss 0.02352508 - samples/sec: 1179.30 - lr: 0.100000\n",
            "2022-11-21 17:05:54,918 epoch 1 - iter 63/71 - loss 0.02337854 - samples/sec: 1184.35 - lr: 0.100000\n",
            "2022-11-21 17:05:55,112 epoch 1 - iter 70/71 - loss 0.02329357 - samples/sec: 1169.71 - lr: 0.100000\n",
            "2022-11-21 17:05:55,123 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:05:55,125 EPOCH 1 done: loss 0.0236 - lr 0.1000000\n",
            "2022-11-21 17:05:55,394 DEV : loss 0.025028346106410027 - f1-score (micro avg)  0.4838\n",
            "2022-11-21 17:05:55,400 BAD EPOCHS (no improvement): 0\n",
            "2022-11-21 17:05:55,401 saving best model\n",
            "2022-11-21 17:06:01,173 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:01,265 epoch 2 - iter 7/71 - loss 0.02192620 - samples/sec: 2532.11 - lr: 0.100000\n",
            "2022-11-21 17:06:01,351 epoch 2 - iter 14/71 - loss 0.02183051 - samples/sec: 2692.68 - lr: 0.100000\n",
            "2022-11-21 17:06:01,437 epoch 2 - iter 21/71 - loss 0.02153093 - samples/sec: 2697.78 - lr: 0.100000\n",
            "2022-11-21 17:06:01,520 epoch 2 - iter 28/71 - loss 0.02149802 - samples/sec: 2805.51 - lr: 0.100000\n",
            "2022-11-21 17:06:01,609 epoch 2 - iter 35/71 - loss 0.02156352 - samples/sec: 2581.35 - lr: 0.100000\n",
            "2022-11-21 17:06:01,693 epoch 2 - iter 42/71 - loss 0.02151825 - samples/sec: 2796.39 - lr: 0.100000\n",
            "2022-11-21 17:06:01,777 epoch 2 - iter 49/71 - loss 0.02150971 - samples/sec: 2818.46 - lr: 0.100000\n",
            "2022-11-21 17:06:01,857 epoch 2 - iter 56/71 - loss 0.02139864 - samples/sec: 2883.19 - lr: 0.100000\n",
            "2022-11-21 17:06:01,937 epoch 2 - iter 63/71 - loss 0.02143511 - samples/sec: 2923.51 - lr: 0.100000\n",
            "2022-11-21 17:06:02,023 epoch 2 - iter 70/71 - loss 0.02139045 - samples/sec: 2691.58 - lr: 0.100000\n",
            "2022-11-21 17:06:02,027 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:02,029 EPOCH 2 done: loss 0.0216 - lr 0.1000000\n",
            "2022-11-21 17:06:02,157 DEV : loss 0.02497534081339836 - f1-score (micro avg)  0.4657\n",
            "2022-11-21 17:06:02,163 BAD EPOCHS (no improvement): 1\n",
            "2022-11-21 17:06:02,165 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:02,246 epoch 3 - iter 7/71 - loss 0.02097244 - samples/sec: 2881.31 - lr: 0.100000\n",
            "2022-11-21 17:06:02,326 epoch 3 - iter 14/71 - loss 0.02086978 - samples/sec: 2896.41 - lr: 0.100000\n",
            "2022-11-21 17:06:02,406 epoch 3 - iter 21/71 - loss 0.02090982 - samples/sec: 2894.20 - lr: 0.100000\n",
            "2022-11-21 17:06:02,484 epoch 3 - iter 28/71 - loss 0.02085266 - samples/sec: 2959.42 - lr: 0.100000\n",
            "2022-11-21 17:06:02,565 epoch 3 - iter 35/71 - loss 0.02092507 - samples/sec: 2876.58 - lr: 0.100000\n",
            "2022-11-21 17:06:02,646 epoch 3 - iter 42/71 - loss 0.02080298 - samples/sec: 2835.70 - lr: 0.100000\n",
            "2022-11-21 17:06:02,727 epoch 3 - iter 49/71 - loss 0.02081782 - samples/sec: 2884.75 - lr: 0.100000\n",
            "2022-11-21 17:06:02,807 epoch 3 - iter 56/71 - loss 0.02078932 - samples/sec: 2883.58 - lr: 0.100000\n",
            "2022-11-21 17:06:02,887 epoch 3 - iter 63/71 - loss 0.02071687 - samples/sec: 2889.88 - lr: 0.100000\n",
            "2022-11-21 17:06:02,967 epoch 3 - iter 70/71 - loss 0.02073641 - samples/sec: 2884.03 - lr: 0.100000\n",
            "2022-11-21 17:06:02,971 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:02,973 EPOCH 3 done: loss 0.0211 - lr 0.1000000\n",
            "2022-11-21 17:06:03,104 DEV : loss 0.02575751207768917 - f1-score (micro avg)  0.4513\n",
            "2022-11-21 17:06:03,111 BAD EPOCHS (no improvement): 2\n",
            "2022-11-21 17:06:03,113 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:03,200 epoch 4 - iter 7/71 - loss 0.02086242 - samples/sec: 2691.19 - lr: 0.100000\n",
            "2022-11-21 17:06:03,281 epoch 4 - iter 14/71 - loss 0.02062649 - samples/sec: 2876.23 - lr: 0.100000\n",
            "2022-11-21 17:06:03,366 epoch 4 - iter 21/71 - loss 0.02044045 - samples/sec: 2708.04 - lr: 0.100000\n",
            "2022-11-21 17:06:03,449 epoch 4 - iter 28/71 - loss 0.02040376 - samples/sec: 2833.37 - lr: 0.100000\n",
            "2022-11-21 17:06:03,532 epoch 4 - iter 35/71 - loss 0.02024986 - samples/sec: 2800.60 - lr: 0.100000\n",
            "2022-11-21 17:06:03,751 epoch 4 - iter 42/71 - loss 0.02024311 - samples/sec: 1084.28 - lr: 0.100000\n",
            "2022-11-21 17:06:03,850 epoch 4 - iter 49/71 - loss 0.02030104 - samples/sec: 2343.34 - lr: 0.100000\n",
            "2022-11-21 17:06:03,936 epoch 4 - iter 56/71 - loss 0.02019127 - samples/sec: 2686.46 - lr: 0.100000\n",
            "2022-11-21 17:06:04,023 epoch 4 - iter 63/71 - loss 0.02018129 - samples/sec: 2660.46 - lr: 0.100000\n",
            "2022-11-21 17:06:04,105 epoch 4 - iter 70/71 - loss 0.02017351 - samples/sec: 2841.02 - lr: 0.100000\n",
            "2022-11-21 17:06:04,110 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:04,111 EPOCH 4 done: loss 0.0204 - lr 0.1000000\n",
            "2022-11-21 17:06:04,243 DEV : loss 0.02574934810400009 - f1-score (micro avg)  0.5415\n",
            "2022-11-21 17:06:04,249 BAD EPOCHS (no improvement): 0\n",
            "2022-11-21 17:06:04,251 saving best model\n",
            "2022-11-21 17:06:11,180 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:11,263 epoch 5 - iter 7/71 - loss 0.01984023 - samples/sec: 2822.78 - lr: 0.100000\n",
            "2022-11-21 17:06:11,342 epoch 5 - iter 14/71 - loss 0.01973042 - samples/sec: 2966.05 - lr: 0.100000\n",
            "2022-11-21 17:06:11,419 epoch 5 - iter 21/71 - loss 0.01993065 - samples/sec: 2995.90 - lr: 0.100000\n",
            "2022-11-21 17:06:11,500 epoch 5 - iter 28/71 - loss 0.02002456 - samples/sec: 2875.66 - lr: 0.100000\n",
            "2022-11-21 17:06:11,578 epoch 5 - iter 35/71 - loss 0.01989555 - samples/sec: 2978.13 - lr: 0.100000\n",
            "2022-11-21 17:06:11,658 epoch 5 - iter 42/71 - loss 0.01992950 - samples/sec: 2906.74 - lr: 0.100000\n",
            "2022-11-21 17:06:11,737 epoch 5 - iter 49/71 - loss 0.01988690 - samples/sec: 2903.38 - lr: 0.100000\n",
            "2022-11-21 17:06:11,819 epoch 5 - iter 56/71 - loss 0.01979756 - samples/sec: 2838.45 - lr: 0.100000\n",
            "2022-11-21 17:06:11,901 epoch 5 - iter 63/71 - loss 0.01980694 - samples/sec: 2817.54 - lr: 0.100000\n",
            "2022-11-21 17:06:11,982 epoch 5 - iter 70/71 - loss 0.01980896 - samples/sec: 2883.33 - lr: 0.100000\n",
            "2022-11-21 17:06:11,986 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:11,988 EPOCH 5 done: loss 0.0200 - lr 0.1000000\n",
            "2022-11-21 17:06:12,120 DEV : loss 0.02394159696996212 - f1-score (micro avg)  0.5415\n",
            "2022-11-21 17:06:12,127 BAD EPOCHS (no improvement): 0\n",
            "2022-11-21 17:06:12,129 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:12,211 epoch 6 - iter 7/71 - loss 0.01983070 - samples/sec: 2877.86 - lr: 0.100000\n",
            "2022-11-21 17:06:12,292 epoch 6 - iter 14/71 - loss 0.01923934 - samples/sec: 2835.58 - lr: 0.100000\n",
            "2022-11-21 17:06:12,372 epoch 6 - iter 21/71 - loss 0.01930983 - samples/sec: 2896.68 - lr: 0.100000\n",
            "2022-11-21 17:06:12,452 epoch 6 - iter 28/71 - loss 0.01942266 - samples/sec: 2923.44 - lr: 0.100000\n",
            "2022-11-21 17:06:12,534 epoch 6 - iter 35/71 - loss 0.01926220 - samples/sec: 2823.54 - lr: 0.100000\n",
            "2022-11-21 17:06:12,616 epoch 6 - iter 42/71 - loss 0.01943011 - samples/sec: 2835.24 - lr: 0.100000\n",
            "2022-11-21 17:06:12,699 epoch 6 - iter 49/71 - loss 0.01944453 - samples/sec: 2809.51 - lr: 0.100000\n",
            "2022-11-21 17:06:12,784 epoch 6 - iter 56/71 - loss 0.01948973 - samples/sec: 2720.23 - lr: 0.100000\n",
            "2022-11-21 17:06:12,866 epoch 6 - iter 63/71 - loss 0.01947184 - samples/sec: 2833.99 - lr: 0.100000\n",
            "2022-11-21 17:06:12,947 epoch 6 - iter 70/71 - loss 0.01943772 - samples/sec: 2866.54 - lr: 0.100000\n",
            "2022-11-21 17:06:12,952 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:12,954 EPOCH 6 done: loss 0.0197 - lr 0.1000000\n",
            "2022-11-21 17:06:13,082 DEV : loss 0.024199146777391434 - f1-score (micro avg)  0.5199\n",
            "2022-11-21 17:06:13,088 BAD EPOCHS (no improvement): 1\n",
            "2022-11-21 17:06:13,090 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:13,175 epoch 7 - iter 7/71 - loss 0.01932788 - samples/sec: 2778.72 - lr: 0.100000\n",
            "2022-11-21 17:06:13,259 epoch 7 - iter 14/71 - loss 0.01934346 - samples/sec: 2729.15 - lr: 0.100000\n",
            "2022-11-21 17:06:13,340 epoch 7 - iter 21/71 - loss 0.01926030 - samples/sec: 2854.79 - lr: 0.100000\n",
            "2022-11-21 17:06:13,421 epoch 7 - iter 28/71 - loss 0.01914042 - samples/sec: 2871.77 - lr: 0.100000\n",
            "2022-11-21 17:06:13,498 epoch 7 - iter 35/71 - loss 0.01901542 - samples/sec: 3005.65 - lr: 0.100000\n",
            "2022-11-21 17:06:13,579 epoch 7 - iter 42/71 - loss 0.01897165 - samples/sec: 2886.81 - lr: 0.100000\n",
            "2022-11-21 17:06:13,662 epoch 7 - iter 49/71 - loss 0.01901424 - samples/sec: 2779.32 - lr: 0.100000\n",
            "2022-11-21 17:06:13,742 epoch 7 - iter 56/71 - loss 0.01906392 - samples/sec: 2875.87 - lr: 0.100000\n",
            "2022-11-21 17:06:13,825 epoch 7 - iter 63/71 - loss 0.01912822 - samples/sec: 2828.64 - lr: 0.100000\n",
            "2022-11-21 17:06:13,908 epoch 7 - iter 70/71 - loss 0.01918411 - samples/sec: 2788.15 - lr: 0.100000\n",
            "2022-11-21 17:06:13,912 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:13,915 EPOCH 7 done: loss 0.0194 - lr 0.1000000\n",
            "2022-11-21 17:06:14,039 DEV : loss 0.028781218454241753 - f1-score (micro avg)  0.4657\n",
            "2022-11-21 17:06:14,045 BAD EPOCHS (no improvement): 2\n",
            "2022-11-21 17:06:14,047 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:14,128 epoch 8 - iter 7/71 - loss 0.01968510 - samples/sec: 2921.22 - lr: 0.100000\n",
            "2022-11-21 17:06:14,209 epoch 8 - iter 14/71 - loss 0.01886858 - samples/sec: 2820.15 - lr: 0.100000\n",
            "2022-11-21 17:06:14,289 epoch 8 - iter 21/71 - loss 0.01893924 - samples/sec: 2894.72 - lr: 0.100000\n",
            "2022-11-21 17:06:14,372 epoch 8 - iter 28/71 - loss 0.01899662 - samples/sec: 2811.39 - lr: 0.100000\n",
            "2022-11-21 17:06:14,452 epoch 8 - iter 35/71 - loss 0.01904496 - samples/sec: 2878.10 - lr: 0.100000\n",
            "2022-11-21 17:06:14,536 epoch 8 - iter 42/71 - loss 0.01894038 - samples/sec: 2729.44 - lr: 0.100000\n",
            "2022-11-21 17:06:14,616 epoch 8 - iter 49/71 - loss 0.01888536 - samples/sec: 2938.05 - lr: 0.100000\n",
            "2022-11-21 17:06:14,696 epoch 8 - iter 56/71 - loss 0.01896567 - samples/sec: 2887.33 - lr: 0.100000\n",
            "2022-11-21 17:06:14,773 epoch 8 - iter 63/71 - loss 0.01889038 - samples/sec: 2994.14 - lr: 0.100000\n",
            "2022-11-21 17:06:14,850 epoch 8 - iter 70/71 - loss 0.01885567 - samples/sec: 3012.95 - lr: 0.100000\n",
            "2022-11-21 17:06:14,854 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:14,856 EPOCH 8 done: loss 0.0194 - lr 0.1000000\n",
            "2022-11-21 17:06:14,979 DEV : loss 0.027149347588419914 - f1-score (micro avg)  0.5235\n",
            "2022-11-21 17:06:14,984 BAD EPOCHS (no improvement): 3\n",
            "2022-11-21 17:06:14,987 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:15,066 epoch 9 - iter 7/71 - loss 0.01966015 - samples/sec: 2979.50 - lr: 0.100000\n",
            "2022-11-21 17:06:15,145 epoch 9 - iter 14/71 - loss 0.01886798 - samples/sec: 2920.34 - lr: 0.100000\n",
            "2022-11-21 17:06:15,229 epoch 9 - iter 21/71 - loss 0.01891276 - samples/sec: 2774.47 - lr: 0.100000\n",
            "2022-11-21 17:06:15,311 epoch 9 - iter 28/71 - loss 0.01892244 - samples/sec: 2797.48 - lr: 0.100000\n",
            "2022-11-21 17:06:15,392 epoch 9 - iter 35/71 - loss 0.01873654 - samples/sec: 2872.25 - lr: 0.100000\n",
            "2022-11-21 17:06:15,468 epoch 9 - iter 42/71 - loss 0.01880128 - samples/sec: 3022.05 - lr: 0.100000\n",
            "2022-11-21 17:06:15,546 epoch 9 - iter 49/71 - loss 0.01886703 - samples/sec: 2966.81 - lr: 0.100000\n",
            "2022-11-21 17:06:15,624 epoch 9 - iter 56/71 - loss 0.01878996 - samples/sec: 2992.72 - lr: 0.100000\n",
            "2022-11-21 17:06:15,701 epoch 9 - iter 63/71 - loss 0.01879115 - samples/sec: 2979.02 - lr: 0.100000\n",
            "2022-11-21 17:06:15,777 epoch 9 - iter 70/71 - loss 0.01884833 - samples/sec: 3064.06 - lr: 0.100000\n",
            "2022-11-21 17:06:15,781 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:15,783 EPOCH 9 done: loss 0.0192 - lr 0.1000000\n",
            "2022-11-21 17:06:15,907 DEV : loss 0.02609359845519066 - f1-score (micro avg)  0.5415\n",
            "Epoch     9: reducing learning rate of group 0 to 5.0000e-02.\n",
            "2022-11-21 17:06:15,913 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:06:15,915 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:15,996 epoch 10 - iter 7/71 - loss 0.01855769 - samples/sec: 2910.39 - lr: 0.050000\n",
            "2022-11-21 17:06:16,074 epoch 10 - iter 14/71 - loss 0.01827195 - samples/sec: 2948.98 - lr: 0.050000\n",
            "2022-11-21 17:06:16,154 epoch 10 - iter 21/71 - loss 0.01823248 - samples/sec: 2914.64 - lr: 0.050000\n",
            "2022-11-21 17:06:16,238 epoch 10 - iter 28/71 - loss 0.01840519 - samples/sec: 2741.26 - lr: 0.050000\n",
            "2022-11-21 17:06:16,317 epoch 10 - iter 35/71 - loss 0.01834624 - samples/sec: 2926.70 - lr: 0.050000\n",
            "2022-11-21 17:06:16,398 epoch 10 - iter 42/71 - loss 0.01819724 - samples/sec: 2854.00 - lr: 0.050000\n",
            "2022-11-21 17:06:16,477 epoch 10 - iter 49/71 - loss 0.01838685 - samples/sec: 2924.37 - lr: 0.050000\n",
            "2022-11-21 17:06:16,559 epoch 10 - iter 56/71 - loss 0.01838276 - samples/sec: 2848.55 - lr: 0.050000\n",
            "2022-11-21 17:06:16,636 epoch 10 - iter 63/71 - loss 0.01844597 - samples/sec: 2963.58 - lr: 0.050000\n",
            "2022-11-21 17:06:16,717 epoch 10 - iter 70/71 - loss 0.01836885 - samples/sec: 2874.06 - lr: 0.050000\n",
            "2022-11-21 17:06:16,721 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:16,723 EPOCH 10 done: loss 0.0187 - lr 0.0500000\n",
            "2022-11-21 17:06:16,847 DEV : loss 0.025975162163376808 - f1-score (micro avg)  0.4368\n",
            "2022-11-21 17:06:16,853 BAD EPOCHS (no improvement): 1\n",
            "2022-11-21 17:06:16,855 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:16,935 epoch 11 - iter 7/71 - loss 0.01875465 - samples/sec: 2917.98 - lr: 0.050000\n",
            "2022-11-21 17:06:17,014 epoch 11 - iter 14/71 - loss 0.01813264 - samples/sec: 2928.97 - lr: 0.050000\n",
            "2022-11-21 17:06:17,094 epoch 11 - iter 21/71 - loss 0.01797635 - samples/sec: 2887.72 - lr: 0.050000\n",
            "2022-11-21 17:06:17,178 epoch 11 - iter 28/71 - loss 0.01802112 - samples/sec: 2840.25 - lr: 0.050000\n",
            "2022-11-21 17:06:17,260 epoch 11 - iter 35/71 - loss 0.01812330 - samples/sec: 2800.03 - lr: 0.050000\n",
            "2022-11-21 17:06:17,342 epoch 11 - iter 42/71 - loss 0.01823322 - samples/sec: 2838.42 - lr: 0.050000\n",
            "2022-11-21 17:06:17,421 epoch 11 - iter 49/71 - loss 0.01822265 - samples/sec: 2927.18 - lr: 0.050000\n",
            "2022-11-21 17:06:17,500 epoch 11 - iter 56/71 - loss 0.01814845 - samples/sec: 2961.48 - lr: 0.050000\n",
            "2022-11-21 17:06:17,578 epoch 11 - iter 63/71 - loss 0.01821038 - samples/sec: 2971.94 - lr: 0.050000\n",
            "2022-11-21 17:06:17,658 epoch 11 - iter 70/71 - loss 0.01815333 - samples/sec: 2896.38 - lr: 0.050000\n",
            "2022-11-21 17:06:17,662 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:17,664 EPOCH 11 done: loss 0.0186 - lr 0.0500000\n",
            "2022-11-21 17:06:17,788 DEV : loss 0.02449801377952099 - f1-score (micro avg)  0.5018\n",
            "2022-11-21 17:06:17,793 BAD EPOCHS (no improvement): 2\n",
            "2022-11-21 17:06:17,796 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:17,875 epoch 12 - iter 7/71 - loss 0.01820191 - samples/sec: 2930.89 - lr: 0.050000\n",
            "2022-11-21 17:06:17,955 epoch 12 - iter 14/71 - loss 0.01777558 - samples/sec: 2899.77 - lr: 0.050000\n",
            "2022-11-21 17:06:18,037 epoch 12 - iter 21/71 - loss 0.01796082 - samples/sec: 2826.35 - lr: 0.050000\n",
            "2022-11-21 17:06:18,115 epoch 12 - iter 28/71 - loss 0.01804686 - samples/sec: 2947.71 - lr: 0.050000\n",
            "2022-11-21 17:06:18,195 epoch 12 - iter 35/71 - loss 0.01805888 - samples/sec: 2914.36 - lr: 0.050000\n",
            "2022-11-21 17:06:18,278 epoch 12 - iter 42/71 - loss 0.01795592 - samples/sec: 2793.02 - lr: 0.050000\n",
            "2022-11-21 17:06:18,363 epoch 12 - iter 49/71 - loss 0.01792306 - samples/sec: 2708.21 - lr: 0.050000\n",
            "2022-11-21 17:06:18,444 epoch 12 - iter 56/71 - loss 0.01795429 - samples/sec: 2868.94 - lr: 0.050000\n",
            "2022-11-21 17:06:18,525 epoch 12 - iter 63/71 - loss 0.01793488 - samples/sec: 2865.56 - lr: 0.050000\n",
            "2022-11-21 17:06:18,603 epoch 12 - iter 70/71 - loss 0.01800397 - samples/sec: 2935.13 - lr: 0.050000\n",
            "2022-11-21 17:06:18,608 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:18,609 EPOCH 12 done: loss 0.0186 - lr 0.0500000\n",
            "2022-11-21 17:06:18,738 DEV : loss 0.02466290257871151 - f1-score (micro avg)  0.5199\n",
            "2022-11-21 17:06:18,745 BAD EPOCHS (no improvement): 3\n",
            "2022-11-21 17:06:18,747 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:18,831 epoch 13 - iter 7/71 - loss 0.01805097 - samples/sec: 2790.31 - lr: 0.050000\n",
            "2022-11-21 17:06:18,911 epoch 13 - iter 14/71 - loss 0.01796064 - samples/sec: 2902.96 - lr: 0.050000\n",
            "2022-11-21 17:06:18,990 epoch 13 - iter 21/71 - loss 0.01786956 - samples/sec: 2919.34 - lr: 0.050000\n",
            "2022-11-21 17:06:19,071 epoch 13 - iter 28/71 - loss 0.01778636 - samples/sec: 2878.04 - lr: 0.050000\n",
            "2022-11-21 17:06:19,154 epoch 13 - iter 35/71 - loss 0.01792030 - samples/sec: 2767.67 - lr: 0.050000\n",
            "2022-11-21 17:06:19,239 epoch 13 - iter 42/71 - loss 0.01795240 - samples/sec: 2746.04 - lr: 0.050000\n",
            "2022-11-21 17:06:19,321 epoch 13 - iter 49/71 - loss 0.01788919 - samples/sec: 2845.09 - lr: 0.050000\n",
            "2022-11-21 17:06:19,400 epoch 13 - iter 56/71 - loss 0.01789806 - samples/sec: 2904.11 - lr: 0.050000\n",
            "2022-11-21 17:06:19,481 epoch 13 - iter 63/71 - loss 0.01791398 - samples/sec: 2871.60 - lr: 0.050000\n",
            "2022-11-21 17:06:19,564 epoch 13 - iter 70/71 - loss 0.01795030 - samples/sec: 2770.81 - lr: 0.050000\n",
            "2022-11-21 17:06:19,569 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:19,571 EPOCH 13 done: loss 0.0181 - lr 0.0500000\n",
            "2022-11-21 17:06:19,697 DEV : loss 0.02437007427215576 - f1-score (micro avg)  0.4982\n",
            "Epoch    13: reducing learning rate of group 0 to 2.5000e-02.\n",
            "2022-11-21 17:06:19,703 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:06:19,706 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:19,793 epoch 14 - iter 7/71 - loss 0.01808212 - samples/sec: 2680.95 - lr: 0.025000\n",
            "2022-11-21 17:06:19,874 epoch 14 - iter 14/71 - loss 0.01763886 - samples/sec: 2858.18 - lr: 0.025000\n",
            "2022-11-21 17:06:19,955 epoch 14 - iter 21/71 - loss 0.01758489 - samples/sec: 2901.23 - lr: 0.025000\n",
            "2022-11-21 17:06:20,038 epoch 14 - iter 28/71 - loss 0.01767206 - samples/sec: 2788.53 - lr: 0.025000\n",
            "2022-11-21 17:06:20,127 epoch 14 - iter 35/71 - loss 0.01770104 - samples/sec: 2590.34 - lr: 0.025000\n",
            "2022-11-21 17:06:20,208 epoch 14 - iter 42/71 - loss 0.01765621 - samples/sec: 2853.96 - lr: 0.025000\n",
            "2022-11-21 17:06:20,290 epoch 14 - iter 49/71 - loss 0.01759461 - samples/sec: 2831.82 - lr: 0.025000\n",
            "2022-11-21 17:06:20,371 epoch 14 - iter 56/71 - loss 0.01757995 - samples/sec: 2888.44 - lr: 0.025000\n",
            "2022-11-21 17:06:20,451 epoch 14 - iter 63/71 - loss 0.01760091 - samples/sec: 2869.20 - lr: 0.025000\n",
            "2022-11-21 17:06:20,541 epoch 14 - iter 70/71 - loss 0.01771869 - samples/sec: 2600.68 - lr: 0.025000\n",
            "2022-11-21 17:06:20,545 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:20,547 EPOCH 14 done: loss 0.0179 - lr 0.0250000\n",
            "2022-11-21 17:06:20,683 DEV : loss 0.02434701658785343 - f1-score (micro avg)  0.4838\n",
            "2022-11-21 17:06:20,690 BAD EPOCHS (no improvement): 1\n",
            "2022-11-21 17:06:20,692 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:20,772 epoch 15 - iter 7/71 - loss 0.01715472 - samples/sec: 2901.76 - lr: 0.025000\n",
            "2022-11-21 17:06:20,852 epoch 15 - iter 14/71 - loss 0.01724968 - samples/sec: 2905.23 - lr: 0.025000\n",
            "2022-11-21 17:06:20,933 epoch 15 - iter 21/71 - loss 0.01731401 - samples/sec: 2867.43 - lr: 0.025000\n",
            "2022-11-21 17:06:21,014 epoch 15 - iter 28/71 - loss 0.01775998 - samples/sec: 2842.82 - lr: 0.025000\n",
            "2022-11-21 17:06:21,095 epoch 15 - iter 35/71 - loss 0.01763201 - samples/sec: 2860.84 - lr: 0.025000\n",
            "2022-11-21 17:06:21,174 epoch 15 - iter 42/71 - loss 0.01763441 - samples/sec: 2952.96 - lr: 0.025000\n",
            "2022-11-21 17:06:21,253 epoch 15 - iter 49/71 - loss 0.01754702 - samples/sec: 2908.01 - lr: 0.025000\n",
            "2022-11-21 17:06:21,335 epoch 15 - iter 56/71 - loss 0.01750749 - samples/sec: 2829.56 - lr: 0.025000\n",
            "2022-11-21 17:06:21,417 epoch 15 - iter 63/71 - loss 0.01755104 - samples/sec: 2823.73 - lr: 0.025000\n",
            "2022-11-21 17:06:21,501 epoch 15 - iter 70/71 - loss 0.01764053 - samples/sec: 2788.83 - lr: 0.025000\n",
            "2022-11-21 17:06:21,505 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:21,507 EPOCH 15 done: loss 0.0179 - lr 0.0250000\n",
            "2022-11-21 17:06:21,636 DEV : loss 0.024451764300465584 - f1-score (micro avg)  0.491\n",
            "2022-11-21 17:06:21,642 BAD EPOCHS (no improvement): 2\n",
            "2022-11-21 17:06:21,644 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:21,725 epoch 16 - iter 7/71 - loss 0.01772634 - samples/sec: 2874.90 - lr: 0.025000\n",
            "2022-11-21 17:06:21,805 epoch 16 - iter 14/71 - loss 0.01744208 - samples/sec: 2921.31 - lr: 0.025000\n",
            "2022-11-21 17:06:21,888 epoch 16 - iter 21/71 - loss 0.01720050 - samples/sec: 2785.86 - lr: 0.025000\n",
            "2022-11-21 17:06:21,969 epoch 16 - iter 28/71 - loss 0.01727251 - samples/sec: 2841.76 - lr: 0.025000\n",
            "2022-11-21 17:06:22,051 epoch 16 - iter 35/71 - loss 0.01737419 - samples/sec: 2828.29 - lr: 0.025000\n",
            "2022-11-21 17:06:22,131 epoch 16 - iter 42/71 - loss 0.01758506 - samples/sec: 2898.93 - lr: 0.025000\n",
            "2022-11-21 17:06:22,212 epoch 16 - iter 49/71 - loss 0.01763505 - samples/sec: 2858.72 - lr: 0.025000\n",
            "2022-11-21 17:06:22,298 epoch 16 - iter 56/71 - loss 0.01762201 - samples/sec: 2710.20 - lr: 0.025000\n",
            "2022-11-21 17:06:22,380 epoch 16 - iter 63/71 - loss 0.01766574 - samples/sec: 2840.28 - lr: 0.025000\n",
            "2022-11-21 17:06:22,460 epoch 16 - iter 70/71 - loss 0.01760660 - samples/sec: 2901.67 - lr: 0.025000\n",
            "2022-11-21 17:06:22,464 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:22,466 EPOCH 16 done: loss 0.0178 - lr 0.0250000\n",
            "2022-11-21 17:06:22,594 DEV : loss 0.024495355784893036 - f1-score (micro avg)  0.4982\n",
            "2022-11-21 17:06:22,599 BAD EPOCHS (no improvement): 3\n",
            "2022-11-21 17:06:22,603 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:22,693 epoch 17 - iter 7/71 - loss 0.01757630 - samples/sec: 2588.66 - lr: 0.025000\n",
            "2022-11-21 17:06:22,773 epoch 17 - iter 14/71 - loss 0.01726606 - samples/sec: 2884.09 - lr: 0.025000\n",
            "2022-11-21 17:06:22,855 epoch 17 - iter 21/71 - loss 0.01736031 - samples/sec: 2844.16 - lr: 0.025000\n",
            "2022-11-21 17:06:22,934 epoch 17 - iter 28/71 - loss 0.01737096 - samples/sec: 2936.03 - lr: 0.025000\n",
            "2022-11-21 17:06:23,013 epoch 17 - iter 35/71 - loss 0.01734365 - samples/sec: 2942.62 - lr: 0.025000\n",
            "2022-11-21 17:06:23,092 epoch 17 - iter 42/71 - loss 0.01736409 - samples/sec: 2939.94 - lr: 0.025000\n",
            "2022-11-21 17:06:23,175 epoch 17 - iter 49/71 - loss 0.01746831 - samples/sec: 2783.17 - lr: 0.025000\n",
            "2022-11-21 17:06:23,257 epoch 17 - iter 56/71 - loss 0.01749692 - samples/sec: 2823.21 - lr: 0.025000\n",
            "2022-11-21 17:06:23,340 epoch 17 - iter 63/71 - loss 0.01751233 - samples/sec: 2802.83 - lr: 0.025000\n",
            "2022-11-21 17:06:23,424 epoch 17 - iter 70/71 - loss 0.01752765 - samples/sec: 2748.79 - lr: 0.025000\n",
            "2022-11-21 17:06:23,429 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:23,430 EPOCH 17 done: loss 0.0179 - lr 0.0250000\n",
            "2022-11-21 17:06:23,564 DEV : loss 0.024940000846982002 - f1-score (micro avg)  0.4874\n",
            "Epoch    17: reducing learning rate of group 0 to 1.2500e-02.\n",
            "2022-11-21 17:06:23,571 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:06:23,573 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:23,659 epoch 18 - iter 7/71 - loss 0.01858113 - samples/sec: 2748.36 - lr: 0.012500\n",
            "2022-11-21 17:06:23,743 epoch 18 - iter 14/71 - loss 0.01828991 - samples/sec: 2759.78 - lr: 0.012500\n",
            "2022-11-21 17:06:23,824 epoch 18 - iter 21/71 - loss 0.01781664 - samples/sec: 2860.46 - lr: 0.012500\n",
            "2022-11-21 17:06:23,905 epoch 18 - iter 28/71 - loss 0.01761534 - samples/sec: 2885.60 - lr: 0.012500\n",
            "2022-11-21 17:06:23,985 epoch 18 - iter 35/71 - loss 0.01736540 - samples/sec: 2895.07 - lr: 0.012500\n",
            "2022-11-21 17:06:24,067 epoch 18 - iter 42/71 - loss 0.01743591 - samples/sec: 2805.66 - lr: 0.012500\n",
            "2022-11-21 17:06:24,147 epoch 18 - iter 49/71 - loss 0.01751107 - samples/sec: 2901.39 - lr: 0.012500\n",
            "2022-11-21 17:06:24,231 epoch 18 - iter 56/71 - loss 0.01742758 - samples/sec: 2747.97 - lr: 0.012500\n",
            "2022-11-21 17:06:24,320 epoch 18 - iter 63/71 - loss 0.01740805 - samples/sec: 2598.26 - lr: 0.012500\n",
            "2022-11-21 17:06:24,400 epoch 18 - iter 70/71 - loss 0.01747616 - samples/sec: 2897.91 - lr: 0.012500\n",
            "2022-11-21 17:06:24,405 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:24,407 EPOCH 18 done: loss 0.0176 - lr 0.0125000\n",
            "2022-11-21 17:06:24,538 DEV : loss 0.02461555413901806 - f1-score (micro avg)  0.491\n",
            "2022-11-21 17:06:24,545 BAD EPOCHS (no improvement): 1\n",
            "2022-11-21 17:06:24,547 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:24,631 epoch 19 - iter 7/71 - loss 0.01790111 - samples/sec: 2785.70 - lr: 0.012500\n",
            "2022-11-21 17:06:24,712 epoch 19 - iter 14/71 - loss 0.01804092 - samples/sec: 2859.55 - lr: 0.012500\n",
            "2022-11-21 17:06:24,821 epoch 19 - iter 21/71 - loss 0.01755350 - samples/sec: 2112.87 - lr: 0.012500\n",
            "2022-11-21 17:06:24,938 epoch 19 - iter 28/71 - loss 0.01744772 - samples/sec: 2001.30 - lr: 0.012500\n",
            "2022-11-21 17:06:25,058 epoch 19 - iter 35/71 - loss 0.01748294 - samples/sec: 1901.65 - lr: 0.012500\n",
            "2022-11-21 17:06:25,172 epoch 19 - iter 42/71 - loss 0.01761744 - samples/sec: 2031.57 - lr: 0.012500\n",
            "2022-11-21 17:06:25,292 epoch 19 - iter 49/71 - loss 0.01748237 - samples/sec: 1926.60 - lr: 0.012500\n",
            "2022-11-21 17:06:25,394 epoch 19 - iter 56/71 - loss 0.01743042 - samples/sec: 2264.63 - lr: 0.012500\n",
            "2022-11-21 17:06:25,481 epoch 19 - iter 63/71 - loss 0.01735783 - samples/sec: 2692.53 - lr: 0.012500\n",
            "2022-11-21 17:06:25,571 epoch 19 - iter 70/71 - loss 0.01740776 - samples/sec: 2617.82 - lr: 0.012500\n",
            "2022-11-21 17:06:25,577 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:25,579 EPOCH 19 done: loss 0.0179 - lr 0.0125000\n",
            "2022-11-21 17:06:25,726 DEV : loss 0.024507775902748108 - f1-score (micro avg)  0.5018\n",
            "2022-11-21 17:06:25,734 BAD EPOCHS (no improvement): 2\n",
            "2022-11-21 17:06:25,736 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:25,886 epoch 20 - iter 7/71 - loss 0.01810809 - samples/sec: 1610.11 - lr: 0.012500\n",
            "2022-11-21 17:06:26,035 epoch 20 - iter 14/71 - loss 0.01788340 - samples/sec: 1545.70 - lr: 0.012500\n",
            "2022-11-21 17:06:26,134 epoch 20 - iter 21/71 - loss 0.01793768 - samples/sec: 2307.79 - lr: 0.012500\n",
            "2022-11-21 17:06:26,228 epoch 20 - iter 28/71 - loss 0.01784608 - samples/sec: 2464.62 - lr: 0.012500\n",
            "2022-11-21 17:06:26,313 epoch 20 - iter 35/71 - loss 0.01762521 - samples/sec: 2757.87 - lr: 0.012500\n",
            "2022-11-21 17:06:26,422 epoch 20 - iter 42/71 - loss 0.01771808 - samples/sec: 2100.35 - lr: 0.012500\n",
            "2022-11-21 17:06:26,516 epoch 20 - iter 49/71 - loss 0.01753335 - samples/sec: 2499.21 - lr: 0.012500\n",
            "2022-11-21 17:06:26,603 epoch 20 - iter 56/71 - loss 0.01745174 - samples/sec: 2632.08 - lr: 0.012500\n",
            "2022-11-21 17:06:26,690 epoch 20 - iter 63/71 - loss 0.01741063 - samples/sec: 2680.17 - lr: 0.012500\n",
            "2022-11-21 17:06:26,812 epoch 20 - iter 70/71 - loss 0.01740233 - samples/sec: 1886.02 - lr: 0.012500\n",
            "2022-11-21 17:06:26,818 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:26,821 EPOCH 20 done: loss 0.0176 - lr 0.0125000\n",
            "2022-11-21 17:06:27,034 DEV : loss 0.024493005126714706 - f1-score (micro avg)  0.5018\n",
            "2022-11-21 17:06:27,047 BAD EPOCHS (no improvement): 3\n",
            "2022-11-21 17:06:27,049 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:27,153 epoch 21 - iter 7/71 - loss 0.01758692 - samples/sec: 2366.21 - lr: 0.012500\n",
            "2022-11-21 17:06:27,238 epoch 21 - iter 14/71 - loss 0.01750777 - samples/sec: 2758.23 - lr: 0.012500\n",
            "2022-11-21 17:06:27,324 epoch 21 - iter 21/71 - loss 0.01739177 - samples/sec: 2696.34 - lr: 0.012500\n",
            "2022-11-21 17:06:27,422 epoch 21 - iter 28/71 - loss 0.01742888 - samples/sec: 2390.77 - lr: 0.012500\n",
            "2022-11-21 17:06:27,524 epoch 21 - iter 35/71 - loss 0.01741713 - samples/sec: 2260.82 - lr: 0.012500\n",
            "2022-11-21 17:06:27,613 epoch 21 - iter 42/71 - loss 0.01737206 - samples/sec: 2591.17 - lr: 0.012500\n",
            "2022-11-21 17:06:27,706 epoch 21 - iter 49/71 - loss 0.01737224 - samples/sec: 2489.08 - lr: 0.012500\n",
            "2022-11-21 17:06:27,791 epoch 21 - iter 56/71 - loss 0.01742031 - samples/sec: 2768.45 - lr: 0.012500\n",
            "2022-11-21 17:06:27,879 epoch 21 - iter 63/71 - loss 0.01737110 - samples/sec: 2636.66 - lr: 0.012500\n",
            "2022-11-21 17:06:27,966 epoch 21 - iter 70/71 - loss 0.01738868 - samples/sec: 2680.72 - lr: 0.012500\n",
            "2022-11-21 17:06:27,972 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:27,973 EPOCH 21 done: loss 0.0177 - lr 0.0125000\n",
            "2022-11-21 17:06:28,145 DEV : loss 0.024466922506690025 - f1-score (micro avg)  0.509\n",
            "Epoch    21: reducing learning rate of group 0 to 6.2500e-03.\n",
            "2022-11-21 17:06:28,157 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:06:28,159 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:28,297 epoch 22 - iter 7/71 - loss 0.01702313 - samples/sec: 1713.98 - lr: 0.006250\n",
            "2022-11-21 17:06:28,416 epoch 22 - iter 14/71 - loss 0.01736981 - samples/sec: 2153.80 - lr: 0.006250\n",
            "2022-11-21 17:06:28,513 epoch 22 - iter 21/71 - loss 0.01739181 - samples/sec: 2434.57 - lr: 0.006250\n",
            "2022-11-21 17:06:28,598 epoch 22 - iter 28/71 - loss 0.01738191 - samples/sec: 2729.68 - lr: 0.006250\n",
            "2022-11-21 17:06:28,685 epoch 22 - iter 35/71 - loss 0.01728659 - samples/sec: 2693.78 - lr: 0.006250\n",
            "2022-11-21 17:06:28,768 epoch 22 - iter 42/71 - loss 0.01714473 - samples/sec: 2849.62 - lr: 0.006250\n",
            "2022-11-21 17:06:28,862 epoch 22 - iter 49/71 - loss 0.01726795 - samples/sec: 2461.64 - lr: 0.006250\n",
            "2022-11-21 17:06:29,013 epoch 22 - iter 56/71 - loss 0.01725684 - samples/sec: 1523.97 - lr: 0.006250\n",
            "2022-11-21 17:06:29,197 epoch 22 - iter 63/71 - loss 0.01717731 - samples/sec: 1246.12 - lr: 0.006250\n",
            "2022-11-21 17:06:29,354 epoch 22 - iter 70/71 - loss 0.01734264 - samples/sec: 1480.51 - lr: 0.006250\n",
            "2022-11-21 17:06:29,362 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:29,367 EPOCH 22 done: loss 0.0177 - lr 0.0062500\n",
            "2022-11-21 17:06:29,592 DEV : loss 0.024597695097327232 - f1-score (micro avg)  0.5018\n",
            "2022-11-21 17:06:29,600 BAD EPOCHS (no improvement): 1\n",
            "2022-11-21 17:06:29,603 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:29,704 epoch 23 - iter 7/71 - loss 0.01733218 - samples/sec: 2298.32 - lr: 0.006250\n",
            "2022-11-21 17:06:29,785 epoch 23 - iter 14/71 - loss 0.01700669 - samples/sec: 2882.50 - lr: 0.006250\n",
            "2022-11-21 17:06:29,867 epoch 23 - iter 21/71 - loss 0.01714152 - samples/sec: 2816.06 - lr: 0.006250\n",
            "2022-11-21 17:06:29,965 epoch 23 - iter 28/71 - loss 0.01720757 - samples/sec: 2359.32 - lr: 0.006250\n",
            "2022-11-21 17:06:30,082 epoch 23 - iter 35/71 - loss 0.01710297 - samples/sec: 1974.49 - lr: 0.006250\n",
            "2022-11-21 17:06:30,167 epoch 23 - iter 42/71 - loss 0.01710117 - samples/sec: 2771.38 - lr: 0.006250\n",
            "2022-11-21 17:06:30,249 epoch 23 - iter 49/71 - loss 0.01711930 - samples/sec: 2848.73 - lr: 0.006250\n",
            "2022-11-21 17:06:30,329 epoch 23 - iter 56/71 - loss 0.01721271 - samples/sec: 2884.63 - lr: 0.006250\n",
            "2022-11-21 17:06:30,425 epoch 23 - iter 63/71 - loss 0.01730814 - samples/sec: 2396.16 - lr: 0.006250\n",
            "2022-11-21 17:06:30,522 epoch 23 - iter 70/71 - loss 0.01732109 - samples/sec: 2397.08 - lr: 0.006250\n",
            "2022-11-21 17:06:30,527 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:30,528 EPOCH 23 done: loss 0.0175 - lr 0.0062500\n",
            "2022-11-21 17:06:30,685 DEV : loss 0.024629075080156326 - f1-score (micro avg)  0.4982\n",
            "2022-11-21 17:06:30,696 BAD EPOCHS (no improvement): 2\n",
            "2022-11-21 17:06:30,698 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:30,885 epoch 24 - iter 7/71 - loss 0.01745118 - samples/sec: 1232.31 - lr: 0.006250\n",
            "2022-11-21 17:06:30,967 epoch 24 - iter 14/71 - loss 0.01756399 - samples/sec: 2839.70 - lr: 0.006250\n",
            "2022-11-21 17:06:31,048 epoch 24 - iter 21/71 - loss 0.01699516 - samples/sec: 2826.52 - lr: 0.006250\n",
            "2022-11-21 17:06:31,155 epoch 24 - iter 28/71 - loss 0.01691070 - samples/sec: 2155.94 - lr: 0.006250\n",
            "2022-11-21 17:06:31,383 epoch 24 - iter 35/71 - loss 0.01699467 - samples/sec: 1004.59 - lr: 0.006250\n",
            "2022-11-21 17:06:31,498 epoch 24 - iter 42/71 - loss 0.01709789 - samples/sec: 2118.61 - lr: 0.006250\n",
            "2022-11-21 17:06:31,601 epoch 24 - iter 49/71 - loss 0.01713151 - samples/sec: 2331.32 - lr: 0.006250\n",
            "2022-11-21 17:06:31,692 epoch 24 - iter 56/71 - loss 0.01724283 - samples/sec: 2561.75 - lr: 0.006250\n",
            "2022-11-21 17:06:31,784 epoch 24 - iter 63/71 - loss 0.01727140 - samples/sec: 2504.72 - lr: 0.006250\n",
            "2022-11-21 17:06:31,870 epoch 24 - iter 70/71 - loss 0.01730177 - samples/sec: 2768.15 - lr: 0.006250\n",
            "2022-11-21 17:06:31,876 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:31,879 EPOCH 24 done: loss 0.0176 - lr 0.0062500\n",
            "2022-11-21 17:06:32,023 DEV : loss 0.02466200478374958 - f1-score (micro avg)  0.5018\n",
            "2022-11-21 17:06:32,033 BAD EPOCHS (no improvement): 3\n",
            "2022-11-21 17:06:32,035 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:32,138 epoch 25 - iter 7/71 - loss 0.01819756 - samples/sec: 2295.47 - lr: 0.006250\n",
            "2022-11-21 17:06:32,236 epoch 25 - iter 14/71 - loss 0.01770744 - samples/sec: 2369.99 - lr: 0.006250\n",
            "2022-11-21 17:06:32,331 epoch 25 - iter 21/71 - loss 0.01737206 - samples/sec: 2456.08 - lr: 0.006250\n",
            "2022-11-21 17:06:32,412 epoch 25 - iter 28/71 - loss 0.01717649 - samples/sec: 2863.12 - lr: 0.006250\n",
            "2022-11-21 17:06:32,507 epoch 25 - iter 35/71 - loss 0.01728888 - samples/sec: 2450.14 - lr: 0.006250\n",
            "2022-11-21 17:06:32,597 epoch 25 - iter 42/71 - loss 0.01723502 - samples/sec: 2593.77 - lr: 0.006250\n",
            "2022-11-21 17:06:32,681 epoch 25 - iter 49/71 - loss 0.01718881 - samples/sec: 2783.19 - lr: 0.006250\n",
            "2022-11-21 17:06:32,770 epoch 25 - iter 56/71 - loss 0.01719464 - samples/sec: 2608.58 - lr: 0.006250\n",
            "2022-11-21 17:06:32,854 epoch 25 - iter 63/71 - loss 0.01723684 - samples/sec: 2779.90 - lr: 0.006250\n",
            "2022-11-21 17:06:32,939 epoch 25 - iter 70/71 - loss 0.01729735 - samples/sec: 2747.78 - lr: 0.006250\n",
            "2022-11-21 17:06:32,945 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:32,947 EPOCH 25 done: loss 0.0174 - lr 0.0062500\n",
            "2022-11-21 17:06:33,102 DEV : loss 0.024611590430140495 - f1-score (micro avg)  0.5018\n",
            "Epoch    25: reducing learning rate of group 0 to 3.1250e-03.\n",
            "2022-11-21 17:06:33,110 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:06:33,113 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:33,213 epoch 26 - iter 7/71 - loss 0.01743881 - samples/sec: 2349.79 - lr: 0.003125\n",
            "2022-11-21 17:06:33,304 epoch 26 - iter 14/71 - loss 0.01732649 - samples/sec: 2540.57 - lr: 0.003125\n",
            "2022-11-21 17:06:33,389 epoch 26 - iter 21/71 - loss 0.01738408 - samples/sec: 2716.91 - lr: 0.003125\n",
            "2022-11-21 17:06:33,473 epoch 26 - iter 28/71 - loss 0.01749190 - samples/sec: 2782.74 - lr: 0.003125\n",
            "2022-11-21 17:06:33,569 epoch 26 - iter 35/71 - loss 0.01734712 - samples/sec: 2395.88 - lr: 0.003125\n",
            "2022-11-21 17:06:33,654 epoch 26 - iter 42/71 - loss 0.01739269 - samples/sec: 2756.04 - lr: 0.003125\n",
            "2022-11-21 17:06:33,738 epoch 26 - iter 49/71 - loss 0.01726977 - samples/sec: 2800.95 - lr: 0.003125\n",
            "2022-11-21 17:06:33,823 epoch 26 - iter 56/71 - loss 0.01724539 - samples/sec: 2745.29 - lr: 0.003125\n",
            "2022-11-21 17:06:33,913 epoch 26 - iter 63/71 - loss 0.01717634 - samples/sec: 2557.82 - lr: 0.003125\n",
            "2022-11-21 17:06:33,997 epoch 26 - iter 70/71 - loss 0.01726189 - samples/sec: 2796.83 - lr: 0.003125\n",
            "2022-11-21 17:06:34,003 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:34,004 EPOCH 26 done: loss 0.0177 - lr 0.0031250\n",
            "2022-11-21 17:06:34,135 DEV : loss 0.024663016200065613 - f1-score (micro avg)  0.5054\n",
            "2022-11-21 17:06:34,142 BAD EPOCHS (no improvement): 1\n",
            "2022-11-21 17:06:34,144 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:34,234 epoch 27 - iter 7/71 - loss 0.01658266 - samples/sec: 2656.11 - lr: 0.003125\n",
            "2022-11-21 17:06:34,359 epoch 27 - iter 14/71 - loss 0.01710474 - samples/sec: 1824.30 - lr: 0.003125\n",
            "2022-11-21 17:06:34,451 epoch 27 - iter 21/71 - loss 0.01729433 - samples/sec: 2512.75 - lr: 0.003125\n",
            "2022-11-21 17:06:34,534 epoch 27 - iter 28/71 - loss 0.01730893 - samples/sec: 2781.42 - lr: 0.003125\n",
            "2022-11-21 17:06:34,616 epoch 27 - iter 35/71 - loss 0.01733131 - samples/sec: 2857.97 - lr: 0.003125\n",
            "2022-11-21 17:06:34,699 epoch 27 - iter 42/71 - loss 0.01736387 - samples/sec: 2785.43 - lr: 0.003125\n",
            "2022-11-21 17:06:34,782 epoch 27 - iter 49/71 - loss 0.01733496 - samples/sec: 2809.20 - lr: 0.003125\n",
            "2022-11-21 17:06:34,863 epoch 27 - iter 56/71 - loss 0.01727741 - samples/sec: 2852.43 - lr: 0.003125\n",
            "2022-11-21 17:06:34,944 epoch 27 - iter 63/71 - loss 0.01722880 - samples/sec: 2839.68 - lr: 0.003125\n",
            "2022-11-21 17:06:35,024 epoch 27 - iter 70/71 - loss 0.01726800 - samples/sec: 2894.00 - lr: 0.003125\n",
            "2022-11-21 17:06:35,029 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:35,030 EPOCH 27 done: loss 0.0174 - lr 0.0031250\n",
            "2022-11-21 17:06:35,157 DEV : loss 0.024614185094833374 - f1-score (micro avg)  0.5018\n",
            "2022-11-21 17:06:35,163 BAD EPOCHS (no improvement): 2\n",
            "2022-11-21 17:06:35,166 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:35,248 epoch 28 - iter 7/71 - loss 0.01714702 - samples/sec: 2902.26 - lr: 0.003125\n",
            "2022-11-21 17:06:35,329 epoch 28 - iter 14/71 - loss 0.01718285 - samples/sec: 2884.31 - lr: 0.003125\n",
            "2022-11-21 17:06:35,412 epoch 28 - iter 21/71 - loss 0.01715704 - samples/sec: 2783.40 - lr: 0.003125\n",
            "2022-11-21 17:06:35,491 epoch 28 - iter 28/71 - loss 0.01733958 - samples/sec: 2900.56 - lr: 0.003125\n",
            "2022-11-21 17:06:35,575 epoch 28 - iter 35/71 - loss 0.01736441 - samples/sec: 2756.71 - lr: 0.003125\n",
            "2022-11-21 17:06:35,658 epoch 28 - iter 42/71 - loss 0.01732728 - samples/sec: 2820.78 - lr: 0.003125\n",
            "2022-11-21 17:06:35,738 epoch 28 - iter 49/71 - loss 0.01737365 - samples/sec: 2869.56 - lr: 0.003125\n",
            "2022-11-21 17:06:35,817 epoch 28 - iter 56/71 - loss 0.01724641 - samples/sec: 2939.03 - lr: 0.003125\n",
            "2022-11-21 17:06:35,897 epoch 28 - iter 63/71 - loss 0.01724009 - samples/sec: 2879.13 - lr: 0.003125\n",
            "2022-11-21 17:06:35,975 epoch 28 - iter 70/71 - loss 0.01725319 - samples/sec: 2943.58 - lr: 0.003125\n",
            "2022-11-21 17:06:35,980 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:35,981 EPOCH 28 done: loss 0.0176 - lr 0.0031250\n",
            "2022-11-21 17:06:36,108 DEV : loss 0.024595487862825394 - f1-score (micro avg)  0.4874\n",
            "2022-11-21 17:06:36,115 BAD EPOCHS (no improvement): 3\n",
            "2022-11-21 17:06:36,117 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:36,199 epoch 29 - iter 7/71 - loss 0.01704114 - samples/sec: 2853.43 - lr: 0.003125\n",
            "2022-11-21 17:06:36,278 epoch 29 - iter 14/71 - loss 0.01716763 - samples/sec: 2910.46 - lr: 0.003125\n",
            "2022-11-21 17:06:36,358 epoch 29 - iter 21/71 - loss 0.01705740 - samples/sec: 2906.75 - lr: 0.003125\n",
            "2022-11-21 17:06:36,437 epoch 29 - iter 28/71 - loss 0.01712735 - samples/sec: 2927.64 - lr: 0.003125\n",
            "2022-11-21 17:06:36,522 epoch 29 - iter 35/71 - loss 0.01717708 - samples/sec: 2723.24 - lr: 0.003125\n",
            "2022-11-21 17:06:36,603 epoch 29 - iter 42/71 - loss 0.01723442 - samples/sec: 2876.58 - lr: 0.003125\n",
            "2022-11-21 17:06:36,683 epoch 29 - iter 49/71 - loss 0.01732441 - samples/sec: 2860.53 - lr: 0.003125\n",
            "2022-11-21 17:06:36,764 epoch 29 - iter 56/71 - loss 0.01732876 - samples/sec: 2881.04 - lr: 0.003125\n",
            "2022-11-21 17:06:36,842 epoch 29 - iter 63/71 - loss 0.01731352 - samples/sec: 2942.21 - lr: 0.003125\n",
            "2022-11-21 17:06:36,922 epoch 29 - iter 70/71 - loss 0.01725385 - samples/sec: 2889.91 - lr: 0.003125\n",
            "2022-11-21 17:06:36,927 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:36,928 EPOCH 29 done: loss 0.0174 - lr 0.0031250\n",
            "2022-11-21 17:06:37,057 DEV : loss 0.02459537796676159 - f1-score (micro avg)  0.4874\n",
            "Epoch    29: reducing learning rate of group 0 to 1.5625e-03.\n",
            "2022-11-21 17:06:37,062 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:06:37,066 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:37,147 epoch 30 - iter 7/71 - loss 0.01670295 - samples/sec: 2907.37 - lr: 0.001563\n",
            "2022-11-21 17:06:37,225 epoch 30 - iter 14/71 - loss 0.01715760 - samples/sec: 2944.19 - lr: 0.001563\n",
            "2022-11-21 17:06:37,309 epoch 30 - iter 21/71 - loss 0.01717938 - samples/sec: 2776.54 - lr: 0.001563\n",
            "2022-11-21 17:06:37,388 epoch 30 - iter 28/71 - loss 0.01723652 - samples/sec: 2913.48 - lr: 0.001563\n",
            "2022-11-21 17:06:37,469 epoch 30 - iter 35/71 - loss 0.01706925 - samples/sec: 2863.58 - lr: 0.001563\n",
            "2022-11-21 17:06:37,553 epoch 30 - iter 42/71 - loss 0.01717917 - samples/sec: 2775.10 - lr: 0.001563\n",
            "2022-11-21 17:06:37,634 epoch 30 - iter 49/71 - loss 0.01723091 - samples/sec: 2889.29 - lr: 0.001563\n",
            "2022-11-21 17:06:37,718 epoch 30 - iter 56/71 - loss 0.01726602 - samples/sec: 2746.71 - lr: 0.001563\n",
            "2022-11-21 17:06:37,796 epoch 30 - iter 63/71 - loss 0.01728972 - samples/sec: 2949.52 - lr: 0.001563\n",
            "2022-11-21 17:06:37,877 epoch 30 - iter 70/71 - loss 0.01723955 - samples/sec: 2865.29 - lr: 0.001563\n",
            "2022-11-21 17:06:37,882 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:37,884 EPOCH 30 done: loss 0.0174 - lr 0.0015625\n",
            "2022-11-21 17:06:38,011 DEV : loss 0.024585746228694916 - f1-score (micro avg)  0.491\n",
            "2022-11-21 17:06:38,017 BAD EPOCHS (no improvement): 1\n",
            "2022-11-21 17:06:38,019 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:38,100 epoch 31 - iter 7/71 - loss 0.01695653 - samples/sec: 2874.23 - lr: 0.001563\n",
            "2022-11-21 17:06:38,181 epoch 31 - iter 14/71 - loss 0.01686637 - samples/sec: 2834.22 - lr: 0.001563\n",
            "2022-11-21 17:06:38,261 epoch 31 - iter 21/71 - loss 0.01664424 - samples/sec: 2902.52 - lr: 0.001563\n",
            "2022-11-21 17:06:38,341 epoch 31 - iter 28/71 - loss 0.01693717 - samples/sec: 2891.13 - lr: 0.001563\n",
            "2022-11-21 17:06:38,424 epoch 31 - iter 35/71 - loss 0.01707058 - samples/sec: 2790.67 - lr: 0.001563\n",
            "2022-11-21 17:06:38,508 epoch 31 - iter 42/71 - loss 0.01723821 - samples/sec: 2780.67 - lr: 0.001563\n",
            "2022-11-21 17:06:38,593 epoch 31 - iter 49/71 - loss 0.01716982 - samples/sec: 2728.01 - lr: 0.001563\n",
            "2022-11-21 17:06:38,674 epoch 31 - iter 56/71 - loss 0.01723335 - samples/sec: 2856.50 - lr: 0.001563\n",
            "2022-11-21 17:06:38,753 epoch 31 - iter 63/71 - loss 0.01721878 - samples/sec: 2955.34 - lr: 0.001563\n",
            "2022-11-21 17:06:38,831 epoch 31 - iter 70/71 - loss 0.01722854 - samples/sec: 2969.08 - lr: 0.001563\n",
            "2022-11-21 17:06:38,836 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:38,837 EPOCH 31 done: loss 0.0177 - lr 0.0015625\n",
            "2022-11-21 17:06:38,963 DEV : loss 0.024622928351163864 - f1-score (micro avg)  0.4982\n",
            "2022-11-21 17:06:38,969 BAD EPOCHS (no improvement): 2\n",
            "2022-11-21 17:06:38,971 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:39,053 epoch 32 - iter 7/71 - loss 0.01820286 - samples/sec: 2793.23 - lr: 0.001563\n",
            "2022-11-21 17:06:39,134 epoch 32 - iter 14/71 - loss 0.01776052 - samples/sec: 2868.04 - lr: 0.001563\n",
            "2022-11-21 17:06:39,216 epoch 32 - iter 21/71 - loss 0.01746052 - samples/sec: 2832.14 - lr: 0.001563\n",
            "2022-11-21 17:06:39,298 epoch 32 - iter 28/71 - loss 0.01751741 - samples/sec: 2835.38 - lr: 0.001563\n",
            "2022-11-21 17:06:39,378 epoch 32 - iter 35/71 - loss 0.01748285 - samples/sec: 2865.30 - lr: 0.001563\n",
            "2022-11-21 17:06:39,459 epoch 32 - iter 42/71 - loss 0.01737772 - samples/sec: 2866.03 - lr: 0.001563\n",
            "2022-11-21 17:06:39,541 epoch 32 - iter 49/71 - loss 0.01725488 - samples/sec: 2800.65 - lr: 0.001563\n",
            "2022-11-21 17:06:39,623 epoch 32 - iter 56/71 - loss 0.01722682 - samples/sec: 2814.01 - lr: 0.001563\n",
            "2022-11-21 17:06:39,707 epoch 32 - iter 63/71 - loss 0.01715452 - samples/sec: 2781.03 - lr: 0.001563\n",
            "2022-11-21 17:06:39,789 epoch 32 - iter 70/71 - loss 0.01722847 - samples/sec: 2817.60 - lr: 0.001563\n",
            "2022-11-21 17:06:39,794 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:39,796 EPOCH 32 done: loss 0.0176 - lr 0.0015625\n",
            "2022-11-21 17:06:39,926 DEV : loss 0.024615267291665077 - f1-score (micro avg)  0.491\n",
            "2022-11-21 17:06:39,933 BAD EPOCHS (no improvement): 3\n",
            "2022-11-21 17:06:39,935 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:40,018 epoch 33 - iter 7/71 - loss 0.01752791 - samples/sec: 2858.63 - lr: 0.001563\n",
            "2022-11-21 17:06:40,100 epoch 33 - iter 14/71 - loss 0.01716214 - samples/sec: 2813.29 - lr: 0.001563\n",
            "2022-11-21 17:06:40,184 epoch 33 - iter 21/71 - loss 0.01735830 - samples/sec: 2751.11 - lr: 0.001563\n",
            "2022-11-21 17:06:40,264 epoch 33 - iter 28/71 - loss 0.01736381 - samples/sec: 2900.41 - lr: 0.001563\n",
            "2022-11-21 17:06:40,348 epoch 33 - iter 35/71 - loss 0.01713709 - samples/sec: 2764.18 - lr: 0.001563\n",
            "2022-11-21 17:06:40,430 epoch 33 - iter 42/71 - loss 0.01729271 - samples/sec: 2797.90 - lr: 0.001563\n",
            "2022-11-21 17:06:40,517 epoch 33 - iter 49/71 - loss 0.01733346 - samples/sec: 2633.96 - lr: 0.001563\n",
            "2022-11-21 17:06:40,608 epoch 33 - iter 56/71 - loss 0.01730160 - samples/sec: 2563.84 - lr: 0.001563\n",
            "2022-11-21 17:06:40,696 epoch 33 - iter 63/71 - loss 0.01728961 - samples/sec: 2648.23 - lr: 0.001563\n",
            "2022-11-21 17:06:40,787 epoch 33 - iter 70/71 - loss 0.01723013 - samples/sec: 2604.13 - lr: 0.001563\n",
            "2022-11-21 17:06:40,792 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:40,793 EPOCH 33 done: loss 0.0174 - lr 0.0015625\n",
            "2022-11-21 17:06:40,926 DEV : loss 0.024609971791505814 - f1-score (micro avg)  0.4946\n",
            "Epoch    33: reducing learning rate of group 0 to 7.8125e-04.\n",
            "2022-11-21 17:06:40,933 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:06:40,935 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:41,030 epoch 34 - iter 7/71 - loss 0.01770444 - samples/sec: 2515.86 - lr: 0.000781\n",
            "2022-11-21 17:06:41,114 epoch 34 - iter 14/71 - loss 0.01745204 - samples/sec: 2773.08 - lr: 0.000781\n",
            "2022-11-21 17:06:41,201 epoch 34 - iter 21/71 - loss 0.01736337 - samples/sec: 2693.02 - lr: 0.000781\n",
            "2022-11-21 17:06:41,282 epoch 34 - iter 28/71 - loss 0.01743620 - samples/sec: 2829.69 - lr: 0.000781\n",
            "2022-11-21 17:06:41,373 epoch 34 - iter 35/71 - loss 0.01747965 - samples/sec: 2567.42 - lr: 0.000781\n",
            "2022-11-21 17:06:41,461 epoch 34 - iter 42/71 - loss 0.01742226 - samples/sec: 2612.18 - lr: 0.000781\n",
            "2022-11-21 17:06:41,555 epoch 34 - iter 49/71 - loss 0.01742908 - samples/sec: 2476.38 - lr: 0.000781\n",
            "2022-11-21 17:06:41,642 epoch 34 - iter 56/71 - loss 0.01734846 - samples/sec: 2644.48 - lr: 0.000781\n",
            "2022-11-21 17:06:41,735 epoch 34 - iter 63/71 - loss 0.01723535 - samples/sec: 2503.81 - lr: 0.000781\n",
            "2022-11-21 17:06:41,828 epoch 34 - iter 70/71 - loss 0.01722336 - samples/sec: 2485.13 - lr: 0.000781\n",
            "2022-11-21 17:06:41,835 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:41,836 EPOCH 34 done: loss 0.0175 - lr 0.0007813\n",
            "2022-11-21 17:06:41,968 DEV : loss 0.02460530586540699 - f1-score (micro avg)  0.4946\n",
            "2022-11-21 17:06:41,976 BAD EPOCHS (no improvement): 1\n",
            "2022-11-21 17:06:41,979 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:42,063 epoch 35 - iter 7/71 - loss 0.01668688 - samples/sec: 2775.19 - lr: 0.000781\n",
            "2022-11-21 17:06:42,151 epoch 35 - iter 14/71 - loss 0.01696128 - samples/sec: 2633.40 - lr: 0.000781\n",
            "2022-11-21 17:06:42,246 epoch 35 - iter 21/71 - loss 0.01714292 - samples/sec: 2462.92 - lr: 0.000781\n",
            "2022-11-21 17:06:42,345 epoch 35 - iter 28/71 - loss 0.01752150 - samples/sec: 2352.28 - lr: 0.000781\n",
            "2022-11-21 17:06:42,431 epoch 35 - iter 35/71 - loss 0.01733578 - samples/sec: 2706.96 - lr: 0.000781\n",
            "2022-11-21 17:06:42,523 epoch 35 - iter 42/71 - loss 0.01724638 - samples/sec: 2505.30 - lr: 0.000781\n",
            "2022-11-21 17:06:42,609 epoch 35 - iter 49/71 - loss 0.01725305 - samples/sec: 2701.96 - lr: 0.000781\n",
            "2022-11-21 17:06:42,700 epoch 35 - iter 56/71 - loss 0.01727032 - samples/sec: 2564.78 - lr: 0.000781\n",
            "2022-11-21 17:06:42,787 epoch 35 - iter 63/71 - loss 0.01723842 - samples/sec: 2675.31 - lr: 0.000781\n",
            "2022-11-21 17:06:42,871 epoch 35 - iter 70/71 - loss 0.01722540 - samples/sec: 2759.43 - lr: 0.000781\n",
            "2022-11-21 17:06:42,877 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:42,879 EPOCH 35 done: loss 0.0174 - lr 0.0007813\n",
            "2022-11-21 17:06:43,022 DEV : loss 0.024606632068753242 - f1-score (micro avg)  0.4946\n",
            "2022-11-21 17:06:43,031 BAD EPOCHS (no improvement): 2\n",
            "2022-11-21 17:06:43,034 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:43,131 epoch 36 - iter 7/71 - loss 0.01642292 - samples/sec: 2440.05 - lr: 0.000781\n",
            "2022-11-21 17:06:43,242 epoch 36 - iter 14/71 - loss 0.01667163 - samples/sec: 2069.05 - lr: 0.000781\n",
            "2022-11-21 17:06:43,371 epoch 36 - iter 21/71 - loss 0.01685681 - samples/sec: 1774.47 - lr: 0.000781\n",
            "2022-11-21 17:06:43,479 epoch 36 - iter 28/71 - loss 0.01699351 - samples/sec: 2147.28 - lr: 0.000781\n",
            "2022-11-21 17:06:43,566 epoch 36 - iter 35/71 - loss 0.01701611 - samples/sec: 2661.87 - lr: 0.000781\n",
            "2022-11-21 17:06:43,652 epoch 36 - iter 42/71 - loss 0.01711530 - samples/sec: 2714.46 - lr: 0.000781\n",
            "2022-11-21 17:06:43,735 epoch 36 - iter 49/71 - loss 0.01720373 - samples/sec: 2814.18 - lr: 0.000781\n",
            "2022-11-21 17:06:43,822 epoch 36 - iter 56/71 - loss 0.01734597 - samples/sec: 2631.49 - lr: 0.000781\n",
            "2022-11-21 17:06:43,904 epoch 36 - iter 63/71 - loss 0.01724359 - samples/sec: 2828.00 - lr: 0.000781\n",
            "2022-11-21 17:06:43,987 epoch 36 - iter 70/71 - loss 0.01721590 - samples/sec: 2798.58 - lr: 0.000781\n",
            "2022-11-21 17:06:43,992 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:43,995 EPOCH 36 done: loss 0.0176 - lr 0.0007813\n",
            "2022-11-21 17:06:44,122 DEV : loss 0.024605439975857735 - f1-score (micro avg)  0.4946\n",
            "2022-11-21 17:06:44,128 BAD EPOCHS (no improvement): 3\n",
            "2022-11-21 17:06:44,131 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:44,220 epoch 37 - iter 7/71 - loss 0.01735677 - samples/sec: 2616.16 - lr: 0.000781\n",
            "2022-11-21 17:06:44,303 epoch 37 - iter 14/71 - loss 0.01740129 - samples/sec: 2812.68 - lr: 0.000781\n",
            "2022-11-21 17:06:44,386 epoch 37 - iter 21/71 - loss 0.01727255 - samples/sec: 2782.00 - lr: 0.000781\n",
            "2022-11-21 17:06:44,466 epoch 37 - iter 28/71 - loss 0.01718415 - samples/sec: 2919.18 - lr: 0.000781\n",
            "2022-11-21 17:06:44,550 epoch 37 - iter 35/71 - loss 0.01718767 - samples/sec: 2762.66 - lr: 0.000781\n",
            "2022-11-21 17:06:44,632 epoch 37 - iter 42/71 - loss 0.01733560 - samples/sec: 2832.16 - lr: 0.000781\n",
            "2022-11-21 17:06:44,724 epoch 37 - iter 49/71 - loss 0.01731392 - samples/sec: 2487.76 - lr: 0.000781\n",
            "2022-11-21 17:06:44,806 epoch 37 - iter 56/71 - loss 0.01723552 - samples/sec: 2811.64 - lr: 0.000781\n",
            "2022-11-21 17:06:44,889 epoch 37 - iter 63/71 - loss 0.01729610 - samples/sec: 2777.06 - lr: 0.000781\n",
            "2022-11-21 17:06:44,972 epoch 37 - iter 70/71 - loss 0.01721963 - samples/sec: 2789.52 - lr: 0.000781\n",
            "2022-11-21 17:06:44,977 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:44,979 EPOCH 37 done: loss 0.0175 - lr 0.0007813\n",
            "2022-11-21 17:06:45,113 DEV : loss 0.024608714506030083 - f1-score (micro avg)  0.4946\n",
            "Epoch    37: reducing learning rate of group 0 to 3.9063e-04.\n",
            "2022-11-21 17:06:45,120 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:06:45,122 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:45,205 epoch 38 - iter 7/71 - loss 0.01728428 - samples/sec: 2817.76 - lr: 0.000391\n",
            "2022-11-21 17:06:45,287 epoch 38 - iter 14/71 - loss 0.01732380 - samples/sec: 2810.40 - lr: 0.000391\n",
            "2022-11-21 17:06:45,368 epoch 38 - iter 21/71 - loss 0.01728342 - samples/sec: 2876.70 - lr: 0.000391\n",
            "2022-11-21 17:06:45,449 epoch 38 - iter 28/71 - loss 0.01720437 - samples/sec: 2849.45 - lr: 0.000391\n",
            "2022-11-21 17:06:45,533 epoch 38 - iter 35/71 - loss 0.01717516 - samples/sec: 2748.41 - lr: 0.000391\n",
            "2022-11-21 17:06:45,615 epoch 38 - iter 42/71 - loss 0.01731398 - samples/sec: 2845.68 - lr: 0.000391\n",
            "2022-11-21 17:06:45,700 epoch 38 - iter 49/71 - loss 0.01730319 - samples/sec: 2720.83 - lr: 0.000391\n",
            "2022-11-21 17:06:45,782 epoch 38 - iter 56/71 - loss 0.01735358 - samples/sec: 2828.65 - lr: 0.000391\n",
            "2022-11-21 17:06:45,862 epoch 38 - iter 63/71 - loss 0.01724606 - samples/sec: 2870.35 - lr: 0.000391\n",
            "2022-11-21 17:06:45,941 epoch 38 - iter 70/71 - loss 0.01721724 - samples/sec: 2912.08 - lr: 0.000391\n",
            "2022-11-21 17:06:45,946 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:45,947 EPOCH 38 done: loss 0.0174 - lr 0.0003906\n",
            "2022-11-21 17:06:46,076 DEV : loss 0.024607177823781967 - f1-score (micro avg)  0.4946\n",
            "2022-11-21 17:06:46,087 BAD EPOCHS (no improvement): 1\n",
            "2022-11-21 17:06:46,090 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:46,171 epoch 39 - iter 7/71 - loss 0.01753147 - samples/sec: 2867.22 - lr: 0.000391\n",
            "2022-11-21 17:06:46,253 epoch 39 - iter 14/71 - loss 0.01771962 - samples/sec: 2830.05 - lr: 0.000391\n",
            "2022-11-21 17:06:46,335 epoch 39 - iter 21/71 - loss 0.01763281 - samples/sec: 2792.05 - lr: 0.000391\n",
            "2022-11-21 17:06:46,418 epoch 39 - iter 28/71 - loss 0.01746736 - samples/sec: 2795.21 - lr: 0.000391\n",
            "2022-11-21 17:06:46,500 epoch 39 - iter 35/71 - loss 0.01736165 - samples/sec: 2813.28 - lr: 0.000391\n",
            "2022-11-21 17:06:46,583 epoch 39 - iter 42/71 - loss 0.01723673 - samples/sec: 2854.20 - lr: 0.000391\n",
            "2022-11-21 17:06:46,664 epoch 39 - iter 49/71 - loss 0.01732249 - samples/sec: 2850.47 - lr: 0.000391\n",
            "2022-11-21 17:06:46,748 epoch 39 - iter 56/71 - loss 0.01725496 - samples/sec: 2762.49 - lr: 0.000391\n",
            "2022-11-21 17:06:46,827 epoch 39 - iter 63/71 - loss 0.01730446 - samples/sec: 2935.54 - lr: 0.000391\n",
            "2022-11-21 17:06:46,908 epoch 39 - iter 70/71 - loss 0.01722170 - samples/sec: 2831.26 - lr: 0.000391\n",
            "2022-11-21 17:06:46,913 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:46,916 EPOCH 39 done: loss 0.0173 - lr 0.0003906\n",
            "2022-11-21 17:06:47,042 DEV : loss 0.024608850479125977 - f1-score (micro avg)  0.4946\n",
            "2022-11-21 17:06:47,048 BAD EPOCHS (no improvement): 2\n",
            "2022-11-21 17:06:47,050 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:47,134 epoch 40 - iter 7/71 - loss 0.01651281 - samples/sec: 2812.19 - lr: 0.000391\n",
            "2022-11-21 17:06:47,218 epoch 40 - iter 14/71 - loss 0.01688832 - samples/sec: 2741.02 - lr: 0.000391\n",
            "2022-11-21 17:06:47,303 epoch 40 - iter 21/71 - loss 0.01701397 - samples/sec: 2742.10 - lr: 0.000391\n",
            "2022-11-21 17:06:47,383 epoch 40 - iter 28/71 - loss 0.01681775 - samples/sec: 2875.57 - lr: 0.000391\n",
            "2022-11-21 17:06:47,464 epoch 40 - iter 35/71 - loss 0.01691777 - samples/sec: 2857.56 - lr: 0.000391\n",
            "2022-11-21 17:06:47,548 epoch 40 - iter 42/71 - loss 0.01701066 - samples/sec: 2771.87 - lr: 0.000391\n",
            "2022-11-21 17:06:47,633 epoch 40 - iter 49/71 - loss 0.01710136 - samples/sec: 2751.93 - lr: 0.000391\n",
            "2022-11-21 17:06:47,719 epoch 40 - iter 56/71 - loss 0.01716300 - samples/sec: 2680.63 - lr: 0.000391\n",
            "2022-11-21 17:06:47,804 epoch 40 - iter 63/71 - loss 0.01717354 - samples/sec: 2743.42 - lr: 0.000391\n",
            "2022-11-21 17:06:47,905 epoch 40 - iter 70/71 - loss 0.01721545 - samples/sec: 2286.63 - lr: 0.000391\n",
            "2022-11-21 17:06:47,909 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:47,913 EPOCH 40 done: loss 0.0174 - lr 0.0003906\n",
            "2022-11-21 17:06:48,048 DEV : loss 0.024610592052340508 - f1-score (micro avg)  0.4946\n",
            "2022-11-21 17:06:48,057 BAD EPOCHS (no improvement): 3\n",
            "2022-11-21 17:06:48,060 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:48,148 epoch 41 - iter 7/71 - loss 0.01778678 - samples/sec: 2637.76 - lr: 0.000391\n",
            "2022-11-21 17:06:48,229 epoch 41 - iter 14/71 - loss 0.01717059 - samples/sec: 2866.11 - lr: 0.000391\n",
            "2022-11-21 17:06:48,319 epoch 41 - iter 21/71 - loss 0.01725433 - samples/sec: 2589.75 - lr: 0.000391\n",
            "2022-11-21 17:06:48,405 epoch 41 - iter 28/71 - loss 0.01712934 - samples/sec: 2711.48 - lr: 0.000391\n",
            "2022-11-21 17:06:48,496 epoch 41 - iter 35/71 - loss 0.01717637 - samples/sec: 2539.05 - lr: 0.000391\n",
            "2022-11-21 17:06:48,583 epoch 41 - iter 42/71 - loss 0.01725726 - samples/sec: 2691.99 - lr: 0.000391\n",
            "2022-11-21 17:06:48,670 epoch 41 - iter 49/71 - loss 0.01735844 - samples/sec: 2679.93 - lr: 0.000391\n",
            "2022-11-21 17:06:48,760 epoch 41 - iter 56/71 - loss 0.01739257 - samples/sec: 2576.64 - lr: 0.000391\n",
            "2022-11-21 17:06:48,842 epoch 41 - iter 63/71 - loss 0.01723659 - samples/sec: 2826.24 - lr: 0.000391\n",
            "2022-11-21 17:06:48,925 epoch 41 - iter 70/71 - loss 0.01721154 - samples/sec: 2766.48 - lr: 0.000391\n",
            "2022-11-21 17:06:48,931 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:48,932 EPOCH 41 done: loss 0.0175 - lr 0.0003906\n",
            "2022-11-21 17:06:49,072 DEV : loss 0.02461145631968975 - f1-score (micro avg)  0.4946\n",
            "Epoch    41: reducing learning rate of group 0 to 1.9531e-04.\n",
            "2022-11-21 17:06:49,081 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:06:49,084 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:49,171 epoch 42 - iter 7/71 - loss 0.01735129 - samples/sec: 2701.42 - lr: 0.000195\n",
            "2022-11-21 17:06:49,266 epoch 42 - iter 14/71 - loss 0.01739720 - samples/sec: 2428.73 - lr: 0.000195\n",
            "2022-11-21 17:06:49,356 epoch 42 - iter 21/71 - loss 0.01742236 - samples/sec: 2570.94 - lr: 0.000195\n",
            "2022-11-21 17:06:49,443 epoch 42 - iter 28/71 - loss 0.01718507 - samples/sec: 2680.88 - lr: 0.000195\n",
            "2022-11-21 17:06:49,531 epoch 42 - iter 35/71 - loss 0.01716641 - samples/sec: 2631.87 - lr: 0.000195\n",
            "2022-11-21 17:06:49,621 epoch 42 - iter 42/71 - loss 0.01711293 - samples/sec: 2579.15 - lr: 0.000195\n",
            "2022-11-21 17:06:49,708 epoch 42 - iter 49/71 - loss 0.01713609 - samples/sec: 2651.20 - lr: 0.000195\n",
            "2022-11-21 17:06:49,798 epoch 42 - iter 56/71 - loss 0.01706908 - samples/sec: 2586.87 - lr: 0.000195\n",
            "2022-11-21 17:06:49,883 epoch 42 - iter 63/71 - loss 0.01717160 - samples/sec: 2722.89 - lr: 0.000195\n",
            "2022-11-21 17:06:49,974 epoch 42 - iter 70/71 - loss 0.01721116 - samples/sec: 2557.11 - lr: 0.000195\n",
            "2022-11-21 17:06:49,979 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:49,981 EPOCH 42 done: loss 0.0175 - lr 0.0001953\n",
            "2022-11-21 17:06:50,118 DEV : loss 0.024613836780190468 - f1-score (micro avg)  0.4946\n",
            "2022-11-21 17:06:50,125 BAD EPOCHS (no improvement): 1\n",
            "2022-11-21 17:06:50,127 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:50,224 epoch 43 - iter 7/71 - loss 0.01708449 - samples/sec: 2388.43 - lr: 0.000195\n",
            "2022-11-21 17:06:50,318 epoch 43 - iter 14/71 - loss 0.01693541 - samples/sec: 2524.78 - lr: 0.000195\n",
            "2022-11-21 17:06:50,401 epoch 43 - iter 21/71 - loss 0.01694858 - samples/sec: 2822.56 - lr: 0.000195\n",
            "2022-11-21 17:06:50,481 epoch 43 - iter 28/71 - loss 0.01707495 - samples/sec: 2879.83 - lr: 0.000195\n",
            "2022-11-21 17:06:50,565 epoch 43 - iter 35/71 - loss 0.01717089 - samples/sec: 2780.87 - lr: 0.000195\n",
            "2022-11-21 17:06:50,647 epoch 43 - iter 42/71 - loss 0.01721820 - samples/sec: 2799.88 - lr: 0.000195\n",
            "2022-11-21 17:06:50,730 epoch 43 - iter 49/71 - loss 0.01722326 - samples/sec: 2813.29 - lr: 0.000195\n",
            "2022-11-21 17:06:50,816 epoch 43 - iter 56/71 - loss 0.01731169 - samples/sec: 2653.54 - lr: 0.000195\n",
            "2022-11-21 17:06:50,897 epoch 43 - iter 63/71 - loss 0.01725752 - samples/sec: 2860.22 - lr: 0.000195\n",
            "2022-11-21 17:06:50,979 epoch 43 - iter 70/71 - loss 0.01720816 - samples/sec: 2846.16 - lr: 0.000195\n",
            "2022-11-21 17:06:50,983 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:50,985 EPOCH 43 done: loss 0.0176 - lr 0.0001953\n",
            "2022-11-21 17:06:51,110 DEV : loss 0.02461644448339939 - f1-score (micro avg)  0.4946\n",
            "2022-11-21 17:06:51,115 BAD EPOCHS (no improvement): 2\n",
            "2022-11-21 17:06:51,118 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:51,201 epoch 44 - iter 7/71 - loss 0.01713360 - samples/sec: 2773.86 - lr: 0.000195\n",
            "2022-11-21 17:06:51,284 epoch 44 - iter 14/71 - loss 0.01691143 - samples/sec: 2807.93 - lr: 0.000195\n",
            "2022-11-21 17:06:51,365 epoch 44 - iter 21/71 - loss 0.01702908 - samples/sec: 2864.11 - lr: 0.000195\n",
            "2022-11-21 17:06:51,445 epoch 44 - iter 28/71 - loss 0.01700214 - samples/sec: 2896.65 - lr: 0.000195\n",
            "2022-11-21 17:06:51,530 epoch 44 - iter 35/71 - loss 0.01707413 - samples/sec: 2720.29 - lr: 0.000195\n",
            "2022-11-21 17:06:51,610 epoch 44 - iter 42/71 - loss 0.01695227 - samples/sec: 2897.50 - lr: 0.000195\n",
            "2022-11-21 17:06:51,692 epoch 44 - iter 49/71 - loss 0.01698994 - samples/sec: 2804.37 - lr: 0.000195\n",
            "2022-11-21 17:06:51,776 epoch 44 - iter 56/71 - loss 0.01720079 - samples/sec: 2758.75 - lr: 0.000195\n",
            "2022-11-21 17:06:51,860 epoch 44 - iter 63/71 - loss 0.01724838 - samples/sec: 2770.35 - lr: 0.000195\n",
            "2022-11-21 17:06:51,940 epoch 44 - iter 70/71 - loss 0.01720984 - samples/sec: 2883.67 - lr: 0.000195\n",
            "2022-11-21 17:06:51,945 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:51,946 EPOCH 44 done: loss 0.0175 - lr 0.0001953\n",
            "2022-11-21 17:06:52,076 DEV : loss 0.02461615391075611 - f1-score (micro avg)  0.4946\n",
            "2022-11-21 17:06:52,083 BAD EPOCHS (no improvement): 3\n",
            "2022-11-21 17:06:52,085 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:52,166 epoch 45 - iter 7/71 - loss 0.01718362 - samples/sec: 2869.48 - lr: 0.000195\n",
            "2022-11-21 17:06:52,254 epoch 45 - iter 14/71 - loss 0.01691395 - samples/sec: 2643.77 - lr: 0.000195\n",
            "2022-11-21 17:06:52,342 epoch 45 - iter 21/71 - loss 0.01702043 - samples/sec: 2637.48 - lr: 0.000195\n",
            "2022-11-21 17:06:52,427 epoch 45 - iter 28/71 - loss 0.01711101 - samples/sec: 2722.60 - lr: 0.000195\n",
            "2022-11-21 17:06:52,509 epoch 45 - iter 35/71 - loss 0.01728496 - samples/sec: 2825.78 - lr: 0.000195\n",
            "2022-11-21 17:06:52,587 epoch 45 - iter 42/71 - loss 0.01725218 - samples/sec: 2954.52 - lr: 0.000195\n",
            "2022-11-21 17:06:52,667 epoch 45 - iter 49/71 - loss 0.01725131 - samples/sec: 2901.57 - lr: 0.000195\n",
            "2022-11-21 17:06:52,750 epoch 45 - iter 56/71 - loss 0.01725318 - samples/sec: 2788.91 - lr: 0.000195\n",
            "2022-11-21 17:06:52,834 epoch 45 - iter 63/71 - loss 0.01723098 - samples/sec: 2778.84 - lr: 0.000195\n",
            "2022-11-21 17:06:52,913 epoch 45 - iter 70/71 - loss 0.01720839 - samples/sec: 2913.24 - lr: 0.000195\n",
            "2022-11-21 17:06:52,918 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:52,919 EPOCH 45 done: loss 0.0175 - lr 0.0001953\n",
            "2022-11-21 17:06:53,045 DEV : loss 0.024619150906801224 - f1-score (micro avg)  0.491\n",
            "Epoch    45: reducing learning rate of group 0 to 9.7656e-05.\n",
            "2022-11-21 17:06:53,051 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:06:53,053 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:53,055 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:53,058 learning rate too small - quitting training!\n",
            "2022-11-21 17:06:53,059 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:59,272 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:06:59,275 loading file resources/word-pair-test-flair/best-model.pt\n",
            "2022-11-21 17:07:01,287 0.5301\t0.5301\t0.5301\t0.5301\n",
            "2022-11-21 17:07:01,289 \n",
            "Results:\n",
            "- F-score (micro) 0.5301\n",
            "- F-score (macro) 0.3752\n",
            "- Accuracy 0.5301\n",
            "\n",
            "By class:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "    entailment     0.5311    0.9697    0.6863       132\n",
            "not_entailment     0.5000    0.0342    0.0640       117\n",
            "\n",
            "     micro avg     0.5301    0.5301    0.5301       249\n",
            "     macro avg     0.5156    0.5019    0.3752       249\n",
            "  weighted avg     0.5165    0.5301    0.3939       249\n",
            "   samples avg     0.5301    0.5301    0.5301       249\n",
            "\n",
            "2022-11-21 17:07:01,292 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_score': 0.5301204819277109,\n",
              " 'dev_score_history': [0.48375451263537905,\n",
              "  0.46570397111913364,\n",
              "  0.45126353790613716,\n",
              "  0.5415162454873647,\n",
              "  0.5415162454873647,\n",
              "  0.51985559566787,\n",
              "  0.46570397111913364,\n",
              "  0.5234657039711191,\n",
              "  0.5415162454873647,\n",
              "  0.4368231046931408,\n",
              "  0.5018050541516246,\n",
              "  0.51985559566787,\n",
              "  0.4981949458483754,\n",
              "  0.48375451263537905,\n",
              "  0.49097472924187724,\n",
              "  0.4981949458483754,\n",
              "  0.48736462093862815,\n",
              "  0.49097472924187724,\n",
              "  0.5018050541516246,\n",
              "  0.5018050541516246,\n",
              "  0.5090252707581228,\n",
              "  0.5018050541516246,\n",
              "  0.4981949458483754,\n",
              "  0.5018050541516246,\n",
              "  0.5018050541516246,\n",
              "  0.5054151624548736,\n",
              "  0.5018050541516246,\n",
              "  0.48736462093862815,\n",
              "  0.48736462093862815,\n",
              "  0.49097472924187724,\n",
              "  0.4981949458483754,\n",
              "  0.49097472924187724,\n",
              "  0.49458483754512633,\n",
              "  0.49458483754512633,\n",
              "  0.49458483754512633,\n",
              "  0.49458483754512633,\n",
              "  0.49458483754512633,\n",
              "  0.49458483754512633,\n",
              "  0.49458483754512633,\n",
              "  0.49458483754512633,\n",
              "  0.49458483754512633,\n",
              "  0.49458483754512633,\n",
              "  0.49458483754512633,\n",
              "  0.49458483754512633,\n",
              "  0.49097472924187724],\n",
              " 'train_loss_history': [0.023556217797256803,\n",
              "  0.02162907050157008,\n",
              "  0.021058224982738707,\n",
              "  0.020414753824086765,\n",
              "  0.020007316513499847,\n",
              "  0.019744841608816042,\n",
              "  0.019440120934695337,\n",
              "  0.019394772545347678,\n",
              "  0.019191954003971955,\n",
              "  0.01873252136214723,\n",
              "  0.018556583252191436,\n",
              "  0.018574399495433346,\n",
              "  0.018082496658173696,\n",
              "  0.01789452826428871,\n",
              "  0.01787622255015086,\n",
              "  0.017849025929522058,\n",
              "  0.017905062474293774,\n",
              "  0.017568147618661017,\n",
              "  0.017909735108741102,\n",
              "  0.017558295657091084,\n",
              "  0.017678436771241323,\n",
              "  0.01770864004856877,\n",
              "  0.01754624195420173,\n",
              "  0.017604007791696196,\n",
              "  0.01735393304023717,\n",
              "  0.01769923025452096,\n",
              "  0.017412485115127017,\n",
              "  0.017579848122139266,\n",
              "  0.017364329682136956,\n",
              "  0.017424101092250472,\n",
              "  0.01770427708698138,\n",
              "  0.017563180059580227,\n",
              "  0.01742801846855722,\n",
              "  0.017460845547135204,\n",
              "  0.017352650695908873,\n",
              "  0.017615149485006337,\n",
              "  0.01745023088016876,\n",
              "  0.017446231490956095,\n",
              "  0.017282224882399062,\n",
              "  0.017445016404657905,\n",
              "  0.017542933484157884,\n",
              "  0.017520638328213503,\n",
              "  0.017588896764170532,\n",
              "  0.017510677729797704,\n",
              "  0.017545801143144083],\n",
              " 'dev_loss_history': [tensor(0.0250, device='cuda:0'),\n",
              "  tensor(0.0250, device='cuda:0'),\n",
              "  tensor(0.0258, device='cuda:0'),\n",
              "  tensor(0.0257, device='cuda:0'),\n",
              "  tensor(0.0239, device='cuda:0'),\n",
              "  tensor(0.0242, device='cuda:0'),\n",
              "  tensor(0.0288, device='cuda:0'),\n",
              "  tensor(0.0271, device='cuda:0'),\n",
              "  tensor(0.0261, device='cuda:0'),\n",
              "  tensor(0.0260, device='cuda:0'),\n",
              "  tensor(0.0245, device='cuda:0'),\n",
              "  tensor(0.0247, device='cuda:0'),\n",
              "  tensor(0.0244, device='cuda:0'),\n",
              "  tensor(0.0243, device='cuda:0'),\n",
              "  tensor(0.0245, device='cuda:0'),\n",
              "  tensor(0.0245, device='cuda:0'),\n",
              "  tensor(0.0249, device='cuda:0'),\n",
              "  tensor(0.0246, device='cuda:0'),\n",
              "  tensor(0.0245, device='cuda:0'),\n",
              "  tensor(0.0245, device='cuda:0'),\n",
              "  tensor(0.0245, device='cuda:0'),\n",
              "  tensor(0.0246, device='cuda:0'),\n",
              "  tensor(0.0246, device='cuda:0'),\n",
              "  tensor(0.0247, device='cuda:0'),\n",
              "  tensor(0.0246, device='cuda:0'),\n",
              "  tensor(0.0247, device='cuda:0'),\n",
              "  tensor(0.0246, device='cuda:0'),\n",
              "  tensor(0.0246, device='cuda:0'),\n",
              "  tensor(0.0246, device='cuda:0'),\n",
              "  tensor(0.0246, device='cuda:0'),\n",
              "  tensor(0.0246, device='cuda:0'),\n",
              "  tensor(0.0246, device='cuda:0'),\n",
              "  tensor(0.0246, device='cuda:0'),\n",
              "  tensor(0.0246, device='cuda:0'),\n",
              "  tensor(0.0246, device='cuda:0'),\n",
              "  tensor(0.0246, device='cuda:0'),\n",
              "  tensor(0.0246, device='cuda:0'),\n",
              "  tensor(0.0246, device='cuda:0'),\n",
              "  tensor(0.0246, device='cuda:0'),\n",
              "  tensor(0.0246, device='cuda:0'),\n",
              "  tensor(0.0246, device='cuda:0'),\n",
              "  tensor(0.0246, device='cuda:0'),\n",
              "  tensor(0.0246, device='cuda:0'),\n",
              "  tensor(0.0246, device='cuda:0'),\n",
              "  tensor(0.0246, device='cuda:0')]}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Step 5: Add TransformerWordEmbeddings to the mix\n",
        "\n",
        " We will set fine-tuning to false for a transformer (static embeddings). We will be combining multiple stattic embeddings this time."
      ],
      "metadata": {
        "id": "HcMzlS397D-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flair.embeddings.token import TransformerWordEmbeddings\n",
        "from flair.embeddings import WordEmbeddings, FlairEmbeddings, StackedEmbeddings\n",
        "from flair.embeddings import DocumentPoolEmbeddings\n",
        "from flair.models import TextPairClassifier\n",
        "from flair.trainers import ModelTrainer\n",
        "\n",
        "embeddings_stack = StackedEmbeddings([\n",
        "    WordEmbeddings('en'),\n",
        "    FlairEmbeddings('en-forward'),\n",
        "    TransformerWordEmbeddings('xlm-roberta-base',\n",
        "                              fine_tune=False, \n",
        "                              subtoken_pooling='first'), # subtoken_pooling is something to experiment here with. You can also try mean.\n",
        "])\n",
        "\n",
        "embedding_stack = DocumentPoolEmbeddings([embedding_stack])\n",
        "\n",
        "# Step 3: Use text pair classification model\n",
        "classifier = TextPairClassifier(document_embeddings=embedding_stack,\n",
        "                                label_type=label_type,\n",
        "                                label_dictionary=label_dictionary,\n",
        "                                )\n",
        "\n",
        "# Step 4: Initialize trainer and train the model\n",
        "trainer = ModelTrainer(classifier, corpus)\n",
        "\n",
        "# if you are using transformer embeddings, you can simply call trainer.fine_tune()\n",
        "trainer.train(base_path='resources/word-pair-test-flair',\n",
        "              use_final_model_for_eval=False,\n",
        "              learning_rate=0.1,\n",
        "              max_epochs=100,\n",
        "              )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMFBrmEz68uQ",
        "outputId": "736b1538-c4d7-45fd-91f4-dbefe19bb652"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-21 17:07:32,683 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:07:32,686 Model: \"TextPairClassifier(\n",
            "  (loss_function): CrossEntropyLoss()\n",
            "  (document_embeddings): DocumentPoolEmbeddings(\n",
            "    fine_tune_mode=none, pooling=mean\n",
            "    (embeddings): StackedEmbeddings(\n",
            "      (list_embedding_0): DocumentPoolEmbeddings(\n",
            "        fine_tune_mode=none, pooling=mean\n",
            "        (embeddings): StackedEmbeddings(\n",
            "          (list_embedding_0): StackedEmbeddings(\n",
            "            (list_embedding_0): WordEmbeddings(\n",
            "              'en'\n",
            "              (embedding): Embedding(1000001, 300)\n",
            "            )\n",
            "            (list_embedding_1): FlairEmbeddings(\n",
            "              (lm): LanguageModel(\n",
            "                (drop): Dropout(p=0.05, inplace=False)\n",
            "                (encoder): Embedding(300, 100)\n",
            "                (rnn): LSTM(100, 2048)\n",
            "                (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): Linear(in_features=2348, out_features=3, bias=True)\n",
            "  (weights): None\n",
            "  (weight_tensor) None\n",
            ")\"\n",
            "2022-11-21 17:07:32,688 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:07:32,689 Corpus: \"Corpus: 2241 train + 277 dev + 249 test sentences\"\n",
            "2022-11-21 17:07:32,691 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:07:32,694 Parameters:\n",
            "2022-11-21 17:07:32,696  - learning_rate: \"0.1\"\n",
            "2022-11-21 17:07:32,698  - mini_batch_size: \"32\"\n",
            "2022-11-21 17:07:32,699  - patience: \"3\"\n",
            "2022-11-21 17:07:32,701  - anneal_factor: \"0.5\"\n",
            "2022-11-21 17:07:32,703  - max_epochs: \"100\"\n",
            "2022-11-21 17:07:32,705  - shuffle: \"True\"\n",
            "2022-11-21 17:07:32,707  - train_with_dev: \"False\"\n",
            "2022-11-21 17:07:32,709  - batch_growth_annealing: \"False\"\n",
            "2022-11-21 17:07:32,712 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:07:32,713 Model training base path: \"resources/word-pair-test-flair\"\n",
            "2022-11-21 17:07:32,777 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:07:32,779 Device: cuda:0\n",
            "2022-11-21 17:07:32,780 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:07:32,782 Embeddings storage mode: cpu\n",
            "2022-11-21 17:07:32,952 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/flair/trainers/trainer.py:65: UserWarning: There should be no best model saved at epoch 1 except there is a model from previous trainings in your training folder. All previous best models will be deleted.\n",
            "  \"There should be no best model saved at epoch 1 except there is a model from previous trainings\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-21 17:07:33,105 epoch 1 - iter 7/71 - loss 0.02971298 - samples/sec: 1487.88 - lr: 0.100000\n",
            "2022-11-21 17:07:33,256 epoch 1 - iter 14/71 - loss 0.02766108 - samples/sec: 1511.02 - lr: 0.100000\n",
            "2022-11-21 17:07:33,375 epoch 1 - iter 21/71 - loss 0.02654351 - samples/sec: 1917.05 - lr: 0.100000\n",
            "2022-11-21 17:07:33,491 epoch 1 - iter 28/71 - loss 0.02584137 - samples/sec: 1979.05 - lr: 0.100000\n",
            "2022-11-21 17:07:33,611 epoch 1 - iter 35/71 - loss 0.02531000 - samples/sec: 1901.32 - lr: 0.100000\n",
            "2022-11-21 17:07:33,727 epoch 1 - iter 42/71 - loss 0.02493115 - samples/sec: 1980.79 - lr: 0.100000\n",
            "2022-11-21 17:07:33,848 epoch 1 - iter 49/71 - loss 0.02464105 - samples/sec: 1884.11 - lr: 0.100000\n",
            "2022-11-21 17:07:33,962 epoch 1 - iter 56/71 - loss 0.02440316 - samples/sec: 2026.27 - lr: 0.100000\n",
            "2022-11-21 17:07:34,072 epoch 1 - iter 63/71 - loss 0.02418056 - samples/sec: 2086.95 - lr: 0.100000\n",
            "2022-11-21 17:07:34,185 epoch 1 - iter 70/71 - loss 0.02401489 - samples/sec: 2035.61 - lr: 0.100000\n",
            "2022-11-21 17:07:34,194 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:07:34,195 EPOCH 1 done: loss 0.0243 - lr 0.1000000\n",
            "2022-11-21 17:07:34,432 DEV : loss 0.02406119741499424 - f1-score (micro avg)  0.4657\n",
            "2022-11-21 17:07:34,440 BAD EPOCHS (no improvement): 0\n",
            "2022-11-21 17:07:34,442 saving best model\n",
            "2022-11-21 17:07:40,417 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:07:40,568 epoch 2 - iter 7/71 - loss 0.02244362 - samples/sec: 1517.95 - lr: 0.100000\n",
            "2022-11-21 17:07:40,706 epoch 2 - iter 14/71 - loss 0.02219699 - samples/sec: 1654.65 - lr: 0.100000\n",
            "2022-11-21 17:07:40,831 epoch 2 - iter 21/71 - loss 0.02214787 - samples/sec: 1834.04 - lr: 0.100000\n",
            "2022-11-21 17:07:40,944 epoch 2 - iter 28/71 - loss 0.02212198 - samples/sec: 2035.02 - lr: 0.100000\n",
            "2022-11-21 17:07:41,071 epoch 2 - iter 35/71 - loss 0.02212735 - samples/sec: 1785.44 - lr: 0.100000\n",
            "2022-11-21 17:07:41,195 epoch 2 - iter 42/71 - loss 0.02206075 - samples/sec: 1854.53 - lr: 0.100000\n",
            "2022-11-21 17:07:41,305 epoch 2 - iter 49/71 - loss 0.02207083 - samples/sec: 2081.55 - lr: 0.100000\n",
            "2022-11-21 17:07:41,414 epoch 2 - iter 56/71 - loss 0.02201097 - samples/sec: 2108.97 - lr: 0.100000\n",
            "2022-11-21 17:07:41,533 epoch 2 - iter 63/71 - loss 0.02197823 - samples/sec: 1913.59 - lr: 0.100000\n",
            "2022-11-21 17:07:41,651 epoch 2 - iter 70/71 - loss 0.02197560 - samples/sec: 1947.79 - lr: 0.100000\n",
            "2022-11-21 17:07:41,660 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:07:41,661 EPOCH 2 done: loss 0.0222 - lr 0.1000000\n",
            "2022-11-21 17:07:41,819 DEV : loss 0.025131085887551308 - f1-score (micro avg)  0.4729\n",
            "2022-11-21 17:07:41,823 BAD EPOCHS (no improvement): 0\n",
            "2022-11-21 17:07:41,826 saving best model\n",
            "2022-11-21 17:07:48,144 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:07:48,315 epoch 3 - iter 7/71 - loss 0.02246979 - samples/sec: 1344.54 - lr: 0.100000\n",
            "2022-11-21 17:07:48,435 epoch 3 - iter 14/71 - loss 0.02196961 - samples/sec: 1909.77 - lr: 0.100000\n",
            "2022-11-21 17:07:48,553 epoch 3 - iter 21/71 - loss 0.02186964 - samples/sec: 1928.20 - lr: 0.100000\n",
            "2022-11-21 17:07:48,668 epoch 3 - iter 28/71 - loss 0.02182327 - samples/sec: 1992.46 - lr: 0.100000\n",
            "2022-11-21 17:07:48,785 epoch 3 - iter 35/71 - loss 0.02174872 - samples/sec: 1965.96 - lr: 0.100000\n",
            "2022-11-21 17:07:48,896 epoch 3 - iter 42/71 - loss 0.02174559 - samples/sec: 2056.88 - lr: 0.100000\n",
            "2022-11-21 17:07:49,013 epoch 3 - iter 49/71 - loss 0.02169374 - samples/sec: 1959.79 - lr: 0.100000\n",
            "2022-11-21 17:07:49,127 epoch 3 - iter 56/71 - loss 0.02167293 - samples/sec: 2012.03 - lr: 0.100000\n",
            "2022-11-21 17:07:49,239 epoch 3 - iter 63/71 - loss 0.02167526 - samples/sec: 2031.90 - lr: 0.100000\n",
            "2022-11-21 17:07:49,355 epoch 3 - iter 70/71 - loss 0.02166273 - samples/sec: 1970.24 - lr: 0.100000\n",
            "2022-11-21 17:07:49,362 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:07:49,363 EPOCH 3 done: loss 0.0219 - lr 0.1000000\n",
            "2022-11-21 17:07:49,524 DEV : loss 0.022885877639055252 - f1-score (micro avg)  0.5235\n",
            "2022-11-21 17:07:49,531 BAD EPOCHS (no improvement): 0\n",
            "2022-11-21 17:07:49,534 saving best model\n",
            "2022-11-21 17:07:55,970 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:07:56,137 epoch 4 - iter 7/71 - loss 0.02140017 - samples/sec: 1382.46 - lr: 0.100000\n",
            "2022-11-21 17:07:56,273 epoch 4 - iter 14/71 - loss 0.02143768 - samples/sec: 1684.99 - lr: 0.100000\n",
            "2022-11-21 17:07:56,395 epoch 4 - iter 21/71 - loss 0.02142801 - samples/sec: 1882.32 - lr: 0.100000\n",
            "2022-11-21 17:07:56,516 epoch 4 - iter 28/71 - loss 0.02141719 - samples/sec: 1903.83 - lr: 0.100000\n",
            "2022-11-21 17:07:56,626 epoch 4 - iter 35/71 - loss 0.02133600 - samples/sec: 2084.05 - lr: 0.100000\n",
            "2022-11-21 17:07:56,749 epoch 4 - iter 42/71 - loss 0.02134402 - samples/sec: 1873.63 - lr: 0.100000\n",
            "2022-11-21 17:07:56,863 epoch 4 - iter 49/71 - loss 0.02135044 - samples/sec: 2033.99 - lr: 0.100000\n",
            "2022-11-21 17:07:56,978 epoch 4 - iter 56/71 - loss 0.02129184 - samples/sec: 1993.96 - lr: 0.100000\n",
            "2022-11-21 17:07:57,097 epoch 4 - iter 63/71 - loss 0.02129722 - samples/sec: 1939.63 - lr: 0.100000\n",
            "2022-11-21 17:07:57,214 epoch 4 - iter 70/71 - loss 0.02128619 - samples/sec: 1958.01 - lr: 0.100000\n",
            "2022-11-21 17:07:57,222 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:07:57,224 EPOCH 4 done: loss 0.0217 - lr 0.1000000\n",
            "2022-11-21 17:07:57,384 DEV : loss 0.023952951654791832 - f1-score (micro avg)  0.4621\n",
            "2022-11-21 17:07:57,389 BAD EPOCHS (no improvement): 1\n",
            "2022-11-21 17:07:57,392 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:07:57,506 epoch 5 - iter 7/71 - loss 0.02042934 - samples/sec: 2025.27 - lr: 0.100000\n",
            "2022-11-21 17:07:57,614 epoch 5 - iter 14/71 - loss 0.02075353 - samples/sec: 2120.60 - lr: 0.100000\n",
            "2022-11-21 17:07:57,741 epoch 5 - iter 21/71 - loss 0.02085504 - samples/sec: 1807.80 - lr: 0.100000\n",
            "2022-11-21 17:07:57,854 epoch 5 - iter 28/71 - loss 0.02086045 - samples/sec: 2024.70 - lr: 0.100000\n",
            "2022-11-21 17:07:57,968 epoch 5 - iter 35/71 - loss 0.02089618 - samples/sec: 2012.32 - lr: 0.100000\n",
            "2022-11-21 17:07:58,073 epoch 5 - iter 42/71 - loss 0.02091366 - samples/sec: 2197.47 - lr: 0.100000\n",
            "2022-11-21 17:07:58,189 epoch 5 - iter 49/71 - loss 0.02098989 - samples/sec: 1979.34 - lr: 0.100000\n",
            "2022-11-21 17:07:58,307 epoch 5 - iter 56/71 - loss 0.02104912 - samples/sec: 1940.56 - lr: 0.100000\n",
            "2022-11-21 17:07:58,421 epoch 5 - iter 63/71 - loss 0.02107390 - samples/sec: 2015.31 - lr: 0.100000\n",
            "2022-11-21 17:07:58,548 epoch 5 - iter 70/71 - loss 0.02105216 - samples/sec: 1806.34 - lr: 0.100000\n",
            "2022-11-21 17:07:58,557 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:07:58,559 EPOCH 5 done: loss 0.0213 - lr 0.1000000\n",
            "2022-11-21 17:07:58,723 DEV : loss 0.023486841470003128 - f1-score (micro avg)  0.5487\n",
            "2022-11-21 17:07:58,730 BAD EPOCHS (no improvement): 0\n",
            "2022-11-21 17:07:58,731 saving best model\n",
            "2022-11-21 17:08:05,078 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:05,232 epoch 6 - iter 7/71 - loss 0.02132922 - samples/sec: 1498.40 - lr: 0.100000\n",
            "2022-11-21 17:08:05,354 epoch 6 - iter 14/71 - loss 0.02106950 - samples/sec: 1865.22 - lr: 0.100000\n",
            "2022-11-21 17:08:05,474 epoch 6 - iter 21/71 - loss 0.02097299 - samples/sec: 1914.75 - lr: 0.100000\n",
            "2022-11-21 17:08:05,585 epoch 6 - iter 28/71 - loss 0.02110641 - samples/sec: 2075.10 - lr: 0.100000\n",
            "2022-11-21 17:08:05,700 epoch 6 - iter 35/71 - loss 0.02113977 - samples/sec: 2001.95 - lr: 0.100000\n",
            "2022-11-21 17:08:05,812 epoch 6 - iter 42/71 - loss 0.02104776 - samples/sec: 2046.06 - lr: 0.100000\n",
            "2022-11-21 17:08:05,931 epoch 6 - iter 49/71 - loss 0.02105436 - samples/sec: 1917.81 - lr: 0.100000\n",
            "2022-11-21 17:08:06,046 epoch 6 - iter 56/71 - loss 0.02102554 - samples/sec: 1994.29 - lr: 0.100000\n",
            "2022-11-21 17:08:06,167 epoch 6 - iter 63/71 - loss 0.02098277 - samples/sec: 1897.68 - lr: 0.100000\n",
            "2022-11-21 17:08:06,286 epoch 6 - iter 70/71 - loss 0.02094341 - samples/sec: 1927.67 - lr: 0.100000\n",
            "2022-11-21 17:08:06,297 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:06,299 EPOCH 6 done: loss 0.0212 - lr 0.1000000\n",
            "2022-11-21 17:08:06,458 DEV : loss 0.02276117354631424 - f1-score (micro avg)  0.5343\n",
            "2022-11-21 17:08:06,463 BAD EPOCHS (no improvement): 1\n",
            "2022-11-21 17:08:06,465 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:06,575 epoch 7 - iter 7/71 - loss 0.02070700 - samples/sec: 2084.31 - lr: 0.100000\n",
            "2022-11-21 17:08:06,694 epoch 7 - iter 14/71 - loss 0.02057699 - samples/sec: 1927.07 - lr: 0.100000\n",
            "2022-11-21 17:08:06,803 epoch 7 - iter 21/71 - loss 0.02047762 - samples/sec: 2093.73 - lr: 0.100000\n",
            "2022-11-21 17:08:06,916 epoch 7 - iter 28/71 - loss 0.02070168 - samples/sec: 2030.38 - lr: 0.100000\n",
            "2022-11-21 17:08:07,040 epoch 7 - iter 35/71 - loss 0.02071765 - samples/sec: 1849.82 - lr: 0.100000\n",
            "2022-11-21 17:08:07,156 epoch 7 - iter 42/71 - loss 0.02078427 - samples/sec: 1977.84 - lr: 0.100000\n",
            "2022-11-21 17:08:07,273 epoch 7 - iter 49/71 - loss 0.02085847 - samples/sec: 1959.26 - lr: 0.100000\n",
            "2022-11-21 17:08:07,389 epoch 7 - iter 56/71 - loss 0.02088965 - samples/sec: 1979.00 - lr: 0.100000\n",
            "2022-11-21 17:08:07,524 epoch 7 - iter 63/71 - loss 0.02080101 - samples/sec: 1690.62 - lr: 0.100000\n",
            "2022-11-21 17:08:07,639 epoch 7 - iter 70/71 - loss 0.02079560 - samples/sec: 1982.40 - lr: 0.100000\n",
            "2022-11-21 17:08:07,648 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:07,651 EPOCH 7 done: loss 0.0211 - lr 0.1000000\n",
            "2022-11-21 17:08:07,809 DEV : loss 0.023295080289244652 - f1-score (micro avg)  0.5451\n",
            "2022-11-21 17:08:07,816 BAD EPOCHS (no improvement): 2\n",
            "2022-11-21 17:08:07,818 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:07,931 epoch 8 - iter 7/71 - loss 0.02090389 - samples/sec: 2045.62 - lr: 0.100000\n",
            "2022-11-21 17:08:08,048 epoch 8 - iter 14/71 - loss 0.02083403 - samples/sec: 1959.84 - lr: 0.100000\n",
            "2022-11-21 17:08:08,158 epoch 8 - iter 21/71 - loss 0.02087524 - samples/sec: 2083.74 - lr: 0.100000\n",
            "2022-11-21 17:08:08,278 epoch 8 - iter 28/71 - loss 0.02079699 - samples/sec: 1910.29 - lr: 0.100000\n",
            "2022-11-21 17:08:08,400 epoch 8 - iter 35/71 - loss 0.02076764 - samples/sec: 1876.92 - lr: 0.100000\n",
            "2022-11-21 17:08:08,519 epoch 8 - iter 42/71 - loss 0.02065590 - samples/sec: 1920.56 - lr: 0.100000\n",
            "2022-11-21 17:08:08,632 epoch 8 - iter 49/71 - loss 0.02069245 - samples/sec: 2040.68 - lr: 0.100000\n",
            "2022-11-21 17:08:08,746 epoch 8 - iter 56/71 - loss 0.02068426 - samples/sec: 2008.16 - lr: 0.100000\n",
            "2022-11-21 17:08:08,861 epoch 8 - iter 63/71 - loss 0.02069975 - samples/sec: 1982.49 - lr: 0.100000\n",
            "2022-11-21 17:08:08,980 epoch 8 - iter 70/71 - loss 0.02069446 - samples/sec: 1938.30 - lr: 0.100000\n",
            "2022-11-21 17:08:08,990 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:08,992 EPOCH 8 done: loss 0.0209 - lr 0.1000000\n",
            "2022-11-21 17:08:09,152 DEV : loss 0.022842995822429657 - f1-score (micro avg)  0.5415\n",
            "2022-11-21 17:08:09,158 BAD EPOCHS (no improvement): 3\n",
            "2022-11-21 17:08:09,160 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:09,273 epoch 9 - iter 7/71 - loss 0.02080726 - samples/sec: 2055.88 - lr: 0.100000\n",
            "2022-11-21 17:08:09,386 epoch 9 - iter 14/71 - loss 0.02067757 - samples/sec: 2044.97 - lr: 0.100000\n",
            "2022-11-21 17:08:09,498 epoch 9 - iter 21/71 - loss 0.02057191 - samples/sec: 2053.76 - lr: 0.100000\n",
            "2022-11-21 17:08:09,622 epoch 9 - iter 28/71 - loss 0.02048418 - samples/sec: 1838.78 - lr: 0.100000\n",
            "2022-11-21 17:08:09,732 epoch 9 - iter 35/71 - loss 0.02043561 - samples/sec: 2089.67 - lr: 0.100000\n",
            "2022-11-21 17:08:09,849 epoch 9 - iter 42/71 - loss 0.02037075 - samples/sec: 1970.17 - lr: 0.100000\n",
            "2022-11-21 17:08:09,971 epoch 9 - iter 49/71 - loss 0.02037029 - samples/sec: 1873.89 - lr: 0.100000\n",
            "2022-11-21 17:08:10,090 epoch 9 - iter 56/71 - loss 0.02043895 - samples/sec: 1930.33 - lr: 0.100000\n",
            "2022-11-21 17:08:10,202 epoch 9 - iter 63/71 - loss 0.02043318 - samples/sec: 2048.49 - lr: 0.100000\n",
            "2022-11-21 17:08:10,323 epoch 9 - iter 70/71 - loss 0.02047726 - samples/sec: 1896.10 - lr: 0.100000\n",
            "2022-11-21 17:08:10,330 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:10,333 EPOCH 9 done: loss 0.0208 - lr 0.1000000\n",
            "2022-11-21 17:08:10,496 DEV : loss 0.024004966020584106 - f1-score (micro avg)  0.4693\n",
            "Epoch     9: reducing learning rate of group 0 to 5.0000e-02.\n",
            "2022-11-21 17:08:10,503 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:08:10,506 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:10,616 epoch 10 - iter 7/71 - loss 0.02055826 - samples/sec: 2100.56 - lr: 0.050000\n",
            "2022-11-21 17:08:10,737 epoch 10 - iter 14/71 - loss 0.02038336 - samples/sec: 1896.91 - lr: 0.050000\n",
            "2022-11-21 17:08:10,855 epoch 10 - iter 21/71 - loss 0.02023658 - samples/sec: 1930.90 - lr: 0.050000\n",
            "2022-11-21 17:08:10,963 epoch 10 - iter 28/71 - loss 0.02035161 - samples/sec: 2127.85 - lr: 0.050000\n",
            "2022-11-21 17:08:11,072 epoch 10 - iter 35/71 - loss 0.02033805 - samples/sec: 2108.89 - lr: 0.050000\n",
            "2022-11-21 17:08:11,188 epoch 10 - iter 42/71 - loss 0.02032505 - samples/sec: 1990.19 - lr: 0.050000\n",
            "2022-11-21 17:08:11,311 epoch 10 - iter 49/71 - loss 0.02036492 - samples/sec: 1851.65 - lr: 0.050000\n",
            "2022-11-21 17:08:11,421 epoch 10 - iter 56/71 - loss 0.02038266 - samples/sec: 2086.69 - lr: 0.050000\n",
            "2022-11-21 17:08:11,552 epoch 10 - iter 63/71 - loss 0.02037090 - samples/sec: 1757.36 - lr: 0.050000\n",
            "2022-11-21 17:08:11,677 epoch 10 - iter 70/71 - loss 0.02038494 - samples/sec: 1837.51 - lr: 0.050000\n",
            "2022-11-21 17:08:11,689 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:11,691 EPOCH 10 done: loss 0.0206 - lr 0.0500000\n",
            "2022-11-21 17:08:11,849 DEV : loss 0.022584810853004456 - f1-score (micro avg)  0.5451\n",
            "2022-11-21 17:08:11,856 BAD EPOCHS (no improvement): 1\n",
            "2022-11-21 17:08:11,858 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:11,970 epoch 11 - iter 7/71 - loss 0.02021866 - samples/sec: 2065.33 - lr: 0.050000\n",
            "2022-11-21 17:08:12,081 epoch 11 - iter 14/71 - loss 0.02013013 - samples/sec: 2071.54 - lr: 0.050000\n",
            "2022-11-21 17:08:12,198 epoch 11 - iter 21/71 - loss 0.02012506 - samples/sec: 1964.26 - lr: 0.050000\n",
            "2022-11-21 17:08:12,308 epoch 11 - iter 28/71 - loss 0.02014402 - samples/sec: 2083.83 - lr: 0.050000\n",
            "2022-11-21 17:08:12,427 epoch 11 - iter 35/71 - loss 0.02015554 - samples/sec: 1922.75 - lr: 0.050000\n",
            "2022-11-21 17:08:12,547 epoch 11 - iter 42/71 - loss 0.02019525 - samples/sec: 1913.09 - lr: 0.050000\n",
            "2022-11-21 17:08:12,667 epoch 11 - iter 49/71 - loss 0.02018513 - samples/sec: 1932.98 - lr: 0.050000\n",
            "2022-11-21 17:08:12,794 epoch 11 - iter 56/71 - loss 0.02021614 - samples/sec: 1799.01 - lr: 0.050000\n",
            "2022-11-21 17:08:12,915 epoch 11 - iter 63/71 - loss 0.02021990 - samples/sec: 1902.78 - lr: 0.050000\n",
            "2022-11-21 17:08:13,026 epoch 11 - iter 70/71 - loss 0.02024846 - samples/sec: 2073.77 - lr: 0.050000\n",
            "2022-11-21 17:08:13,035 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:13,037 EPOCH 11 done: loss 0.0205 - lr 0.0500000\n",
            "2022-11-21 17:08:13,204 DEV : loss 0.022656604647636414 - f1-score (micro avg)  0.5487\n",
            "2022-11-21 17:08:13,211 BAD EPOCHS (no improvement): 0\n",
            "2022-11-21 17:08:13,213 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:13,325 epoch 12 - iter 7/71 - loss 0.01971096 - samples/sec: 2086.81 - lr: 0.050000\n",
            "2022-11-21 17:08:13,439 epoch 12 - iter 14/71 - loss 0.01977816 - samples/sec: 2000.59 - lr: 0.050000\n",
            "2022-11-21 17:08:13,555 epoch 12 - iter 21/71 - loss 0.02012497 - samples/sec: 1968.45 - lr: 0.050000\n",
            "2022-11-21 17:08:13,675 epoch 12 - iter 28/71 - loss 0.02005367 - samples/sec: 1911.02 - lr: 0.050000\n",
            "2022-11-21 17:08:13,799 epoch 12 - iter 35/71 - loss 0.02008593 - samples/sec: 1856.05 - lr: 0.050000\n",
            "2022-11-21 17:08:13,908 epoch 12 - iter 42/71 - loss 0.02006903 - samples/sec: 2092.65 - lr: 0.050000\n",
            "2022-11-21 17:08:14,025 epoch 12 - iter 49/71 - loss 0.02010152 - samples/sec: 1960.44 - lr: 0.050000\n",
            "2022-11-21 17:08:14,151 epoch 12 - iter 56/71 - loss 0.02007876 - samples/sec: 1829.93 - lr: 0.050000\n",
            "2022-11-21 17:08:14,270 epoch 12 - iter 63/71 - loss 0.02006942 - samples/sec: 1915.84 - lr: 0.050000\n",
            "2022-11-21 17:08:14,381 epoch 12 - iter 70/71 - loss 0.02017175 - samples/sec: 2085.36 - lr: 0.050000\n",
            "2022-11-21 17:08:14,390 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:14,391 EPOCH 12 done: loss 0.0204 - lr 0.0500000\n",
            "2022-11-21 17:08:14,551 DEV : loss 0.022681143134832382 - f1-score (micro avg)  0.5379\n",
            "2022-11-21 17:08:14,557 BAD EPOCHS (no improvement): 1\n",
            "2022-11-21 17:08:14,560 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:14,683 epoch 13 - iter 7/71 - loss 0.02005758 - samples/sec: 1868.06 - lr: 0.050000\n",
            "2022-11-21 17:08:14,800 epoch 13 - iter 14/71 - loss 0.01988912 - samples/sec: 1958.64 - lr: 0.050000\n",
            "2022-11-21 17:08:14,915 epoch 13 - iter 21/71 - loss 0.01992504 - samples/sec: 2002.54 - lr: 0.050000\n",
            "2022-11-21 17:08:15,033 epoch 13 - iter 28/71 - loss 0.02006297 - samples/sec: 1934.32 - lr: 0.050000\n",
            "2022-11-21 17:08:15,143 epoch 13 - iter 35/71 - loss 0.02011489 - samples/sec: 2092.84 - lr: 0.050000\n",
            "2022-11-21 17:08:15,264 epoch 13 - iter 42/71 - loss 0.02005541 - samples/sec: 1907.01 - lr: 0.050000\n",
            "2022-11-21 17:08:15,398 epoch 13 - iter 49/71 - loss 0.02004643 - samples/sec: 1700.02 - lr: 0.050000\n",
            "2022-11-21 17:08:15,511 epoch 13 - iter 56/71 - loss 0.02004124 - samples/sec: 2041.37 - lr: 0.050000\n",
            "2022-11-21 17:08:15,628 epoch 13 - iter 63/71 - loss 0.02006718 - samples/sec: 1963.24 - lr: 0.050000\n",
            "2022-11-21 17:08:15,737 epoch 13 - iter 70/71 - loss 0.02009590 - samples/sec: 2109.92 - lr: 0.050000\n",
            "2022-11-21 17:08:15,744 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:15,745 EPOCH 13 done: loss 0.0204 - lr 0.0500000\n",
            "2022-11-21 17:08:15,908 DEV : loss 0.023268375545740128 - f1-score (micro avg)  0.4621\n",
            "2022-11-21 17:08:15,915 BAD EPOCHS (no improvement): 2\n",
            "2022-11-21 17:08:15,917 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:16,041 epoch 14 - iter 7/71 - loss 0.02016113 - samples/sec: 1859.45 - lr: 0.050000\n",
            "2022-11-21 17:08:16,158 epoch 14 - iter 14/71 - loss 0.02004371 - samples/sec: 1951.39 - lr: 0.050000\n",
            "2022-11-21 17:08:16,271 epoch 14 - iter 21/71 - loss 0.02011357 - samples/sec: 2024.28 - lr: 0.050000\n",
            "2022-11-21 17:08:16,387 epoch 14 - iter 28/71 - loss 0.02013369 - samples/sec: 1970.15 - lr: 0.050000\n",
            "2022-11-21 17:08:16,501 epoch 14 - iter 35/71 - loss 0.02016881 - samples/sec: 1995.06 - lr: 0.050000\n",
            "2022-11-21 17:08:16,616 epoch 14 - iter 42/71 - loss 0.02022409 - samples/sec: 2004.65 - lr: 0.050000\n",
            "2022-11-21 17:08:16,733 epoch 14 - iter 49/71 - loss 0.02015562 - samples/sec: 1963.79 - lr: 0.050000\n",
            "2022-11-21 17:08:16,846 epoch 14 - iter 56/71 - loss 0.02013189 - samples/sec: 2021.55 - lr: 0.050000\n",
            "2022-11-21 17:08:16,959 epoch 14 - iter 63/71 - loss 0.02017062 - samples/sec: 2004.22 - lr: 0.050000\n",
            "2022-11-21 17:08:17,080 epoch 14 - iter 70/71 - loss 0.02009429 - samples/sec: 1895.77 - lr: 0.050000\n",
            "2022-11-21 17:08:17,090 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:17,092 EPOCH 14 done: loss 0.0203 - lr 0.0500000\n",
            "2022-11-21 17:08:17,255 DEV : loss 0.022686423733830452 - f1-score (micro avg)  0.5596\n",
            "2022-11-21 17:08:17,261 BAD EPOCHS (no improvement): 0\n",
            "2022-11-21 17:08:17,263 saving best model\n",
            "2022-11-21 17:08:23,683 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:23,831 epoch 15 - iter 7/71 - loss 0.01974141 - samples/sec: 1557.30 - lr: 0.050000\n",
            "2022-11-21 17:08:23,950 epoch 15 - iter 14/71 - loss 0.02000908 - samples/sec: 1933.22 - lr: 0.050000\n",
            "2022-11-21 17:08:24,071 epoch 15 - iter 21/71 - loss 0.01986190 - samples/sec: 1889.28 - lr: 0.050000\n",
            "2022-11-21 17:08:24,191 epoch 15 - iter 28/71 - loss 0.01994911 - samples/sec: 1901.18 - lr: 0.050000\n",
            "2022-11-21 17:08:24,304 epoch 15 - iter 35/71 - loss 0.01993335 - samples/sec: 2045.55 - lr: 0.050000\n",
            "2022-11-21 17:08:24,423 epoch 15 - iter 42/71 - loss 0.01991497 - samples/sec: 1923.73 - lr: 0.050000\n",
            "2022-11-21 17:08:24,542 epoch 15 - iter 49/71 - loss 0.01990471 - samples/sec: 1917.93 - lr: 0.050000\n",
            "2022-11-21 17:08:24,652 epoch 15 - iter 56/71 - loss 0.01992507 - samples/sec: 2096.58 - lr: 0.050000\n",
            "2022-11-21 17:08:24,763 epoch 15 - iter 63/71 - loss 0.01998316 - samples/sec: 2072.60 - lr: 0.050000\n",
            "2022-11-21 17:08:24,877 epoch 15 - iter 70/71 - loss 0.02000306 - samples/sec: 2014.62 - lr: 0.050000\n",
            "2022-11-21 17:08:24,885 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:24,886 EPOCH 15 done: loss 0.0203 - lr 0.0500000\n",
            "2022-11-21 17:08:25,046 DEV : loss 0.02287181094288826 - f1-score (micro avg)  0.5235\n",
            "2022-11-21 17:08:25,052 BAD EPOCHS (no improvement): 1\n",
            "2022-11-21 17:08:25,054 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:25,161 epoch 16 - iter 7/71 - loss 0.01989121 - samples/sec: 2162.00 - lr: 0.050000\n",
            "2022-11-21 17:08:25,270 epoch 16 - iter 14/71 - loss 0.01995528 - samples/sec: 2089.85 - lr: 0.050000\n",
            "2022-11-21 17:08:25,379 epoch 16 - iter 21/71 - loss 0.01974973 - samples/sec: 2120.71 - lr: 0.050000\n",
            "2022-11-21 17:08:25,493 epoch 16 - iter 28/71 - loss 0.01985334 - samples/sec: 2009.37 - lr: 0.050000\n",
            "2022-11-21 17:08:25,616 epoch 16 - iter 35/71 - loss 0.01983790 - samples/sec: 1852.98 - lr: 0.050000\n",
            "2022-11-21 17:08:25,745 epoch 16 - iter 42/71 - loss 0.01987808 - samples/sec: 1776.63 - lr: 0.050000\n",
            "2022-11-21 17:08:25,853 epoch 16 - iter 49/71 - loss 0.01992441 - samples/sec: 2109.55 - lr: 0.050000\n",
            "2022-11-21 17:08:25,970 epoch 16 - iter 56/71 - loss 0.01995036 - samples/sec: 1961.66 - lr: 0.050000\n",
            "2022-11-21 17:08:26,090 epoch 16 - iter 63/71 - loss 0.01997057 - samples/sec: 1918.76 - lr: 0.050000\n",
            "2022-11-21 17:08:26,200 epoch 16 - iter 70/71 - loss 0.01995878 - samples/sec: 2068.04 - lr: 0.050000\n",
            "2022-11-21 17:08:26,209 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:26,210 EPOCH 16 done: loss 0.0202 - lr 0.0500000\n",
            "2022-11-21 17:08:26,369 DEV : loss 0.023143472149968147 - f1-score (micro avg)  0.4549\n",
            "2022-11-21 17:08:26,374 BAD EPOCHS (no improvement): 2\n",
            "2022-11-21 17:08:26,377 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:26,489 epoch 17 - iter 7/71 - loss 0.01944723 - samples/sec: 2051.98 - lr: 0.050000\n",
            "2022-11-21 17:08:26,602 epoch 17 - iter 14/71 - loss 0.01964327 - samples/sec: 2032.88 - lr: 0.050000\n",
            "2022-11-21 17:08:26,709 epoch 17 - iter 21/71 - loss 0.01980169 - samples/sec: 2143.46 - lr: 0.050000\n",
            "2022-11-21 17:08:26,832 epoch 17 - iter 28/71 - loss 0.01990678 - samples/sec: 1841.93 - lr: 0.050000\n",
            "2022-11-21 17:08:26,945 epoch 17 - iter 35/71 - loss 0.02004606 - samples/sec: 2026.30 - lr: 0.050000\n",
            "2022-11-21 17:08:27,060 epoch 17 - iter 42/71 - loss 0.01997496 - samples/sec: 1993.30 - lr: 0.050000\n",
            "2022-11-21 17:08:27,182 epoch 17 - iter 49/71 - loss 0.01989701 - samples/sec: 1879.07 - lr: 0.050000\n",
            "2022-11-21 17:08:27,296 epoch 17 - iter 56/71 - loss 0.01988681 - samples/sec: 2008.68 - lr: 0.050000\n",
            "2022-11-21 17:08:27,405 epoch 17 - iter 63/71 - loss 0.01996481 - samples/sec: 2111.45 - lr: 0.050000\n",
            "2022-11-21 17:08:27,529 epoch 17 - iter 70/71 - loss 0.01990723 - samples/sec: 1850.62 - lr: 0.050000\n",
            "2022-11-21 17:08:27,539 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:27,540 EPOCH 17 done: loss 0.0202 - lr 0.0500000\n",
            "2022-11-21 17:08:27,701 DEV : loss 0.023136641830205917 - f1-score (micro avg)  0.4946\n",
            "2022-11-21 17:08:27,707 BAD EPOCHS (no improvement): 3\n",
            "2022-11-21 17:08:27,709 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:27,820 epoch 18 - iter 7/71 - loss 0.01979946 - samples/sec: 2097.18 - lr: 0.050000\n",
            "2022-11-21 17:08:27,928 epoch 18 - iter 14/71 - loss 0.01964110 - samples/sec: 2104.51 - lr: 0.050000\n",
            "2022-11-21 17:08:28,040 epoch 18 - iter 21/71 - loss 0.01966001 - samples/sec: 2046.56 - lr: 0.050000\n",
            "2022-11-21 17:08:28,150 epoch 18 - iter 28/71 - loss 0.01980022 - samples/sec: 2085.10 - lr: 0.050000\n",
            "2022-11-21 17:08:28,256 epoch 18 - iter 35/71 - loss 0.01984944 - samples/sec: 2163.30 - lr: 0.050000\n",
            "2022-11-21 17:08:28,374 epoch 18 - iter 42/71 - loss 0.01977403 - samples/sec: 1944.56 - lr: 0.050000\n",
            "2022-11-21 17:08:28,502 epoch 18 - iter 49/71 - loss 0.01979053 - samples/sec: 1786.94 - lr: 0.050000\n",
            "2022-11-21 17:08:28,647 epoch 18 - iter 56/71 - loss 0.01978553 - samples/sec: 1578.99 - lr: 0.050000\n",
            "2022-11-21 17:08:28,767 epoch 18 - iter 63/71 - loss 0.01975862 - samples/sec: 1902.46 - lr: 0.050000\n",
            "2022-11-21 17:08:28,881 epoch 18 - iter 70/71 - loss 0.01985091 - samples/sec: 2011.01 - lr: 0.050000\n",
            "2022-11-21 17:08:28,891 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:28,892 EPOCH 18 done: loss 0.0201 - lr 0.0500000\n",
            "2022-11-21 17:08:29,053 DEV : loss 0.022988995537161827 - f1-score (micro avg)  0.4946\n",
            "Epoch    18: reducing learning rate of group 0 to 2.5000e-02.\n",
            "2022-11-21 17:08:29,060 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:08:29,063 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:29,178 epoch 19 - iter 7/71 - loss 0.01982750 - samples/sec: 2011.70 - lr: 0.025000\n",
            "2022-11-21 17:08:29,305 epoch 19 - iter 14/71 - loss 0.01986806 - samples/sec: 1791.75 - lr: 0.025000\n",
            "2022-11-21 17:08:29,421 epoch 19 - iter 21/71 - loss 0.02005454 - samples/sec: 1983.88 - lr: 0.025000\n",
            "2022-11-21 17:08:29,533 epoch 19 - iter 28/71 - loss 0.02000338 - samples/sec: 2034.38 - lr: 0.025000\n",
            "2022-11-21 17:08:29,648 epoch 19 - iter 35/71 - loss 0.01979300 - samples/sec: 1984.97 - lr: 0.025000\n",
            "2022-11-21 17:08:29,757 epoch 19 - iter 42/71 - loss 0.01974491 - samples/sec: 2098.07 - lr: 0.025000\n",
            "2022-11-21 17:08:29,875 epoch 19 - iter 49/71 - loss 0.01978610 - samples/sec: 1942.43 - lr: 0.025000\n",
            "2022-11-21 17:08:29,995 epoch 19 - iter 56/71 - loss 0.01980413 - samples/sec: 1894.99 - lr: 0.025000\n",
            "2022-11-21 17:08:30,101 epoch 19 - iter 63/71 - loss 0.01974379 - samples/sec: 2171.72 - lr: 0.025000\n",
            "2022-11-21 17:08:30,210 epoch 19 - iter 70/71 - loss 0.01978435 - samples/sec: 2104.84 - lr: 0.025000\n",
            "2022-11-21 17:08:30,220 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:30,221 EPOCH 19 done: loss 0.0200 - lr 0.0250000\n",
            "2022-11-21 17:08:30,381 DEV : loss 0.023081893101334572 - f1-score (micro avg)  0.491\n",
            "2022-11-21 17:08:30,387 BAD EPOCHS (no improvement): 1\n",
            "2022-11-21 17:08:30,389 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:30,504 epoch 20 - iter 7/71 - loss 0.01936891 - samples/sec: 1998.21 - lr: 0.025000\n",
            "2022-11-21 17:08:30,617 epoch 20 - iter 14/71 - loss 0.01957779 - samples/sec: 2044.16 - lr: 0.025000\n",
            "2022-11-21 17:08:30,730 epoch 20 - iter 21/71 - loss 0.01969009 - samples/sec: 2017.85 - lr: 0.025000\n",
            "2022-11-21 17:08:30,843 epoch 20 - iter 28/71 - loss 0.01966225 - samples/sec: 2029.12 - lr: 0.025000\n",
            "2022-11-21 17:08:30,956 epoch 20 - iter 35/71 - loss 0.01966334 - samples/sec: 2042.72 - lr: 0.025000\n",
            "2022-11-21 17:08:31,077 epoch 20 - iter 42/71 - loss 0.01971940 - samples/sec: 1880.42 - lr: 0.025000\n",
            "2022-11-21 17:08:31,195 epoch 20 - iter 49/71 - loss 0.01973895 - samples/sec: 1948.67 - lr: 0.025000\n",
            "2022-11-21 17:08:31,308 epoch 20 - iter 56/71 - loss 0.01975507 - samples/sec: 2019.40 - lr: 0.025000\n",
            "2022-11-21 17:08:31,420 epoch 20 - iter 63/71 - loss 0.01973578 - samples/sec: 2047.79 - lr: 0.025000\n",
            "2022-11-21 17:08:31,531 epoch 20 - iter 70/71 - loss 0.01975564 - samples/sec: 2046.47 - lr: 0.025000\n",
            "2022-11-21 17:08:31,540 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:31,542 EPOCH 20 done: loss 0.0200 - lr 0.0250000\n",
            "2022-11-21 17:08:31,703 DEV : loss 0.02281816117465496 - f1-score (micro avg)  0.5235\n",
            "2022-11-21 17:08:31,709 BAD EPOCHS (no improvement): 2\n",
            "2022-11-21 17:08:31,711 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:31,823 epoch 21 - iter 7/71 - loss 0.01980011 - samples/sec: 2060.35 - lr: 0.025000\n",
            "2022-11-21 17:08:31,937 epoch 21 - iter 14/71 - loss 0.01964858 - samples/sec: 2004.13 - lr: 0.025000\n",
            "2022-11-21 17:08:32,053 epoch 21 - iter 21/71 - loss 0.01973730 - samples/sec: 1968.85 - lr: 0.025000\n",
            "2022-11-21 17:08:32,158 epoch 21 - iter 28/71 - loss 0.01956817 - samples/sec: 2173.51 - lr: 0.025000\n",
            "2022-11-21 17:08:32,276 epoch 21 - iter 35/71 - loss 0.01947314 - samples/sec: 1942.15 - lr: 0.025000\n",
            "2022-11-21 17:08:32,394 epoch 21 - iter 42/71 - loss 0.01958965 - samples/sec: 1931.44 - lr: 0.025000\n",
            "2022-11-21 17:08:32,515 epoch 21 - iter 49/71 - loss 0.01951464 - samples/sec: 1892.41 - lr: 0.025000\n",
            "2022-11-21 17:08:32,632 epoch 21 - iter 56/71 - loss 0.01961644 - samples/sec: 1953.16 - lr: 0.025000\n",
            "2022-11-21 17:08:32,750 epoch 21 - iter 63/71 - loss 0.01971168 - samples/sec: 1929.70 - lr: 0.025000\n",
            "2022-11-21 17:08:32,863 epoch 21 - iter 70/71 - loss 0.01972879 - samples/sec: 2027.39 - lr: 0.025000\n",
            "2022-11-21 17:08:32,873 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:32,875 EPOCH 21 done: loss 0.0199 - lr 0.0250000\n",
            "2022-11-21 17:08:33,036 DEV : loss 0.02299617789685726 - f1-score (micro avg)  0.509\n",
            "2022-11-21 17:08:33,043 BAD EPOCHS (no improvement): 3\n",
            "2022-11-21 17:08:33,045 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:33,151 epoch 22 - iter 7/71 - loss 0.01958776 - samples/sec: 2184.52 - lr: 0.025000\n",
            "2022-11-21 17:08:33,262 epoch 22 - iter 14/71 - loss 0.01958846 - samples/sec: 2063.64 - lr: 0.025000\n",
            "2022-11-21 17:08:33,381 epoch 22 - iter 21/71 - loss 0.01961119 - samples/sec: 1942.01 - lr: 0.025000\n",
            "2022-11-21 17:08:33,514 epoch 22 - iter 28/71 - loss 0.01965750 - samples/sec: 1709.34 - lr: 0.025000\n",
            "2022-11-21 17:08:33,630 epoch 22 - iter 35/71 - loss 0.01974188 - samples/sec: 1984.27 - lr: 0.025000\n",
            "2022-11-21 17:08:33,748 epoch 22 - iter 42/71 - loss 0.01980323 - samples/sec: 1949.55 - lr: 0.025000\n",
            "2022-11-21 17:08:33,857 epoch 22 - iter 49/71 - loss 0.01981249 - samples/sec: 2092.54 - lr: 0.025000\n",
            "2022-11-21 17:08:33,984 epoch 22 - iter 56/71 - loss 0.01980944 - samples/sec: 1795.04 - lr: 0.025000\n",
            "2022-11-21 17:08:34,096 epoch 22 - iter 63/71 - loss 0.01973261 - samples/sec: 2050.49 - lr: 0.025000\n",
            "2022-11-21 17:08:34,209 epoch 22 - iter 70/71 - loss 0.01970879 - samples/sec: 2031.20 - lr: 0.025000\n",
            "2022-11-21 17:08:34,218 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:34,220 EPOCH 22 done: loss 0.0199 - lr 0.0250000\n",
            "2022-11-21 17:08:34,379 DEV : loss 0.02277824841439724 - f1-score (micro avg)  0.5343\n",
            "Epoch    22: reducing learning rate of group 0 to 1.2500e-02.\n",
            "2022-11-21 17:08:34,384 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:08:34,387 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:34,500 epoch 23 - iter 7/71 - loss 0.01946886 - samples/sec: 2054.28 - lr: 0.012500\n",
            "2022-11-21 17:08:34,611 epoch 23 - iter 14/71 - loss 0.01973293 - samples/sec: 2102.29 - lr: 0.012500\n",
            "2022-11-21 17:08:34,726 epoch 23 - iter 21/71 - loss 0.01953382 - samples/sec: 1995.59 - lr: 0.012500\n",
            "2022-11-21 17:08:34,841 epoch 23 - iter 28/71 - loss 0.01973611 - samples/sec: 1977.91 - lr: 0.012500\n",
            "2022-11-21 17:08:34,960 epoch 23 - iter 35/71 - loss 0.01990693 - samples/sec: 1923.81 - lr: 0.012500\n",
            "2022-11-21 17:08:35,072 epoch 23 - iter 42/71 - loss 0.01982077 - samples/sec: 2049.34 - lr: 0.012500\n",
            "2022-11-21 17:08:35,190 epoch 23 - iter 49/71 - loss 0.01969270 - samples/sec: 1949.75 - lr: 0.012500\n",
            "2022-11-21 17:08:35,307 epoch 23 - iter 56/71 - loss 0.01966688 - samples/sec: 1964.98 - lr: 0.012500\n",
            "2022-11-21 17:08:35,431 epoch 23 - iter 63/71 - loss 0.01966292 - samples/sec: 1853.76 - lr: 0.012500\n",
            "2022-11-21 17:08:35,547 epoch 23 - iter 70/71 - loss 0.01967348 - samples/sec: 1970.33 - lr: 0.012500\n",
            "2022-11-21 17:08:35,556 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:35,558 EPOCH 23 done: loss 0.0199 - lr 0.0125000\n",
            "2022-11-21 17:08:35,720 DEV : loss 0.022774744778871536 - f1-score (micro avg)  0.5415\n",
            "2022-11-21 17:08:35,725 BAD EPOCHS (no improvement): 1\n",
            "2022-11-21 17:08:35,727 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:35,837 epoch 24 - iter 7/71 - loss 0.01917014 - samples/sec: 2087.53 - lr: 0.012500\n",
            "2022-11-21 17:08:35,957 epoch 24 - iter 14/71 - loss 0.01952485 - samples/sec: 1914.43 - lr: 0.012500\n",
            "2022-11-21 17:08:36,068 epoch 24 - iter 21/71 - loss 0.01961858 - samples/sec: 2055.60 - lr: 0.012500\n",
            "2022-11-21 17:08:36,182 epoch 24 - iter 28/71 - loss 0.01953707 - samples/sec: 2022.10 - lr: 0.012500\n",
            "2022-11-21 17:08:36,293 epoch 24 - iter 35/71 - loss 0.01970732 - samples/sec: 2054.13 - lr: 0.012500\n",
            "2022-11-21 17:08:36,404 epoch 24 - iter 42/71 - loss 0.01971563 - samples/sec: 2068.64 - lr: 0.012500\n",
            "2022-11-21 17:08:36,541 epoch 24 - iter 49/71 - loss 0.01969454 - samples/sec: 1667.07 - lr: 0.012500\n",
            "2022-11-21 17:08:36,671 epoch 24 - iter 56/71 - loss 0.01965583 - samples/sec: 1760.92 - lr: 0.012500\n",
            "2022-11-21 17:08:36,782 epoch 24 - iter 63/71 - loss 0.01967563 - samples/sec: 2070.73 - lr: 0.012500\n",
            "2022-11-21 17:08:36,899 epoch 24 - iter 70/71 - loss 0.01965508 - samples/sec: 1953.04 - lr: 0.012500\n",
            "2022-11-21 17:08:36,908 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:36,910 EPOCH 24 done: loss 0.0199 - lr 0.0125000\n",
            "2022-11-21 17:08:37,067 DEV : loss 0.022815996780991554 - f1-score (micro avg)  0.5199\n",
            "2022-11-21 17:08:37,073 BAD EPOCHS (no improvement): 2\n",
            "2022-11-21 17:08:37,076 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:37,190 epoch 25 - iter 7/71 - loss 0.01971806 - samples/sec: 2007.66 - lr: 0.012500\n",
            "2022-11-21 17:08:37,313 epoch 25 - iter 14/71 - loss 0.01962475 - samples/sec: 1868.62 - lr: 0.012500\n",
            "2022-11-21 17:08:37,433 epoch 25 - iter 21/71 - loss 0.01972617 - samples/sec: 1909.52 - lr: 0.012500\n",
            "2022-11-21 17:08:37,547 epoch 25 - iter 28/71 - loss 0.01967368 - samples/sec: 2012.18 - lr: 0.012500\n",
            "2022-11-21 17:08:37,666 epoch 25 - iter 35/71 - loss 0.01976420 - samples/sec: 1922.95 - lr: 0.012500\n",
            "2022-11-21 17:08:37,773 epoch 25 - iter 42/71 - loss 0.01963281 - samples/sec: 2135.37 - lr: 0.012500\n",
            "2022-11-21 17:08:37,883 epoch 25 - iter 49/71 - loss 0.01968942 - samples/sec: 2092.21 - lr: 0.012500\n",
            "2022-11-21 17:08:38,002 epoch 25 - iter 56/71 - loss 0.01965249 - samples/sec: 1920.99 - lr: 0.012500\n",
            "2022-11-21 17:08:38,122 epoch 25 - iter 63/71 - loss 0.01956304 - samples/sec: 1900.41 - lr: 0.012500\n",
            "2022-11-21 17:08:38,239 epoch 25 - iter 70/71 - loss 0.01963591 - samples/sec: 1966.08 - lr: 0.012500\n",
            "2022-11-21 17:08:38,249 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:38,251 EPOCH 25 done: loss 0.0199 - lr 0.0125000\n",
            "2022-11-21 17:08:38,412 DEV : loss 0.022784551605582237 - f1-score (micro avg)  0.5271\n",
            "2022-11-21 17:08:38,418 BAD EPOCHS (no improvement): 3\n",
            "2022-11-21 17:08:38,420 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:38,528 epoch 26 - iter 7/71 - loss 0.01928861 - samples/sec: 2148.38 - lr: 0.012500\n",
            "2022-11-21 17:08:38,638 epoch 26 - iter 14/71 - loss 0.01924378 - samples/sec: 2077.83 - lr: 0.012500\n",
            "2022-11-21 17:08:38,753 epoch 26 - iter 21/71 - loss 0.01950426 - samples/sec: 1987.67 - lr: 0.012500\n",
            "2022-11-21 17:08:38,866 epoch 26 - iter 28/71 - loss 0.01964794 - samples/sec: 2036.79 - lr: 0.012500\n",
            "2022-11-21 17:08:38,984 epoch 26 - iter 35/71 - loss 0.01974382 - samples/sec: 1947.13 - lr: 0.012500\n",
            "2022-11-21 17:08:39,100 epoch 26 - iter 42/71 - loss 0.01975145 - samples/sec: 1969.80 - lr: 0.012500\n",
            "2022-11-21 17:08:39,210 epoch 26 - iter 49/71 - loss 0.01973055 - samples/sec: 2078.51 - lr: 0.012500\n",
            "2022-11-21 17:08:39,333 epoch 26 - iter 56/71 - loss 0.01970144 - samples/sec: 1872.43 - lr: 0.012500\n",
            "2022-11-21 17:08:39,447 epoch 26 - iter 63/71 - loss 0.01970502 - samples/sec: 2004.37 - lr: 0.012500\n",
            "2022-11-21 17:08:39,567 epoch 26 - iter 70/71 - loss 0.01962473 - samples/sec: 1916.32 - lr: 0.012500\n",
            "2022-11-21 17:08:39,573 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:39,575 EPOCH 26 done: loss 0.0199 - lr 0.0125000\n",
            "2022-11-21 17:08:39,737 DEV : loss 0.02278013899922371 - f1-score (micro avg)  0.5235\n",
            "Epoch    26: reducing learning rate of group 0 to 6.2500e-03.\n",
            "2022-11-21 17:08:39,743 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:08:39,746 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:39,857 epoch 27 - iter 7/71 - loss 0.01981559 - samples/sec: 2059.12 - lr: 0.006250\n",
            "2022-11-21 17:08:39,973 epoch 27 - iter 14/71 - loss 0.01960713 - samples/sec: 1966.13 - lr: 0.006250\n",
            "2022-11-21 17:08:40,085 epoch 27 - iter 21/71 - loss 0.01951924 - samples/sec: 2054.46 - lr: 0.006250\n",
            "2022-11-21 17:08:40,209 epoch 27 - iter 28/71 - loss 0.01954338 - samples/sec: 1829.88 - lr: 0.006250\n",
            "2022-11-21 17:08:40,318 epoch 27 - iter 35/71 - loss 0.01952373 - samples/sec: 2112.13 - lr: 0.006250\n",
            "2022-11-21 17:08:40,435 epoch 27 - iter 42/71 - loss 0.01954751 - samples/sec: 1963.65 - lr: 0.006250\n",
            "2022-11-21 17:08:40,551 epoch 27 - iter 49/71 - loss 0.01957760 - samples/sec: 1990.41 - lr: 0.006250\n",
            "2022-11-21 17:08:40,668 epoch 27 - iter 56/71 - loss 0.01961513 - samples/sec: 1953.05 - lr: 0.006250\n",
            "2022-11-21 17:08:40,785 epoch 27 - iter 63/71 - loss 0.01959696 - samples/sec: 1972.56 - lr: 0.006250\n",
            "2022-11-21 17:08:40,899 epoch 27 - iter 70/71 - loss 0.01960454 - samples/sec: 1999.00 - lr: 0.006250\n",
            "2022-11-21 17:08:40,909 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:40,910 EPOCH 27 done: loss 0.0199 - lr 0.0062500\n",
            "2022-11-21 17:08:41,067 DEV : loss 0.02285788021981716 - f1-score (micro avg)  0.4982\n",
            "2022-11-21 17:08:41,072 BAD EPOCHS (no improvement): 1\n",
            "2022-11-21 17:08:41,076 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:41,187 epoch 28 - iter 7/71 - loss 0.01924813 - samples/sec: 2062.65 - lr: 0.006250\n",
            "2022-11-21 17:08:41,296 epoch 28 - iter 14/71 - loss 0.01969906 - samples/sec: 2119.17 - lr: 0.006250\n",
            "2022-11-21 17:08:41,407 epoch 28 - iter 21/71 - loss 0.01979180 - samples/sec: 2061.99 - lr: 0.006250\n",
            "2022-11-21 17:08:41,525 epoch 28 - iter 28/71 - loss 0.01982801 - samples/sec: 1937.86 - lr: 0.006250\n",
            "2022-11-21 17:08:41,642 epoch 28 - iter 35/71 - loss 0.01978274 - samples/sec: 1969.02 - lr: 0.006250\n",
            "2022-11-21 17:08:41,770 epoch 28 - iter 42/71 - loss 0.01977954 - samples/sec: 1780.17 - lr: 0.006250\n",
            "2022-11-21 17:08:41,878 epoch 28 - iter 49/71 - loss 0.01979271 - samples/sec: 2135.67 - lr: 0.006250\n",
            "2022-11-21 17:08:41,992 epoch 28 - iter 56/71 - loss 0.01975499 - samples/sec: 1996.88 - lr: 0.006250\n",
            "2022-11-21 17:08:42,116 epoch 28 - iter 63/71 - loss 0.01972230 - samples/sec: 1838.42 - lr: 0.006250\n",
            "2022-11-21 17:08:42,227 epoch 28 - iter 70/71 - loss 0.01959349 - samples/sec: 2070.92 - lr: 0.006250\n",
            "2022-11-21 17:08:42,235 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:42,236 EPOCH 28 done: loss 0.0198 - lr 0.0062500\n",
            "2022-11-21 17:08:42,396 DEV : loss 0.022858507931232452 - f1-score (micro avg)  0.4946\n",
            "2022-11-21 17:08:42,401 BAD EPOCHS (no improvement): 2\n",
            "2022-11-21 17:08:42,404 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:42,509 epoch 29 - iter 7/71 - loss 0.01969991 - samples/sec: 2185.91 - lr: 0.006250\n",
            "2022-11-21 17:08:42,622 epoch 29 - iter 14/71 - loss 0.01944765 - samples/sec: 2032.59 - lr: 0.006250\n",
            "2022-11-21 17:08:42,739 epoch 29 - iter 21/71 - loss 0.01954527 - samples/sec: 1945.89 - lr: 0.006250\n",
            "2022-11-21 17:08:42,856 epoch 29 - iter 28/71 - loss 0.01951869 - samples/sec: 1949.16 - lr: 0.006250\n",
            "2022-11-21 17:08:42,972 epoch 29 - iter 35/71 - loss 0.01955130 - samples/sec: 1978.82 - lr: 0.006250\n",
            "2022-11-21 17:08:43,082 epoch 29 - iter 42/71 - loss 0.01964166 - samples/sec: 2080.66 - lr: 0.006250\n",
            "2022-11-21 17:08:43,203 epoch 29 - iter 49/71 - loss 0.01952419 - samples/sec: 1882.99 - lr: 0.006250\n",
            "2022-11-21 17:08:43,328 epoch 29 - iter 56/71 - loss 0.01952006 - samples/sec: 1840.74 - lr: 0.006250\n",
            "2022-11-21 17:08:43,438 epoch 29 - iter 63/71 - loss 0.01957728 - samples/sec: 2084.44 - lr: 0.006250\n",
            "2022-11-21 17:08:43,553 epoch 29 - iter 70/71 - loss 0.01959072 - samples/sec: 1989.15 - lr: 0.006250\n",
            "2022-11-21 17:08:43,560 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:43,562 EPOCH 29 done: loss 0.0198 - lr 0.0062500\n",
            "2022-11-21 17:08:43,723 DEV : loss 0.022849010303616524 - f1-score (micro avg)  0.5054\n",
            "2022-11-21 17:08:43,728 BAD EPOCHS (no improvement): 3\n",
            "2022-11-21 17:08:43,731 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:43,839 epoch 30 - iter 7/71 - loss 0.01952549 - samples/sec: 2132.71 - lr: 0.006250\n",
            "2022-11-21 17:08:43,957 epoch 30 - iter 14/71 - loss 0.01963117 - samples/sec: 1932.83 - lr: 0.006250\n",
            "2022-11-21 17:08:44,080 epoch 30 - iter 21/71 - loss 0.01935817 - samples/sec: 1858.63 - lr: 0.006250\n",
            "2022-11-21 17:08:44,192 epoch 30 - iter 28/71 - loss 0.01933457 - samples/sec: 2040.17 - lr: 0.006250\n",
            "2022-11-21 17:08:44,302 epoch 30 - iter 35/71 - loss 0.01934325 - samples/sec: 2077.71 - lr: 0.006250\n",
            "2022-11-21 17:08:44,417 epoch 30 - iter 42/71 - loss 0.01941861 - samples/sec: 1988.37 - lr: 0.006250\n",
            "2022-11-21 17:08:44,534 epoch 30 - iter 49/71 - loss 0.01943397 - samples/sec: 1959.32 - lr: 0.006250\n",
            "2022-11-21 17:08:44,656 epoch 30 - iter 56/71 - loss 0.01951519 - samples/sec: 1877.59 - lr: 0.006250\n",
            "2022-11-21 17:08:44,779 epoch 30 - iter 63/71 - loss 0.01960239 - samples/sec: 1862.77 - lr: 0.006250\n",
            "2022-11-21 17:08:44,895 epoch 30 - iter 70/71 - loss 0.01958964 - samples/sec: 1975.30 - lr: 0.006250\n",
            "2022-11-21 17:08:44,905 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:44,907 EPOCH 30 done: loss 0.0198 - lr 0.0062500\n",
            "2022-11-21 17:08:45,068 DEV : loss 0.022841183468699455 - f1-score (micro avg)  0.5126\n",
            "Epoch    30: reducing learning rate of group 0 to 3.1250e-03.\n",
            "2022-11-21 17:08:45,074 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:08:45,077 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:45,191 epoch 31 - iter 7/71 - loss 0.01952649 - samples/sec: 2010.61 - lr: 0.003125\n",
            "2022-11-21 17:08:45,303 epoch 31 - iter 14/71 - loss 0.01950901 - samples/sec: 2047.55 - lr: 0.003125\n",
            "2022-11-21 17:08:45,415 epoch 31 - iter 21/71 - loss 0.01957647 - samples/sec: 2056.08 - lr: 0.003125\n",
            "2022-11-21 17:08:45,524 epoch 31 - iter 28/71 - loss 0.01977460 - samples/sec: 2102.53 - lr: 0.003125\n",
            "2022-11-21 17:08:45,648 epoch 31 - iter 35/71 - loss 0.01970454 - samples/sec: 1847.53 - lr: 0.003125\n",
            "2022-11-21 17:08:45,765 epoch 31 - iter 42/71 - loss 0.01976903 - samples/sec: 1957.57 - lr: 0.003125\n",
            "2022-11-21 17:08:45,880 epoch 31 - iter 49/71 - loss 0.01965806 - samples/sec: 1983.73 - lr: 0.003125\n",
            "2022-11-21 17:08:45,992 epoch 31 - iter 56/71 - loss 0.01960072 - samples/sec: 2050.38 - lr: 0.003125\n",
            "2022-11-21 17:08:46,121 epoch 31 - iter 63/71 - loss 0.01958610 - samples/sec: 1773.10 - lr: 0.003125\n",
            "2022-11-21 17:08:46,243 epoch 31 - iter 70/71 - loss 0.01957635 - samples/sec: 1939.28 - lr: 0.003125\n",
            "2022-11-21 17:08:46,253 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:46,254 EPOCH 31 done: loss 0.0198 - lr 0.0031250\n",
            "2022-11-21 17:08:46,418 DEV : loss 0.02285752445459366 - f1-score (micro avg)  0.4982\n",
            "2022-11-21 17:08:46,425 BAD EPOCHS (no improvement): 1\n",
            "2022-11-21 17:08:46,426 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:46,549 epoch 32 - iter 7/71 - loss 0.01915209 - samples/sec: 1932.58 - lr: 0.003125\n",
            "2022-11-21 17:08:46,666 epoch 32 - iter 14/71 - loss 0.01946141 - samples/sec: 1958.86 - lr: 0.003125\n",
            "2022-11-21 17:08:46,785 epoch 32 - iter 21/71 - loss 0.01934385 - samples/sec: 1938.14 - lr: 0.003125\n",
            "2022-11-21 17:08:46,906 epoch 32 - iter 28/71 - loss 0.01939882 - samples/sec: 1888.49 - lr: 0.003125\n",
            "2022-11-21 17:08:47,028 epoch 32 - iter 35/71 - loss 0.01948615 - samples/sec: 1911.87 - lr: 0.003125\n",
            "2022-11-21 17:08:47,145 epoch 32 - iter 42/71 - loss 0.01949918 - samples/sec: 1985.39 - lr: 0.003125\n",
            "2022-11-21 17:08:47,266 epoch 32 - iter 49/71 - loss 0.01949340 - samples/sec: 1898.38 - lr: 0.003125\n",
            "2022-11-21 17:08:47,380 epoch 32 - iter 56/71 - loss 0.01957432 - samples/sec: 2014.27 - lr: 0.003125\n",
            "2022-11-21 17:08:47,491 epoch 32 - iter 63/71 - loss 0.01956172 - samples/sec: 2114.97 - lr: 0.003125\n",
            "2022-11-21 17:08:47,619 epoch 32 - iter 70/71 - loss 0.01957103 - samples/sec: 1822.38 - lr: 0.003125\n",
            "2022-11-21 17:08:47,627 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:47,630 EPOCH 32 done: loss 0.0198 - lr 0.0031250\n",
            "2022-11-21 17:08:47,797 DEV : loss 0.02285735495388508 - f1-score (micro avg)  0.4982\n",
            "2022-11-21 17:08:47,805 BAD EPOCHS (no improvement): 2\n",
            "2022-11-21 17:08:47,808 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:47,934 epoch 33 - iter 7/71 - loss 0.01974993 - samples/sec: 1856.07 - lr: 0.003125\n",
            "2022-11-21 17:08:48,046 epoch 33 - iter 14/71 - loss 0.02003060 - samples/sec: 2050.72 - lr: 0.003125\n",
            "2022-11-21 17:08:48,175 epoch 33 - iter 21/71 - loss 0.01993360 - samples/sec: 1812.26 - lr: 0.003125\n",
            "2022-11-21 17:08:48,287 epoch 33 - iter 28/71 - loss 0.01980086 - samples/sec: 2089.74 - lr: 0.003125\n",
            "2022-11-21 17:08:48,414 epoch 33 - iter 35/71 - loss 0.01991209 - samples/sec: 1809.26 - lr: 0.003125\n",
            "2022-11-21 17:08:48,532 epoch 33 - iter 42/71 - loss 0.01984172 - samples/sec: 1956.17 - lr: 0.003125\n",
            "2022-11-21 17:08:49,084 epoch 33 - iter 49/71 - loss 0.01979181 - samples/sec: 408.01 - lr: 0.003125\n",
            "2022-11-21 17:08:49,200 epoch 33 - iter 56/71 - loss 0.01978108 - samples/sec: 1959.94 - lr: 0.003125\n",
            "2022-11-21 17:08:49,320 epoch 33 - iter 63/71 - loss 0.01966068 - samples/sec: 1949.49 - lr: 0.003125\n",
            "2022-11-21 17:08:49,430 epoch 33 - iter 70/71 - loss 0.01956267 - samples/sec: 2106.32 - lr: 0.003125\n",
            "2022-11-21 17:08:49,441 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:49,444 EPOCH 33 done: loss 0.0200 - lr 0.0031250\n",
            "2022-11-21 17:08:49,608 DEV : loss 0.022837521508336067 - f1-score (micro avg)  0.5126\n",
            "2022-11-21 17:08:49,614 BAD EPOCHS (no improvement): 3\n",
            "2022-11-21 17:08:49,618 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:49,743 epoch 34 - iter 7/71 - loss 0.02004367 - samples/sec: 1848.99 - lr: 0.003125\n",
            "2022-11-21 17:08:49,864 epoch 34 - iter 14/71 - loss 0.02003479 - samples/sec: 1965.21 - lr: 0.003125\n",
            "2022-11-21 17:08:49,985 epoch 34 - iter 21/71 - loss 0.01962215 - samples/sec: 1890.95 - lr: 0.003125\n",
            "2022-11-21 17:08:50,102 epoch 34 - iter 28/71 - loss 0.01965495 - samples/sec: 1961.97 - lr: 0.003125\n",
            "2022-11-21 17:08:50,212 epoch 34 - iter 35/71 - loss 0.01975477 - samples/sec: 2090.65 - lr: 0.003125\n",
            "2022-11-21 17:08:50,326 epoch 34 - iter 42/71 - loss 0.01968813 - samples/sec: 2000.74 - lr: 0.003125\n",
            "2022-11-21 17:08:50,441 epoch 34 - iter 49/71 - loss 0.01958769 - samples/sec: 2000.07 - lr: 0.003125\n",
            "2022-11-21 17:08:50,556 epoch 34 - iter 56/71 - loss 0.01956917 - samples/sec: 1995.92 - lr: 0.003125\n",
            "2022-11-21 17:08:50,668 epoch 34 - iter 63/71 - loss 0.01951636 - samples/sec: 2037.44 - lr: 0.003125\n",
            "2022-11-21 17:08:50,785 epoch 34 - iter 70/71 - loss 0.01956357 - samples/sec: 1972.79 - lr: 0.003125\n",
            "2022-11-21 17:08:50,793 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:50,796 EPOCH 34 done: loss 0.0199 - lr 0.0031250\n",
            "2022-11-21 17:08:50,958 DEV : loss 0.022837387397885323 - f1-score (micro avg)  0.509\n",
            "Epoch    34: reducing learning rate of group 0 to 1.5625e-03.\n",
            "2022-11-21 17:08:50,966 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:08:50,970 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:51,089 epoch 35 - iter 7/71 - loss 0.01952240 - samples/sec: 1937.38 - lr: 0.001563\n",
            "2022-11-21 17:08:51,204 epoch 35 - iter 14/71 - loss 0.01977199 - samples/sec: 1998.69 - lr: 0.001563\n",
            "2022-11-21 17:08:51,321 epoch 35 - iter 21/71 - loss 0.01962490 - samples/sec: 1975.49 - lr: 0.001563\n",
            "2022-11-21 17:08:51,489 epoch 35 - iter 28/71 - loss 0.01961144 - samples/sec: 1357.76 - lr: 0.001563\n",
            "2022-11-21 17:08:51,612 epoch 35 - iter 35/71 - loss 0.01964172 - samples/sec: 1870.98 - lr: 0.001563\n",
            "2022-11-21 17:08:51,726 epoch 35 - iter 42/71 - loss 0.01967982 - samples/sec: 2030.35 - lr: 0.001563\n",
            "2022-11-21 17:08:51,839 epoch 35 - iter 49/71 - loss 0.01962944 - samples/sec: 2056.30 - lr: 0.001563\n",
            "2022-11-21 17:08:51,962 epoch 35 - iter 56/71 - loss 0.01969108 - samples/sec: 1863.77 - lr: 0.001563\n",
            "2022-11-21 17:08:52,081 epoch 35 - iter 63/71 - loss 0.01961452 - samples/sec: 1932.54 - lr: 0.001563\n",
            "2022-11-21 17:08:52,199 epoch 35 - iter 70/71 - loss 0.01956421 - samples/sec: 1955.47 - lr: 0.001563\n",
            "2022-11-21 17:08:52,210 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:52,211 EPOCH 35 done: loss 0.0197 - lr 0.0015625\n",
            "2022-11-21 17:08:52,377 DEV : loss 0.022838426753878593 - f1-score (micro avg)  0.509\n",
            "2022-11-21 17:08:52,385 BAD EPOCHS (no improvement): 1\n",
            "2022-11-21 17:08:52,386 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:52,509 epoch 36 - iter 7/71 - loss 0.01927251 - samples/sec: 1931.77 - lr: 0.001563\n",
            "2022-11-21 17:08:52,627 epoch 36 - iter 14/71 - loss 0.01920402 - samples/sec: 1938.68 - lr: 0.001563\n",
            "2022-11-21 17:08:52,744 epoch 36 - iter 21/71 - loss 0.01916191 - samples/sec: 1962.89 - lr: 0.001563\n",
            "2022-11-21 17:08:52,866 epoch 36 - iter 28/71 - loss 0.01937298 - samples/sec: 1913.50 - lr: 0.001563\n",
            "2022-11-21 17:08:52,986 epoch 36 - iter 35/71 - loss 0.01944004 - samples/sec: 1920.49 - lr: 0.001563\n",
            "2022-11-21 17:08:53,100 epoch 36 - iter 42/71 - loss 0.01960114 - samples/sec: 2015.18 - lr: 0.001563\n",
            "2022-11-21 17:08:53,217 epoch 36 - iter 49/71 - loss 0.01949734 - samples/sec: 2025.21 - lr: 0.001563\n",
            "2022-11-21 17:08:53,338 epoch 36 - iter 56/71 - loss 0.01952889 - samples/sec: 1922.18 - lr: 0.001563\n",
            "2022-11-21 17:08:53,460 epoch 36 - iter 63/71 - loss 0.01954944 - samples/sec: 1877.93 - lr: 0.001563\n",
            "2022-11-21 17:08:53,574 epoch 36 - iter 70/71 - loss 0.01956251 - samples/sec: 2028.79 - lr: 0.001563\n",
            "2022-11-21 17:08:53,582 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:53,584 EPOCH 36 done: loss 0.0197 - lr 0.0015625\n",
            "2022-11-21 17:08:53,746 DEV : loss 0.022840362042188644 - f1-score (micro avg)  0.509\n",
            "2022-11-21 17:08:53,752 BAD EPOCHS (no improvement): 2\n",
            "2022-11-21 17:08:53,754 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:53,870 epoch 37 - iter 7/71 - loss 0.01951227 - samples/sec: 2045.45 - lr: 0.001563\n",
            "2022-11-21 17:08:53,986 epoch 37 - iter 14/71 - loss 0.01953293 - samples/sec: 1994.66 - lr: 0.001563\n",
            "2022-11-21 17:08:54,102 epoch 37 - iter 21/71 - loss 0.01988991 - samples/sec: 1969.88 - lr: 0.001563\n",
            "2022-11-21 17:08:54,220 epoch 37 - iter 28/71 - loss 0.01985919 - samples/sec: 1955.04 - lr: 0.001563\n",
            "2022-11-21 17:08:54,328 epoch 37 - iter 35/71 - loss 0.01965860 - samples/sec: 2131.40 - lr: 0.001563\n",
            "2022-11-21 17:08:54,449 epoch 37 - iter 42/71 - loss 0.01963009 - samples/sec: 1887.60 - lr: 0.001563\n",
            "2022-11-21 17:08:54,569 epoch 37 - iter 49/71 - loss 0.01955595 - samples/sec: 1907.41 - lr: 0.001563\n",
            "2022-11-21 17:08:54,696 epoch 37 - iter 56/71 - loss 0.01952743 - samples/sec: 1810.37 - lr: 0.001563\n",
            "2022-11-21 17:08:54,813 epoch 37 - iter 63/71 - loss 0.01953164 - samples/sec: 1959.64 - lr: 0.001563\n",
            "2022-11-21 17:08:54,936 epoch 37 - iter 70/71 - loss 0.01955442 - samples/sec: 1869.19 - lr: 0.001563\n",
            "2022-11-21 17:08:54,947 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:54,949 EPOCH 37 done: loss 0.0199 - lr 0.0015625\n",
            "2022-11-21 17:08:55,121 DEV : loss 0.022836828604340553 - f1-score (micro avg)  0.509\n",
            "2022-11-21 17:08:55,128 BAD EPOCHS (no improvement): 3\n",
            "2022-11-21 17:08:55,132 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:55,254 epoch 38 - iter 7/71 - loss 0.01976889 - samples/sec: 1908.31 - lr: 0.001563\n",
            "2022-11-21 17:08:55,369 epoch 38 - iter 14/71 - loss 0.01942389 - samples/sec: 2015.14 - lr: 0.001563\n",
            "2022-11-21 17:08:55,489 epoch 38 - iter 21/71 - loss 0.01942494 - samples/sec: 1927.38 - lr: 0.001563\n",
            "2022-11-21 17:08:55,623 epoch 38 - iter 28/71 - loss 0.01960908 - samples/sec: 1706.88 - lr: 0.001563\n",
            "2022-11-21 17:08:55,733 epoch 38 - iter 35/71 - loss 0.01961044 - samples/sec: 2080.97 - lr: 0.001563\n",
            "2022-11-21 17:08:55,855 epoch 38 - iter 42/71 - loss 0.01953268 - samples/sec: 1879.85 - lr: 0.001563\n",
            "2022-11-21 17:08:55,976 epoch 38 - iter 49/71 - loss 0.01953656 - samples/sec: 1895.69 - lr: 0.001563\n",
            "2022-11-21 17:08:56,089 epoch 38 - iter 56/71 - loss 0.01948618 - samples/sec: 2039.76 - lr: 0.001563\n",
            "2022-11-21 17:08:56,211 epoch 38 - iter 63/71 - loss 0.01951508 - samples/sec: 1886.64 - lr: 0.001563\n",
            "2022-11-21 17:08:56,336 epoch 38 - iter 70/71 - loss 0.01955942 - samples/sec: 1829.32 - lr: 0.001563\n",
            "2022-11-21 17:08:56,350 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:56,351 EPOCH 38 done: loss 0.0197 - lr 0.0015625\n",
            "2022-11-21 17:08:56,517 DEV : loss 0.02284242957830429 - f1-score (micro avg)  0.509\n",
            "Epoch    38: reducing learning rate of group 0 to 7.8125e-04.\n",
            "2022-11-21 17:08:56,523 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:08:56,526 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:56,644 epoch 39 - iter 7/71 - loss 0.01904724 - samples/sec: 1963.40 - lr: 0.000781\n",
            "2022-11-21 17:08:56,759 epoch 39 - iter 14/71 - loss 0.01908832 - samples/sec: 1985.33 - lr: 0.000781\n",
            "2022-11-21 17:08:56,871 epoch 39 - iter 21/71 - loss 0.01941669 - samples/sec: 2059.67 - lr: 0.000781\n",
            "2022-11-21 17:08:56,991 epoch 39 - iter 28/71 - loss 0.01959589 - samples/sec: 1920.21 - lr: 0.000781\n",
            "2022-11-21 17:08:57,108 epoch 39 - iter 35/71 - loss 0.01958959 - samples/sec: 1973.42 - lr: 0.000781\n",
            "2022-11-21 17:08:57,232 epoch 39 - iter 42/71 - loss 0.01957500 - samples/sec: 1849.86 - lr: 0.000781\n",
            "2022-11-21 17:08:57,357 epoch 39 - iter 49/71 - loss 0.01953209 - samples/sec: 1834.33 - lr: 0.000781\n",
            "2022-11-21 17:08:57,483 epoch 39 - iter 56/71 - loss 0.01954751 - samples/sec: 1837.08 - lr: 0.000781\n",
            "2022-11-21 17:08:57,616 epoch 39 - iter 63/71 - loss 0.01958455 - samples/sec: 1733.96 - lr: 0.000781\n",
            "2022-11-21 17:08:57,730 epoch 39 - iter 70/71 - loss 0.01955474 - samples/sec: 2005.70 - lr: 0.000781\n",
            "2022-11-21 17:08:57,740 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:57,742 EPOCH 39 done: loss 0.0197 - lr 0.0007813\n",
            "2022-11-21 17:08:57,906 DEV : loss 0.022846613079309464 - f1-score (micro avg)  0.5054\n",
            "2022-11-21 17:08:57,915 BAD EPOCHS (no improvement): 1\n",
            "2022-11-21 17:08:57,916 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:58,033 epoch 40 - iter 7/71 - loss 0.01939962 - samples/sec: 2008.24 - lr: 0.000781\n",
            "2022-11-21 17:08:58,148 epoch 40 - iter 14/71 - loss 0.01965971 - samples/sec: 1999.54 - lr: 0.000781\n",
            "2022-11-21 17:08:58,262 epoch 40 - iter 21/71 - loss 0.01961515 - samples/sec: 2032.60 - lr: 0.000781\n",
            "2022-11-21 17:08:58,381 epoch 40 - iter 28/71 - loss 0.01968930 - samples/sec: 1937.39 - lr: 0.000781\n",
            "2022-11-21 17:08:58,498 epoch 40 - iter 35/71 - loss 0.01970562 - samples/sec: 1967.38 - lr: 0.000781\n",
            "2022-11-21 17:08:58,620 epoch 40 - iter 42/71 - loss 0.01961409 - samples/sec: 1900.08 - lr: 0.000781\n",
            "2022-11-21 17:08:58,738 epoch 40 - iter 49/71 - loss 0.01959446 - samples/sec: 1957.68 - lr: 0.000781\n",
            "2022-11-21 17:08:58,860 epoch 40 - iter 56/71 - loss 0.01955240 - samples/sec: 1890.51 - lr: 0.000781\n",
            "2022-11-21 17:08:58,979 epoch 40 - iter 63/71 - loss 0.01960788 - samples/sec: 1919.83 - lr: 0.000781\n",
            "2022-11-21 17:08:59,105 epoch 40 - iter 70/71 - loss 0.01954740 - samples/sec: 1830.97 - lr: 0.000781\n",
            "2022-11-21 17:08:59,114 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:59,115 EPOCH 40 done: loss 0.0199 - lr 0.0007813\n",
            "2022-11-21 17:08:59,280 DEV : loss 0.022851191461086273 - f1-score (micro avg)  0.4982\n",
            "2022-11-21 17:08:59,288 BAD EPOCHS (no improvement): 2\n",
            "2022-11-21 17:08:59,290 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:08:59,402 epoch 41 - iter 7/71 - loss 0.01935947 - samples/sec: 2070.75 - lr: 0.000781\n",
            "2022-11-21 17:08:59,531 epoch 41 - iter 14/71 - loss 0.01942580 - samples/sec: 1781.07 - lr: 0.000781\n",
            "2022-11-21 17:08:59,646 epoch 41 - iter 21/71 - loss 0.01943037 - samples/sec: 2002.69 - lr: 0.000781\n",
            "2022-11-21 17:08:59,767 epoch 41 - iter 28/71 - loss 0.01938148 - samples/sec: 1908.30 - lr: 0.000781\n",
            "2022-11-21 17:08:59,887 epoch 41 - iter 35/71 - loss 0.01926607 - samples/sec: 1911.92 - lr: 0.000781\n",
            "2022-11-21 17:09:00,010 epoch 41 - iter 42/71 - loss 0.01939703 - samples/sec: 1878.42 - lr: 0.000781\n",
            "2022-11-21 17:09:00,137 epoch 41 - iter 49/71 - loss 0.01951889 - samples/sec: 1833.85 - lr: 0.000781\n",
            "2022-11-21 17:09:00,267 epoch 41 - iter 56/71 - loss 0.01949711 - samples/sec: 1771.37 - lr: 0.000781\n",
            "2022-11-21 17:09:00,438 epoch 41 - iter 63/71 - loss 0.01947597 - samples/sec: 1335.77 - lr: 0.000781\n",
            "2022-11-21 17:09:00,586 epoch 41 - iter 70/71 - loss 0.01955360 - samples/sec: 1541.01 - lr: 0.000781\n",
            "2022-11-21 17:09:00,595 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:09:00,597 EPOCH 41 done: loss 0.0197 - lr 0.0007813\n",
            "2022-11-21 17:09:00,760 DEV : loss 0.022850781679153442 - f1-score (micro avg)  0.4982\n",
            "2022-11-21 17:09:00,767 BAD EPOCHS (no improvement): 3\n",
            "2022-11-21 17:09:00,770 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:09:00,880 epoch 42 - iter 7/71 - loss 0.01939630 - samples/sec: 2095.06 - lr: 0.000781\n",
            "2022-11-21 17:09:00,994 epoch 42 - iter 14/71 - loss 0.01943837 - samples/sec: 2003.38 - lr: 0.000781\n",
            "2022-11-21 17:09:01,113 epoch 42 - iter 21/71 - loss 0.01974436 - samples/sec: 1919.15 - lr: 0.000781\n",
            "2022-11-21 17:09:01,231 epoch 42 - iter 28/71 - loss 0.01980998 - samples/sec: 1984.24 - lr: 0.000781\n",
            "2022-11-21 17:09:01,351 epoch 42 - iter 35/71 - loss 0.01976467 - samples/sec: 1923.74 - lr: 0.000781\n",
            "2022-11-21 17:09:01,473 epoch 42 - iter 42/71 - loss 0.01961239 - samples/sec: 1878.87 - lr: 0.000781\n",
            "2022-11-21 17:09:01,603 epoch 42 - iter 49/71 - loss 0.01966107 - samples/sec: 1775.44 - lr: 0.000781\n",
            "2022-11-21 17:09:01,745 epoch 42 - iter 56/71 - loss 0.01956573 - samples/sec: 1625.46 - lr: 0.000781\n",
            "2022-11-21 17:09:01,874 epoch 42 - iter 63/71 - loss 0.01957025 - samples/sec: 1816.43 - lr: 0.000781\n",
            "2022-11-21 17:09:02,002 epoch 42 - iter 70/71 - loss 0.01954922 - samples/sec: 1803.20 - lr: 0.000781\n",
            "2022-11-21 17:09:02,012 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:09:02,013 EPOCH 42 done: loss 0.0198 - lr 0.0007813\n",
            "2022-11-21 17:09:02,194 DEV : loss 0.022854110226035118 - f1-score (micro avg)  0.4946\n",
            "Epoch    42: reducing learning rate of group 0 to 3.9063e-04.\n",
            "2022-11-21 17:09:02,203 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:09:02,205 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:09:02,328 epoch 43 - iter 7/71 - loss 0.01969883 - samples/sec: 1866.10 - lr: 0.000391\n",
            "2022-11-21 17:09:02,458 epoch 43 - iter 14/71 - loss 0.01949791 - samples/sec: 1785.20 - lr: 0.000391\n",
            "2022-11-21 17:09:02,585 epoch 43 - iter 21/71 - loss 0.01965112 - samples/sec: 1803.92 - lr: 0.000391\n",
            "2022-11-21 17:09:02,724 epoch 43 - iter 28/71 - loss 0.01960625 - samples/sec: 1714.52 - lr: 0.000391\n",
            "2022-11-21 17:09:02,862 epoch 43 - iter 35/71 - loss 0.01966981 - samples/sec: 1691.13 - lr: 0.000391\n",
            "2022-11-21 17:09:02,988 epoch 43 - iter 42/71 - loss 0.01964497 - samples/sec: 1852.74 - lr: 0.000391\n",
            "2022-11-21 17:09:03,109 epoch 43 - iter 49/71 - loss 0.01960968 - samples/sec: 1885.22 - lr: 0.000391\n",
            "2022-11-21 17:09:03,237 epoch 43 - iter 56/71 - loss 0.01958563 - samples/sec: 1788.97 - lr: 0.000391\n",
            "2022-11-21 17:09:03,348 epoch 43 - iter 63/71 - loss 0.01954170 - samples/sec: 2098.11 - lr: 0.000391\n",
            "2022-11-21 17:09:03,464 epoch 43 - iter 70/71 - loss 0.01954015 - samples/sec: 1988.84 - lr: 0.000391\n",
            "2022-11-21 17:09:03,474 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:09:03,476 EPOCH 43 done: loss 0.0200 - lr 0.0003906\n",
            "2022-11-21 17:09:03,649 DEV : loss 0.022855469956994057 - f1-score (micro avg)  0.4946\n",
            "2022-11-21 17:09:03,657 BAD EPOCHS (no improvement): 1\n",
            "2022-11-21 17:09:03,660 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:09:03,779 epoch 44 - iter 7/71 - loss 0.01895640 - samples/sec: 1916.53 - lr: 0.000391\n",
            "2022-11-21 17:09:03,893 epoch 44 - iter 14/71 - loss 0.01942230 - samples/sec: 2035.69 - lr: 0.000391\n",
            "2022-11-21 17:09:04,004 epoch 44 - iter 21/71 - loss 0.01948149 - samples/sec: 2053.58 - lr: 0.000391\n",
            "2022-11-21 17:09:04,123 epoch 44 - iter 28/71 - loss 0.01962714 - samples/sec: 1929.47 - lr: 0.000391\n",
            "2022-11-21 17:09:04,236 epoch 44 - iter 35/71 - loss 0.01948072 - samples/sec: 2054.25 - lr: 0.000391\n",
            "2022-11-21 17:09:04,358 epoch 44 - iter 42/71 - loss 0.01947816 - samples/sec: 1905.28 - lr: 0.000391\n",
            "2022-11-21 17:09:04,472 epoch 44 - iter 49/71 - loss 0.01954443 - samples/sec: 2012.87 - lr: 0.000391\n",
            "2022-11-21 17:09:04,589 epoch 44 - iter 56/71 - loss 0.01952021 - samples/sec: 1965.75 - lr: 0.000391\n",
            "2022-11-21 17:09:04,713 epoch 44 - iter 63/71 - loss 0.01951797 - samples/sec: 1844.98 - lr: 0.000391\n",
            "2022-11-21 17:09:04,823 epoch 44 - iter 70/71 - loss 0.01954354 - samples/sec: 2077.30 - lr: 0.000391\n",
            "2022-11-21 17:09:04,832 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:09:04,833 EPOCH 44 done: loss 0.0199 - lr 0.0003906\n",
            "2022-11-21 17:09:05,009 DEV : loss 0.02285407856106758 - f1-score (micro avg)  0.4946\n",
            "2022-11-21 17:09:05,015 BAD EPOCHS (no improvement): 2\n",
            "2022-11-21 17:09:05,022 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:09:05,142 epoch 45 - iter 7/71 - loss 0.01952290 - samples/sec: 1981.46 - lr: 0.000391\n",
            "2022-11-21 17:09:05,257 epoch 45 - iter 14/71 - loss 0.01958912 - samples/sec: 2006.64 - lr: 0.000391\n",
            "2022-11-21 17:09:05,375 epoch 45 - iter 21/71 - loss 0.01987891 - samples/sec: 1935.10 - lr: 0.000391\n",
            "2022-11-21 17:09:05,492 epoch 45 - iter 28/71 - loss 0.01968859 - samples/sec: 1969.09 - lr: 0.000391\n",
            "2022-11-21 17:09:05,618 epoch 45 - iter 35/71 - loss 0.01964696 - samples/sec: 1819.49 - lr: 0.000391\n",
            "2022-11-21 17:09:05,743 epoch 45 - iter 42/71 - loss 0.01963807 - samples/sec: 1844.75 - lr: 0.000391\n",
            "2022-11-21 17:09:05,868 epoch 45 - iter 49/71 - loss 0.01963988 - samples/sec: 1848.58 - lr: 0.000391\n",
            "2022-11-21 17:09:05,977 epoch 45 - iter 56/71 - loss 0.01961247 - samples/sec: 2108.47 - lr: 0.000391\n",
            "2022-11-21 17:09:06,103 epoch 45 - iter 63/71 - loss 0.01957436 - samples/sec: 1828.32 - lr: 0.000391\n",
            "2022-11-21 17:09:06,223 epoch 45 - iter 70/71 - loss 0.01954875 - samples/sec: 1921.22 - lr: 0.000391\n",
            "2022-11-21 17:09:06,234 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:09:06,235 EPOCH 45 done: loss 0.0197 - lr 0.0003906\n",
            "2022-11-21 17:09:06,405 DEV : loss 0.022852519527077675 - f1-score (micro avg)  0.4946\n",
            "2022-11-21 17:09:06,413 BAD EPOCHS (no improvement): 3\n",
            "2022-11-21 17:09:06,417 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:09:06,527 epoch 46 - iter 7/71 - loss 0.01965450 - samples/sec: 2097.62 - lr: 0.000391\n",
            "2022-11-21 17:09:06,642 epoch 46 - iter 14/71 - loss 0.01938139 - samples/sec: 1990.37 - lr: 0.000391\n",
            "2022-11-21 17:09:06,772 epoch 46 - iter 21/71 - loss 0.01940466 - samples/sec: 1785.04 - lr: 0.000391\n",
            "2022-11-21 17:09:06,899 epoch 46 - iter 28/71 - loss 0.01942487 - samples/sec: 1809.03 - lr: 0.000391\n",
            "2022-11-21 17:09:07,020 epoch 46 - iter 35/71 - loss 0.01955300 - samples/sec: 1912.92 - lr: 0.000391\n",
            "2022-11-21 17:09:07,140 epoch 46 - iter 42/71 - loss 0.01952083 - samples/sec: 1918.08 - lr: 0.000391\n",
            "2022-11-21 17:09:07,257 epoch 46 - iter 49/71 - loss 0.01959081 - samples/sec: 1962.81 - lr: 0.000391\n",
            "2022-11-21 17:09:07,377 epoch 46 - iter 56/71 - loss 0.01956133 - samples/sec: 1929.35 - lr: 0.000391\n",
            "2022-11-21 17:09:07,494 epoch 46 - iter 63/71 - loss 0.01951114 - samples/sec: 1968.57 - lr: 0.000391\n",
            "2022-11-21 17:09:07,609 epoch 46 - iter 70/71 - loss 0.01954707 - samples/sec: 2005.32 - lr: 0.000391\n",
            "2022-11-21 17:09:07,620 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:09:07,623 EPOCH 46 done: loss 0.0198 - lr 0.0003906\n",
            "2022-11-21 17:09:07,784 DEV : loss 0.022854572162032127 - f1-score (micro avg)  0.4946\n",
            "Epoch    46: reducing learning rate of group 0 to 1.9531e-04.\n",
            "2022-11-21 17:09:07,791 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:09:07,794 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:09:07,904 epoch 47 - iter 7/71 - loss 0.01978617 - samples/sec: 2100.37 - lr: 0.000195\n",
            "2022-11-21 17:09:08,023 epoch 47 - iter 14/71 - loss 0.01979356 - samples/sec: 1926.37 - lr: 0.000195\n",
            "2022-11-21 17:09:08,147 epoch 47 - iter 21/71 - loss 0.01960212 - samples/sec: 1860.61 - lr: 0.000195\n",
            "2022-11-21 17:09:08,259 epoch 47 - iter 28/71 - loss 0.01946519 - samples/sec: 2047.34 - lr: 0.000195\n",
            "2022-11-21 17:09:08,375 epoch 47 - iter 35/71 - loss 0.01961205 - samples/sec: 1986.63 - lr: 0.000195\n",
            "2022-11-21 17:09:08,498 epoch 47 - iter 42/71 - loss 0.01959789 - samples/sec: 1865.91 - lr: 0.000195\n",
            "2022-11-21 17:09:08,619 epoch 47 - iter 49/71 - loss 0.01955935 - samples/sec: 1902.33 - lr: 0.000195\n",
            "2022-11-21 17:09:08,739 epoch 47 - iter 56/71 - loss 0.01961687 - samples/sec: 1956.91 - lr: 0.000195\n",
            "2022-11-21 17:09:08,864 epoch 47 - iter 63/71 - loss 0.01960624 - samples/sec: 1853.27 - lr: 0.000195\n",
            "2022-11-21 17:09:08,985 epoch 47 - iter 70/71 - loss 0.01954794 - samples/sec: 1899.34 - lr: 0.000195\n",
            "2022-11-21 17:09:08,995 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:09:08,997 EPOCH 47 done: loss 0.0197 - lr 0.0001953\n",
            "2022-11-21 17:09:09,164 DEV : loss 0.022855546325445175 - f1-score (micro avg)  0.4946\n",
            "2022-11-21 17:09:09,171 BAD EPOCHS (no improvement): 1\n",
            "2022-11-21 17:09:09,174 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:09:09,292 epoch 48 - iter 7/71 - loss 0.01959443 - samples/sec: 1954.63 - lr: 0.000195\n",
            "2022-11-21 17:09:09,413 epoch 48 - iter 14/71 - loss 0.01946999 - samples/sec: 1882.15 - lr: 0.000195\n",
            "2022-11-21 17:09:09,535 epoch 48 - iter 21/71 - loss 0.01936684 - samples/sec: 1883.40 - lr: 0.000195\n",
            "2022-11-21 17:09:09,656 epoch 48 - iter 28/71 - loss 0.01949449 - samples/sec: 1899.58 - lr: 0.000195\n",
            "2022-11-21 17:09:09,775 epoch 48 - iter 35/71 - loss 0.01951953 - samples/sec: 1968.93 - lr: 0.000195\n",
            "2022-11-21 17:09:09,898 epoch 48 - iter 42/71 - loss 0.01941406 - samples/sec: 1871.98 - lr: 0.000195\n",
            "2022-11-21 17:09:10,018 epoch 48 - iter 49/71 - loss 0.01950056 - samples/sec: 1907.08 - lr: 0.000195\n",
            "2022-11-21 17:09:10,156 epoch 48 - iter 56/71 - loss 0.01955326 - samples/sec: 1666.44 - lr: 0.000195\n",
            "2022-11-21 17:09:10,272 epoch 48 - iter 63/71 - loss 0.01958854 - samples/sec: 1983.46 - lr: 0.000195\n",
            "2022-11-21 17:09:10,390 epoch 48 - iter 70/71 - loss 0.01954475 - samples/sec: 1981.52 - lr: 0.000195\n",
            "2022-11-21 17:09:10,399 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:09:10,401 EPOCH 48 done: loss 0.0198 - lr 0.0001953\n",
            "2022-11-21 17:09:10,566 DEV : loss 0.02285517379641533 - f1-score (micro avg)  0.4946\n",
            "2022-11-21 17:09:10,575 BAD EPOCHS (no improvement): 2\n",
            "2022-11-21 17:09:10,579 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:09:10,699 epoch 49 - iter 7/71 - loss 0.01946630 - samples/sec: 1914.81 - lr: 0.000195\n",
            "2022-11-21 17:09:10,819 epoch 49 - iter 14/71 - loss 0.01945432 - samples/sec: 1903.40 - lr: 0.000195\n",
            "2022-11-21 17:09:10,942 epoch 49 - iter 21/71 - loss 0.01966979 - samples/sec: 1865.66 - lr: 0.000195\n",
            "2022-11-21 17:09:11,056 epoch 49 - iter 28/71 - loss 0.01965304 - samples/sec: 2020.16 - lr: 0.000195\n",
            "2022-11-21 17:09:11,169 epoch 49 - iter 35/71 - loss 0.01961126 - samples/sec: 2045.79 - lr: 0.000195\n",
            "2022-11-21 17:09:11,286 epoch 49 - iter 42/71 - loss 0.01965526 - samples/sec: 1964.14 - lr: 0.000195\n",
            "2022-11-21 17:09:11,408 epoch 49 - iter 49/71 - loss 0.01967172 - samples/sec: 1892.27 - lr: 0.000195\n",
            "2022-11-21 17:09:11,522 epoch 49 - iter 56/71 - loss 0.01958590 - samples/sec: 2014.28 - lr: 0.000195\n",
            "2022-11-21 17:09:11,639 epoch 49 - iter 63/71 - loss 0.01954370 - samples/sec: 1979.16 - lr: 0.000195\n",
            "2022-11-21 17:09:11,764 epoch 49 - iter 70/71 - loss 0.01954484 - samples/sec: 1832.72 - lr: 0.000195\n",
            "2022-11-21 17:09:11,776 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:09:11,777 EPOCH 49 done: loss 0.0198 - lr 0.0001953\n",
            "2022-11-21 17:09:11,937 DEV : loss 0.022854939103126526 - f1-score (micro avg)  0.4946\n",
            "2022-11-21 17:09:11,943 BAD EPOCHS (no improvement): 3\n",
            "2022-11-21 17:09:11,946 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:09:12,069 epoch 50 - iter 7/71 - loss 0.01990056 - samples/sec: 1902.85 - lr: 0.000195\n",
            "2022-11-21 17:09:12,188 epoch 50 - iter 14/71 - loss 0.01986407 - samples/sec: 1928.88 - lr: 0.000195\n",
            "2022-11-21 17:09:12,308 epoch 50 - iter 21/71 - loss 0.01981297 - samples/sec: 1922.47 - lr: 0.000195\n",
            "2022-11-21 17:09:12,416 epoch 50 - iter 28/71 - loss 0.01961604 - samples/sec: 2127.30 - lr: 0.000195\n",
            "2022-11-21 17:09:12,534 epoch 50 - iter 35/71 - loss 0.01969049 - samples/sec: 1944.47 - lr: 0.000195\n",
            "2022-11-21 17:09:12,644 epoch 50 - iter 42/71 - loss 0.01961421 - samples/sec: 2075.89 - lr: 0.000195\n",
            "2022-11-21 17:09:12,769 epoch 50 - iter 49/71 - loss 0.01960177 - samples/sec: 1857.62 - lr: 0.000195\n",
            "2022-11-21 17:09:12,899 epoch 50 - iter 56/71 - loss 0.01956368 - samples/sec: 1775.18 - lr: 0.000195\n",
            "2022-11-21 17:09:13,011 epoch 50 - iter 63/71 - loss 0.01956552 - samples/sec: 2061.37 - lr: 0.000195\n",
            "2022-11-21 17:09:13,129 epoch 50 - iter 70/71 - loss 0.01954717 - samples/sec: 1981.68 - lr: 0.000195\n",
            "2022-11-21 17:09:13,141 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:09:13,144 EPOCH 50 done: loss 0.0197 - lr 0.0001953\n",
            "2022-11-21 17:09:13,312 DEV : loss 0.022854626178741455 - f1-score (micro avg)  0.4946\n",
            "Epoch    50: reducing learning rate of group 0 to 9.7656e-05.\n",
            "2022-11-21 17:09:13,320 BAD EPOCHS (no improvement): 4\n",
            "2022-11-21 17:09:13,322 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:09:13,325 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:09:13,327 learning rate too small - quitting training!\n",
            "2022-11-21 17:09:13,329 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:09:20,054 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-21 17:09:20,059 loading file resources/word-pair-test-flair/best-model.pt\n",
            "2022-11-21 17:09:22,560 0.5141\t0.5141\t0.5141\t0.5141\n",
            "2022-11-21 17:09:22,562 \n",
            "Results:\n",
            "- F-score (micro) 0.5141\n",
            "- F-score (macro) 0.4984\n",
            "- Accuracy 0.5141\n",
            "\n",
            "By class:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "    entailment     0.5342    0.6515    0.5870       132\n",
            "not_entailment     0.4773    0.3590    0.4098       117\n",
            "\n",
            "     micro avg     0.5141    0.5141    0.5141       249\n",
            "     macro avg     0.5057    0.5052    0.4984       249\n",
            "  weighted avg     0.5074    0.5141    0.5037       249\n",
            "   samples avg     0.5141    0.5141    0.5141       249\n",
            "\n",
            "2022-11-21 17:09:22,565 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_score': 0.5140562248995983,\n",
              " 'dev_score_history': [0.46570397111913364,\n",
              "  0.4729241877256318,\n",
              "  0.5234657039711191,\n",
              "  0.4620938628158845,\n",
              "  0.5487364620938628,\n",
              "  0.5342960288808665,\n",
              "  0.5451263537906137,\n",
              "  0.5415162454873647,\n",
              "  0.4693140794223827,\n",
              "  0.5451263537906137,\n",
              "  0.5487364620938628,\n",
              "  0.5379061371841155,\n",
              "  0.4620938628158845,\n",
              "  0.5595667870036101,\n",
              "  0.5234657039711191,\n",
              "  0.4548736462093863,\n",
              "  0.49458483754512633,\n",
              "  0.49458483754512633,\n",
              "  0.49097472924187724,\n",
              "  0.5234657039711191,\n",
              "  0.5090252707581228,\n",
              "  0.5342960288808665,\n",
              "  0.5415162454873647,\n",
              "  0.51985559566787,\n",
              "  0.5270758122743683,\n",
              "  0.5234657039711191,\n",
              "  0.4981949458483754,\n",
              "  0.49458483754512633,\n",
              "  0.5054151624548736,\n",
              "  0.5126353790613718,\n",
              "  0.4981949458483754,\n",
              "  0.4981949458483754,\n",
              "  0.5126353790613718,\n",
              "  0.5090252707581228,\n",
              "  0.5090252707581228,\n",
              "  0.5090252707581228,\n",
              "  0.5090252707581228,\n",
              "  0.5090252707581228,\n",
              "  0.5054151624548736,\n",
              "  0.4981949458483754,\n",
              "  0.4981949458483754,\n",
              "  0.49458483754512633,\n",
              "  0.49458483754512633,\n",
              "  0.49458483754512633,\n",
              "  0.49458483754512633,\n",
              "  0.49458483754512633,\n",
              "  0.49458483754512633,\n",
              "  0.49458483754512633,\n",
              "  0.49458483754512633,\n",
              "  0.49458483754512633],\n",
              " 'train_loss_history': [0.02429734367922128,\n",
              "  0.022187951551596537,\n",
              "  0.021934603564923282,\n",
              "  0.021665856328621235,\n",
              "  0.021272224887361062,\n",
              "  0.02123457049748984,\n",
              "  0.021070780630677684,\n",
              "  0.02089295702180603,\n",
              "  0.02078082392865768,\n",
              "  0.020588133123398678,\n",
              "  0.020521411082937164,\n",
              "  0.020449651957941288,\n",
              "  0.020438888013389483,\n",
              "  0.020349981953979655,\n",
              "  0.020268587356483972,\n",
              "  0.020190476094543375,\n",
              "  0.020182429174927928,\n",
              "  0.02008642431137446,\n",
              "  0.020024759064078173,\n",
              "  0.020009346914738216,\n",
              "  0.019913218546424364,\n",
              "  0.019924631213460955,\n",
              "  0.019931073169631653,\n",
              "  0.019883174706867155,\n",
              "  0.019853583700366446,\n",
              "  0.01988332692345888,\n",
              "  0.019900749805302344,\n",
              "  0.019830846988638826,\n",
              "  0.0198175417211959,\n",
              "  0.01979180107899725,\n",
              "  0.019822944838359172,\n",
              "  0.019797880832556368,\n",
              "  0.0200123332111711,\n",
              "  0.019938342728714812,\n",
              "  0.01974531450956754,\n",
              "  0.01974322156467804,\n",
              "  0.01992909270490402,\n",
              "  0.019742047438287457,\n",
              "  0.01973910142352995,\n",
              "  0.01992781334612743,\n",
              "  0.019695509655674563,\n",
              "  0.019806363602825065,\n",
              "  0.02003289824987511,\n",
              "  0.01991314187958431,\n",
              "  0.019744422780462057,\n",
              "  0.019786073846617003,\n",
              "  0.019732216714695595,\n",
              "  0.019824846947315494,\n",
              "  0.0198173529332581,\n",
              "  0.019740111271433936],\n",
              " 'dev_loss_history': [tensor(0.0241, device='cuda:0'),\n",
              "  tensor(0.0251, device='cuda:0'),\n",
              "  tensor(0.0229, device='cuda:0'),\n",
              "  tensor(0.0240, device='cuda:0'),\n",
              "  tensor(0.0235, device='cuda:0'),\n",
              "  tensor(0.0228, device='cuda:0'),\n",
              "  tensor(0.0233, device='cuda:0'),\n",
              "  tensor(0.0228, device='cuda:0'),\n",
              "  tensor(0.0240, device='cuda:0'),\n",
              "  tensor(0.0226, device='cuda:0'),\n",
              "  tensor(0.0227, device='cuda:0'),\n",
              "  tensor(0.0227, device='cuda:0'),\n",
              "  tensor(0.0233, device='cuda:0'),\n",
              "  tensor(0.0227, device='cuda:0'),\n",
              "  tensor(0.0229, device='cuda:0'),\n",
              "  tensor(0.0231, device='cuda:0'),\n",
              "  tensor(0.0231, device='cuda:0'),\n",
              "  tensor(0.0230, device='cuda:0'),\n",
              "  tensor(0.0231, device='cuda:0'),\n",
              "  tensor(0.0228, device='cuda:0'),\n",
              "  tensor(0.0230, device='cuda:0'),\n",
              "  tensor(0.0228, device='cuda:0'),\n",
              "  tensor(0.0228, device='cuda:0'),\n",
              "  tensor(0.0228, device='cuda:0'),\n",
              "  tensor(0.0228, device='cuda:0'),\n",
              "  tensor(0.0228, device='cuda:0'),\n",
              "  tensor(0.0229, device='cuda:0'),\n",
              "  tensor(0.0229, device='cuda:0'),\n",
              "  tensor(0.0228, device='cuda:0'),\n",
              "  tensor(0.0228, device='cuda:0'),\n",
              "  tensor(0.0229, device='cuda:0'),\n",
              "  tensor(0.0229, device='cuda:0'),\n",
              "  tensor(0.0228, device='cuda:0'),\n",
              "  tensor(0.0228, device='cuda:0'),\n",
              "  tensor(0.0228, device='cuda:0'),\n",
              "  tensor(0.0228, device='cuda:0'),\n",
              "  tensor(0.0228, device='cuda:0'),\n",
              "  tensor(0.0228, device='cuda:0'),\n",
              "  tensor(0.0228, device='cuda:0'),\n",
              "  tensor(0.0229, device='cuda:0'),\n",
              "  tensor(0.0229, device='cuda:0'),\n",
              "  tensor(0.0229, device='cuda:0'),\n",
              "  tensor(0.0229, device='cuda:0'),\n",
              "  tensor(0.0229, device='cuda:0'),\n",
              "  tensor(0.0229, device='cuda:0'),\n",
              "  tensor(0.0229, device='cuda:0'),\n",
              "  tensor(0.0229, device='cuda:0'),\n",
              "  tensor(0.0229, device='cuda:0'),\n",
              "  tensor(0.0229, device='cuda:0'),\n",
              "  tensor(0.0229, device='cuda:0')]}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}