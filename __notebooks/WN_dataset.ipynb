{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f57803e",
   "metadata": {},
   "source": [
    "## This is a tool to generate relation pairs from the Wordnets\n",
    "\n",
    "It supports any wordnet under the `wn` package\n",
    "It exports the following pairs:\n",
    " * Hypernym-hyponym (hypernym, instance_hypernym)\n",
    " * Hypernym-*-hyponym (grand[grand]child of hypernym)\n",
    " * Holonym-meronym (meronym, mero_location, mero_member, mero_part, mero_portion, mero_substance)\n",
    " * Cohyponym (i.e two hyponyms of the same hypernym)\n",
    " * Synonym (i.e., two senses of the same synset)\n",
    " * Antonym (sense-to-sense relation)\n",
    " \n",
    "The generation is optimized by caching calls to the underlying SQLite DB. DB is (optionally) moved to ramdisk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e94d2ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T16:13:49.820482Z",
     "start_time": "2022-11-25T16:13:49.687999Z"
    }
   },
   "outputs": [],
   "source": [
    "import wn\n",
    "from typing import List, Dict, Optional, Generator\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b02223",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T13:20:34.615899Z",
     "start_time": "2022-11-25T13:20:34.612606Z"
    }
   },
   "outputs": [],
   "source": [
    "# Trying to use ramdisk below to speedup everything a bit\n",
    "# !diskutil erasevolume HFS+ 'RAMDisk' `hdiutil attach -nobrowse -nomount ram://2097152`\n",
    "# !cp -r ~/.wn_data /Volumes/RAMDisk/\n",
    "\n",
    "from pathlib import Path\n",
    "wn.config._dbpath = Path(\"/Volumes/RAMDisk/.wn_data/wn.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b25e1bb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T13:20:34.630886Z",
     "start_time": "2022-11-25T13:20:34.627242Z"
    }
   },
   "outputs": [],
   "source": [
    "LEXICON_ID: str = \"omw-en31\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8002b2df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T13:20:34.637640Z",
     "start_time": "2022-11-25T13:20:34.633016Z"
    }
   },
   "outputs": [],
   "source": [
    "REL_HYPERNYM: str = \"hypernym\"\n",
    "REL_INSTANCE_HYPERNYM: str = \"instance_hypernym\"\n",
    "REL_HOLONYM: str = \"holonym\"\n",
    "REL_ANTONYM: str = \"antonym\"\n",
    "REL_HYPERNYM_LEAP: str = \"hypernym_leap_%s\"\n",
    "\n",
    "REL_SYNONYM: str = \"synonym\"\n",
    "REL_COHYPONYM: str = \"co_hyponym\"\n",
    "\n",
    "\n",
    "SYNSET_RELATIONS: List[str] = [\n",
    "    # Covered by synset.hypernyms\n",
    "    # REL_HYPERNYM,\n",
    "    # REL_INSTANCE_HYPERNYM,\n",
    "    \n",
    "    # Covered computationally\n",
    "    # Also hypernym_leap_1, hypernym_leap_2...\n",
    "\n",
    "    # Covered by synset.meronyms\n",
    "    # REL_HOLONYM,\n",
    "    \n",
    "    # Covered computationally\n",
    "    # Also synonym\n",
    "    # Also co_hyponym\n",
    "]\n",
    "    \n",
    "# Synset-synset stats:\n",
    "# [('derivation', 50397),\n",
    "#  ('pertainym', 7920),\n",
    "#  ('antonym', 7772),\n",
    "#  ('is_exemplified_by', 390),\n",
    "#  ('also', 324),\n",
    "#  ('domain_region', 98),\n",
    "#  ('participle', 73),\n",
    "#  ('domain_topic', 12),\n",
    "#  ('has_domain_topic', 11),\n",
    "#  ('exemplifies', 8),\n",
    "#  ('has_domain_region', 4),\n",
    "#  ('similar', 2)]\n",
    "    \n",
    "SENSE_RELATIONS: List[str] = [\n",
    "    REL_ANTONYM,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bd2e156",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T13:20:34.989003Z",
     "start_time": "2022-11-25T13:20:34.966247Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "root: wn.Synset = wn.synset(id=\"omw-en31-00001740-n\", lexicon=LEXICON_ID)\n",
    "sample: wn.Synset = wn.synset(id=\"omw-en31-05990115-n\", lexicon=LEXICON_ID)\n",
    "sample2: wn.Synset = wn.synset(id=\"omw-en31-07961030-n\", lexicon=LEXICON_ID)\n",
    "\n",
    "Relation = namedtuple(\n",
    "    \"Relation\",\n",
    "    [\n",
    "        \"synset_id_left\",\n",
    "        \"synset_id_right\",\n",
    "        \"sense_id_left\",\n",
    "        \"sense_id_right\",\n",
    "        \"pos_left\",\n",
    "        \"pos_right\",\n",
    "        \"rel\",\n",
    "        \"lemma_left\",\n",
    "        \"lemma_right\",\n",
    "        \"path_len\",  # Min length of the path between two synsets on hypernym/hyponym three\n",
    "        \"level_left\", # Min depth on the hypernymy tree\n",
    "        \"level_right\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46e6a995",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T17:38:43.944274Z",
     "start_time": "2022-11-25T17:38:43.622224Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import combinations, product\n",
    "from functools import cache\n",
    "\n",
    "# Wrapping expensive calls into @cache decorator\n",
    "@cache\n",
    "def get_sense_lemma(sense: wn.Sense) -> str:\n",
    "    return sense.word().lemma()\n",
    "\n",
    "@cache\n",
    "def get_shortest_path_len(synset_left: wn.Synset, synset_right: wn.Synset) -> int:\n",
    "    try:\n",
    "        return len(synset_left.shortest_path(synset_right))\n",
    "    except wn.Error:\n",
    "        return 1000\n",
    "        \n",
    "\n",
    "@cache\n",
    "def get_level(synset: wn.Synset) -> int:\n",
    "    return synset.min_depth()\n",
    "\n",
    "@cache\n",
    "def get_synset_from_sense(sense: wn.Sense) -> wn.Synset:\n",
    "    return sense.synset()\n",
    "\n",
    "\n",
    "def get_relation_record(\n",
    "    sense_left: wn.Sense, sense_right: wn.Sense, rel_type: str\n",
    ") -> Relation:\n",
    "    synset_left: wn.Synset = get_synset_from_sense(sense_left)\n",
    "    synset_right: wn.Synset = get_synset_from_sense(sense_right)\n",
    "\n",
    "    return Relation(\n",
    "        synset_id_left=synset_left.id,\n",
    "        synset_id_right=synset_right.id,\n",
    "        sense_id_left=sense_left.id,\n",
    "        sense_id_right=sense_right.id,\n",
    "        pos_left=synset_left.pos,\n",
    "        pos_right=synset_right.pos,\n",
    "        rel=rel_type,\n",
    "        lemma_left=get_sense_lemma(sense_left),\n",
    "        lemma_right=get_sense_lemma(sense_right),\n",
    "        path_len=get_shortest_path_len(synset_left, synset_right),\n",
    "        level_left=get_level(synset_left),\n",
    "        level_right=get_level(synset_right),\n",
    "    )\n",
    "\n",
    "\n",
    "def export_hypernyms(\n",
    "    hypernym: wn.Synset, hyponym: wn.Synset, curr_depth: int, max_depth: int\n",
    ") -> List[Relation]:\n",
    "    if curr_depth == 0:\n",
    "        rel: str = REL_HYPERNYM\n",
    "    else:\n",
    "        rel = REL_HYPERNYM_LEAP % curr_depth\n",
    "\n",
    "    res: List[Relation] = []\n",
    "    for a, b in product(hypernym.senses(), hyponym.senses()):\n",
    "        res.append(get_relation_record(sense_left=a, sense_right=b, rel_type=rel))\n",
    "\n",
    "    if curr_depth < max_depth - 1:\n",
    "        for child_hyponym in hyponym.hyponyms():\n",
    "            res += export_hypernyms(\n",
    "                hypernym=hypernym,\n",
    "                hyponym=child_hyponym,\n",
    "                curr_depth=curr_depth + 1,\n",
    "                max_depth=max_depth,\n",
    "            )\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def extract_relations(synset: wn.Synset, hypernym_depth: int = 2) -> List[Relation]:\n",
    "    lemmas: List[str] = synset.lemmas()\n",
    "    pos = synset.pos\n",
    "    res: List[Relation] = []\n",
    "\n",
    "    # Synonyms\n",
    "    for a, b in combinations(synset.senses(), 2):\n",
    "        res.append(\n",
    "            # TODO: add reverse relation?\n",
    "            get_relation_record(sense_left=a, sense_right=b, rel_type=REL_SYNONYM)\n",
    "        )\n",
    "\n",
    "    # hypernyms:\n",
    "    for hyponym in synset.hyponyms():\n",
    "        res += export_hypernyms(\n",
    "            hypernym=synset, hyponym=hyponym, curr_depth=0, max_depth=hypernym_depth\n",
    "        )\n",
    "\n",
    "    # holonyms:\n",
    "    for meronym in synset.meronyms():\n",
    "        for a, b in product(synset.senses(), meronym.senses()):\n",
    "            res.append(\n",
    "                get_relation_record(sense_left=a, sense_right=b, rel_type=REL_HOLONYM)\n",
    "            )\n",
    "\n",
    "    # cohyponyms:\n",
    "    for hyp1, hyp2 in combinations(synset.hyponyms(), 2):\n",
    "        # TODO: check for reverse relations?\n",
    "        for a, b in product(hyp1.senses(), hyp2.senses()):\n",
    "            res.append(\n",
    "                get_relation_record(sense_left=a, sense_right=b, rel_type=REL_COHYPONYM)\n",
    "            )\n",
    "\n",
    "    # Sense 2 Sense relations\n",
    "    for sense in synset.senses():\n",
    "        for rel, related_senses in sense.relations(*SENSE_RELATIONS).items():\n",
    "            for related_sense in related_senses:\n",
    "                res.append(\n",
    "                    get_relation_record(\n",
    "                        sense_left=sense, sense_right=related_sense, rel_type=rel\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68676c85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T17:38:44.074728Z",
     "start_time": "2022-11-25T17:38:44.071623Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# extract_relations(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7da4e9a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T17:38:44.605557Z",
     "start_time": "2022-11-25T17:38:44.600550Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_all_relations(\n",
    "    hypernym_depth: int = 2, first_n: Optional[int] = None\n",
    ") -> Generator[Relation, None, None]:\n",
    "    with tqdm(desc=\"Relations out\") as pbar:\n",
    "        for i, synset in enumerate(\n",
    "            tqdm(wn.synsets(lexicon=LEXICON_ID), desc=\"Synsets in\")\n",
    "        ):\n",
    "            for rel in extract_relations(synset, hypernym_depth=hypernym_depth):\n",
    "                pbar.update(1)\n",
    "                yield rel\n",
    "\n",
    "            if first_n is not None and i + 1 >= first_n:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e908ecb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T18:52:51.739729Z",
     "start_time": "2022-11-25T17:38:45.280642Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a800acdbe5f0470b80370cfcd7756d28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Relations out: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a1bed7b4e7441a91020080a314be42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Synsets in:   0%|          | 0/117791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import smart_open\n",
    "import csv\n",
    "\n",
    "with open(\"all_relations_raw.depth3.csv.bz2\", \"wt\") as fp_out:\n",
    "    w = csv.DictWriter(fp_out, fieldnames=Relation._fields)\n",
    "    w.writeheader()\n",
    "    for rel in get_all_relations(hypernym_depth=3):\n",
    "        w.writerow(rel._asdict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d6c367",
   "metadata": {},
   "source": [
    "## Some remarks on further dataset cleansing\n",
    " * Dataset is huge. Think twice if you want to use ALL of the pairs\n",
    " * You may (or you may not) remove multi-word lemmas\n",
    " * You definitely want to remove all **word** pairs that belong to more than one relation (especially when it's about polysemy)\n",
    " * For your convenience, each word pair has a distance between the words on the hypernym/hyponym tree. You might want to filter some relations that are too close, for example, antonyms with the same hypernym.\n",
    " * Each pair also has a distance from the synset to the top. You might want to remove noun pairs that are too close to the top (abstract part) of the wordnet\n",
    " * You might merge hypernym-leap-* relations with direct hypernym-hyponym relations (to see if the classifier can learn a general sense of hypernymy-hyponymy) or keep them (to see if the classifier can distinguish between direct hypernym-hyponym pair and an indirect one) or remove them at all\n",
    " * You definitely want to build your train/val/test dataset carefully so you don't have data leakage when, for example, a pair of hypernym-hyponym is available in train and a pair of hypernym-*-hyponym is available in the test dataset\n",
    " * You must pay attention to the negative pairs (random relation). They shouldn't be present in the dataset of relations above (for an apparent reason), and also, they shouldn't be too close to each other (i.e., the shortest path length should be more than, say, 7).\n",
    " * You have to pay attention to class disbalance (for example, there are much smaller amounts of antonyms than other relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6c63f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
