{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4803ff3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T12:30:12.123703Z",
     "start_time": "2022-11-28T12:30:12.032496Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple, Counter, defaultdict\n",
    "from itertools import combinations\n",
    "from random import choices\n",
    "from typing import Set, Dict, List\n",
    "import bz2\n",
    "import json\n",
    "import csv\n",
    "import unicodedata\n",
    "from tqdm.notebook import tqdm\n",
    "import lzma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0427d818",
   "metadata": {},
   "source": [
    "### Trying to prepare synonym/antonym/random dataset from the ULIF datasource\n",
    "\n",
    "https://svc2.ulif.org.ua/dictua/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "166cb585",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T12:30:12.130307Z",
     "start_time": "2022-11-28T12:30:12.125657Z"
    }
   },
   "outputs": [],
   "source": [
    "def deaccent(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove accentuation from the given string. Input text is either a unicode string or utf8 encoded bytestring.\n",
    "\n",
    "    Return input string with accents removed, as unicode.\n",
    "\n",
    "    >>> deaccent(\"Šéf chomutovských komunistů dostal poštou bílý prášek\")\n",
    "    u'Sef chomutovskych komunistu dostal postou bily prasek'\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    res = []\n",
    "\n",
    "    for c in text:\n",
    "        if c in \"їйЇЙ\":\n",
    "            res.append(c)\n",
    "        else:\n",
    "            norm = unicodedata.normalize(\"NFD\", c)\n",
    "            res.append(\n",
    "                unicodedata.normalize(\n",
    "                    \"NFC\",\n",
    "                    \"\".join(ch for ch in norm if unicodedata.category(ch) != \"Mn\"),\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    return \"\".join(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04185905",
   "metadata": {},
   "source": [
    "### Relation types to export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1da4327",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T12:30:12.136902Z",
     "start_time": "2022-11-28T12:30:12.133842Z"
    }
   },
   "outputs": [],
   "source": [
    "REL_ANTONYM: str = \"antonym\"\n",
    "REL_RANDOM: str = \"random\"\n",
    "REL_SYNONYM: str = \"synonym\"\n",
    "\n",
    "Relation = namedtuple(\"Relation\", [\"lemma_left\", \"lemma_right\", \"rel\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7793a32d",
   "metadata": {},
   "source": [
    "### Collecting lemma pairs and normalizing them\n",
    " * `all_words` will be used to draw random pairs for neg sampling\n",
    " * `all_relations` will have all possible combinations of synonyms in \"synsets\" and antonyms\n",
    " \n",
    " Important thing: we are sorting words in pairs to prevent symmetric pairs like synonym1/synonym2 and synonym2/synonym1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdf6f2bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T12:35:11.390622Z",
     "start_time": "2022-11-28T12:30:12.139851Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ab08850a7c47c1aa5485daddd01873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_words: Set[str] = set()\n",
    "words_by_pos: Dict[str, set] = defaultdict(set)\n",
    "all_relations: Set[Relation] = set()\n",
    "\n",
    "with bz2.open(\"aux data/from_1_try5.jsonlines.bz2\", \"rt\") as fp:\n",
    "    for i, l in enumerate(tqdm(fp)):\n",
    "        data: Dict = json.loads(l)\n",
    "        base_lemma: str = deaccent(data[\"base\"][\"value\"]).lower().strip(\" 0123456789\")\n",
    "\n",
    "        all_words.add(base_lemma)\n",
    "        words_by_pos[data[\"base\"][\"type\"]].add(base_lemma)\n",
    "\n",
    "        if \"antonyms\" in data:\n",
    "            antonym = data[\"antonyms\"].get(\"antonyms\")\n",
    "            lemma = data[\"antonyms\"].get(\"lemmas\")\n",
    "\n",
    "            if antonym is not None and lemma is not None:\n",
    "                antonyms_pair = sorted(\n",
    "                    [deaccent(lemma).lower(), deaccent(antonym).lower()]\n",
    "                )\n",
    "                all_relations.add(\n",
    "                    Relation(\n",
    "                        lemma_left=antonyms_pair[0],\n",
    "                        lemma_right=antonyms_pair[1],\n",
    "                        rel=REL_ANTONYM,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        for synonym_blocks in data.get(\"synonyms\", []):\n",
    "            for synonyms in synonym_blocks:\n",
    "                for syn1, syn2 in combinations(synonyms.get(\"synonyms\", []), 2):\n",
    "                    if syn1 is None or syn2 is None:\n",
    "                        break\n",
    "\n",
    "                    synonyms_pair = sorted(\n",
    "                        [deaccent(syn1).lower(), deaccent(syn2).lower()]\n",
    "                    )\n",
    "                    all_relations.add(\n",
    "                        Relation(\n",
    "                            lemma_left=synonyms_pair[0],\n",
    "                            lemma_right=synonyms_pair[1],\n",
    "                            rel=REL_SYNONYM,\n",
    "                        )\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739c6d37",
   "metadata": {},
   "source": [
    "### Sanity check for the number of pairs under relations and duplicates\n",
    "Also we'll use `word_pair_counter` later to not to draw random synonyms as a neg sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "659acaf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T12:35:12.420044Z",
     "start_time": "2022-11-28T12:35:11.393098Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6a805873094154bcb08b478ec9994e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/278008 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_pair_counter = Counter()\n",
    "rel_type_counter = Counter()\n",
    "\n",
    "for rel in tqdm(all_relations):\n",
    "    word_pair_counter.update([tuple(sorted([rel.lemma_left, rel.lemma_right]))])\n",
    "    rel_type_counter.update([rel.rel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd0693b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T12:35:12.461059Z",
     "start_time": "2022-11-28T12:35:12.422417Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('вилазити', 'залазити'), 2),\n",
       " (('де прийшлося', 'де припало'), 1),\n",
       " (('живоїд', 'шкуролуп'), 1),\n",
       " (('обілляти', 'полляти'), 1),\n",
       " (('бебехнути', 'хвиснути'), 1),\n",
       " (('обшарпанець', 'харпак'), 1),\n",
       " (('протнути', 'розлягтися'), 1),\n",
       " (('високомовність', 'пишномовність'), 1),\n",
       " (('відсапнутися', 'дихнути'), 1),\n",
       " (('згнітити', 'стримувати'), 1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_pair_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6115f6b4",
   "metadata": {},
   "source": [
    "### Adding lemma frequency dict to filter out infrequent pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bf3b8ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T12:36:31.232766Z",
     "start_time": "2022-11-28T12:35:12.464434Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c8ac1ef5f184a9fa975348e0b20d210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lemma_freqs: Dict[str, float] = defaultdict(float)\n",
    "\n",
    "def fix_nulls(fp_in):\n",
    "    for line in fp_in:\n",
    "        yield line.replace('\\0', '')\n",
    "\n",
    "with lzma.open(\"aux data/lemma_freqs.csv.xz\", \"rt\") as fp_in:\n",
    "    r = csv.DictReader(fix_nulls(fp_in))\n",
    "    \n",
    "    for l in tqdm(r):\n",
    "        lemma_freqs[l[\"lemma\"]] += float(l[\"freq_in_corpus\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375329b2",
   "metadata": {},
   "source": [
    "### Ignoring antonyms cause we don't have many of them and low freq lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1546e058",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T12:36:31.756411Z",
     "start_time": "2022-11-28T12:36:31.234902Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5023edbd80b455bb4e52cd8a4f3387d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/278008 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ULIF_HF_SYNONYMS: List[Relation] = []\n",
    "\n",
    "for rel in tqdm(all_relations):\n",
    "    if rel.rel != REL_SYNONYM:\n",
    "        continue\n",
    "    \n",
    "    if lemma_freqs[rel.lemma_left] < 5e-6:\n",
    "        continue\n",
    "\n",
    "    if lemma_freqs[rel.lemma_right] < 5e-6:\n",
    "        continue\n",
    "    \n",
    "    ULIF_HF_SYNONYMS.append(rel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42380d69",
   "metadata": {},
   "source": [
    "### Drawing NEG_RATIO more random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21bfce3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T12:36:31.770147Z",
     "start_time": "2022-11-28T12:36:31.764263Z"
    }
   },
   "outputs": [],
   "source": [
    "NEG_RATIO: int = 5\n",
    "NEG_SAMPLES_COUNT: int = len(ULIF_HF_SYNONYMS) * NEG_RATIO\n",
    "NEG_SAMPLES_POS_ALIGNED_COUNT: int = int(NEG_SAMPLES_COUNT * 0.8)\n",
    "NEG_SAMPLES_RANDOM_COUNT: int = NEG_SAMPLES_COUNT - NEG_SAMPLES_POS_ALIGNED_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b7cb581",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T12:36:31.778891Z",
     "start_time": "2022-11-28T12:36:31.773672Z"
    }
   },
   "outputs": [],
   "source": [
    "pos_counts: Counter[str, int] = Counter({p: len(w) for p, w in words_by_pos.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa4312c",
   "metadata": {},
   "source": [
    "### First we draw truly random pairs from different POS combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bae5bbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T12:36:32.122830Z",
     "start_time": "2022-11-28T12:36:31.782801Z"
    }
   },
   "outputs": [],
   "source": [
    "NEG_SAMPLES: Set[Relation] = set()\n",
    "ALL_WORDS: List[str] = list(all_words)\n",
    "\n",
    "while True:\n",
    "    candidates = tuple(sorted(choices(ALL_WORDS, k=2)))\n",
    "    \n",
    "    if candidates[0] == candidates[1]:\n",
    "        continue\n",
    "\n",
    "    if \" \" in candidates[0] or \" \" in candidates[1]:\n",
    "        continue\n",
    "\n",
    "    if candidates in word_pair_counter:\n",
    "        continue\n",
    "    \n",
    "    NEG_SAMPLES.add(Relation(lemma_left=candidates[0], lemma_right=candidates[1], rel=REL_RANDOM))\n",
    "    if len(NEG_SAMPLES) == NEG_SAMPLES_RANDOM_COUNT:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b207a732",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T12:37:44.674820Z",
     "start_time": "2022-11-28T12:36:32.125520Z"
    }
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    pos = choices(list(pos_counts.keys()), weights = list(pos_counts.values()), k = 1)[0]\n",
    "\n",
    "    candidates = tuple(sorted(choices(list(words_by_pos[pos]), k=2)))\n",
    "    \n",
    "    if candidates[0] == candidates[1]:\n",
    "        continue\n",
    "\n",
    "    if \" \" in candidates[0] or \" \" in candidates[1]:\n",
    "        continue\n",
    "\n",
    "    if candidates in word_pair_counter:\n",
    "        continue\n",
    "\n",
    "    NEG_SAMPLES.add(Relation(lemma_left=candidates[0], lemma_right=candidates[1], rel=REL_RANDOM))\n",
    "    if len(NEG_SAMPLES) == NEG_SAMPLES_POS_ALIGNED_COUNT:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b500064",
   "metadata": {},
   "source": [
    "### Here we adhere to the naming convention from the original RUMEN dataset\n",
    "\n",
    "\n",
    "```python\n",
    "def get_names(cat):\n",
    "    if cat == 0:\n",
    "        return \"RANDOM\"\n",
    "    if cat == 1:\n",
    "        return \"HYPER\"\n",
    "    if cat == 2:\n",
    "        return \"SYN\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c8b83c",
   "metadata": {},
   "source": [
    "### Exporting ulif hf synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7f5eb9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T12:37:44.756763Z",
     "start_time": "2022-11-28T12:37:44.678582Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"aux data/ulif_hf_synonyms.csv\", \"w\") as fp_out:\n",
    "    w = csv.DictWriter(fp_out, fieldnames=[\"W1\", \"W2\", \"rel\"])\n",
    "    w.writeheader()\n",
    "    for syn in ULIF_HF_SYNONYMS:\n",
    "        w.writerow({\n",
    "            \"W1\": syn.lemma_left,\n",
    "            \"W2\": syn.lemma_right,\n",
    "            \"rel\": 2,\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a18f141",
   "metadata": {},
   "source": [
    "### Exporting random pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f60c3941",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T12:37:45.029537Z",
     "start_time": "2022-11-28T12:37:44.761836Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"aux data/ulif_random.csv\", \"w\") as fp_out:\n",
    "    w = csv.DictWriter(fp_out, fieldnames=[\"W1\", \"W2\", \"rel\"])\n",
    "    w.writeheader()\n",
    "    for syn in NEG_SAMPLES:\n",
    "        w.writerow({\n",
    "            \"W1\": syn.lemma_left,\n",
    "            \"W2\": syn.lemma_right,\n",
    "            \"rel\": 0,\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dedcd51",
   "metadata": {},
   "source": [
    "### Now let's try another source of synonyms: https://synonimy.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5389e7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T12:37:45.629254Z",
     "start_time": "2022-11-28T12:37:45.031768Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"aux data/synonimy_info_clean.json\", \"r\") as fp_in:\n",
    "    synonimy_info: Dict = json.load(fp_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca512cce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T12:37:47.837734Z",
     "start_time": "2022-11-28T12:37:45.633211Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da784a19a6a473696764b3a3d3fa5e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SYNONIMY_INFO_SYNONYMS: Set[Relation] = set()\n",
    "\n",
    "for lemma_rec in tqdm(synonimy_info):\n",
    "    synset = set([deaccent(lemma_rec[\"lemma\"]).lower()] + [deaccent(clean).lower() for clean in lemma_rec[\"synsets\"][0][\"clean\"]])\n",
    "\n",
    "    for syn1, syn2 in combinations(synset, 2):\n",
    "        synonyms_pair = sorted(\n",
    "            [syn1, syn2]\n",
    "        )\n",
    "        \n",
    "        SYNONIMY_INFO_SYNONYMS.add(\n",
    "            Relation(\n",
    "                lemma_left=synonyms_pair[0],\n",
    "                lemma_right=synonyms_pair[1],\n",
    "                rel=REL_SYNONYM,\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bf2fa60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T12:37:48.287518Z",
     "start_time": "2022-11-28T12:37:47.839758Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb87a9305154a64b5ac5b76b206f8c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/282411 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SYNONIMY_INFO_HF_SYNONYMS: List[Relation] = []\n",
    "\n",
    "for rel in tqdm(SYNONIMY_INFO_SYNONYMS):\n",
    "    if lemma_freqs[rel.lemma_left] < 5e-6:\n",
    "        continue\n",
    "\n",
    "    if lemma_freqs[rel.lemma_right] < 5e-6:\n",
    "        continue\n",
    "    \n",
    "    SYNONIMY_INFO_HF_SYNONYMS.append(rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43ce275e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T12:37:48.404413Z",
     "start_time": "2022-11-28T12:37:48.291106Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"aux data/synonimy_info_hf_synonyms.csv\", \"w\") as fp_out:\n",
    "    w = csv.DictWriter(fp_out, fieldnames=[\"W1\", \"W2\", \"rel\"])\n",
    "    w.writeheader()\n",
    "    for syn in SYNONIMY_INFO_HF_SYNONYMS:\n",
    "        w.writerow({\n",
    "            \"W1\": syn.lemma_left,\n",
    "            \"W2\": syn.lemma_right,\n",
    "            \"rel\": 2,\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9daee7",
   "metadata": {},
   "source": [
    "### Now let's combine ulif synonyms/random and synonimy_info/random datasets for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99dd04ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T12:37:49.020958Z",
     "start_time": "2022-11-28T12:37:48.406596Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"aux data/ulif_hf_synonyms_and_random.csv\", \"w\") as fp_out:\n",
    "    w = csv.DictWriter(fp_out, fieldnames=[\"W1\", \"W2\", \"rel\"])\n",
    "    w.writeheader()\n",
    "    for syn in ULIF_HF_SYNONYMS:\n",
    "        w.writerow({\n",
    "            \"W1\": syn.lemma_left,\n",
    "            \"W2\": syn.lemma_right,\n",
    "            \"rel\": 2,\n",
    "        })\n",
    "\n",
    "    for syn in NEG_SAMPLES:\n",
    "        w.writerow({\n",
    "            \"W1\": syn.lemma_left,\n",
    "            \"W2\": syn.lemma_right,\n",
    "            \"rel\": 0,\n",
    "        })\n",
    "\n",
    "with open(\"aux data/synonimy_info_hf_synonyms_and_random.csv\", \"w\") as fp_out:\n",
    "    w = csv.DictWriter(fp_out, fieldnames=[\"W1\", \"W2\", \"rel\"])\n",
    "    w.writeheader()\n",
    "    for syn in SYNONIMY_INFO_HF_SYNONYMS:\n",
    "        w.writerow({\n",
    "            \"W1\": syn.lemma_left,\n",
    "            \"W2\": syn.lemma_right,\n",
    "            \"rel\": 2,\n",
    "        })\n",
    "\n",
    "    for syn in NEG_SAMPLES:\n",
    "        w.writerow({\n",
    "            \"W1\": syn.lemma_left,\n",
    "            \"W2\": syn.lemma_right,\n",
    "            \"rel\": 0,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad1b20f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442ac794",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
